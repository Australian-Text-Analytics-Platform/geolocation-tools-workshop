{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATAP Notebook for the Geolocation project\n",
    "\n",
    "This notebook helps you access the Geolocation tools in a Python development environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "* [Premise](#section-premise)\n",
    "* [Requirements](#section-requirements) \n",
    "* [Data Preparation](#section-datapreparation)\n",
    "* [Identifying Placenames in Text](#section-identifyingplacenames)\n",
    " * [Named Entity Recognition](#section-ner)\n",
    " * [Reviewing Candidate Placenames](#section-reviewplacenames)\n",
    "* [Finding Locations for Placenames](#section-findinglocs)\n",
    " * [Identifying States and Capitals](#section-statescapitals)\n",
    " * [Searching a Gazzetteer for Locations](#section-searchgazetteer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise <a class=\"anchor\" id=\"section-premise\"></a>\n",
    "*This section explains the Geolocation project and tools.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geolocation project relates to doctoral research done by [Fiannuala Morgan](https://finnoscarmorgan.github.io/) at the Australian National University. It uses software to identify placenames in archived historical texts, then compares them to data about known locations to identify where the placenames may be located. \n",
    "\n",
    "This notebook is designed to allow you to perform similar operations on textual documents as a step-by-step process. Various issues and considerations about such operations will be discussed during the process. Some choices will have to be made by you during certain steps in order to complete the process. The notebook uses the Python programming language but users are not expected to read or understand the Python code that is included. However, you will need a basic understanding of computer programming if you wish to edit aspects of the notebook.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "It will teach you how to\n",
    "<ul>\n",
    "    <li>use the spaCy library to identify and classify Named Entities (NEs)</li>\n",
    "    <li>identify multi-word expressions (MWE) that are NEs</li>\n",
    "    <li>search for spatial data about specific locations or places in gazetteers of such data</li>\n",
    "    <li>determine which locations are referred to by placenames, based on the context in which they are used in a text</li>\n",
    "</ul>\n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements <a class=\"anchor\" id=\"section-requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses various Python libraries. Most will come with your Python installation, but the following are crucial:\n",
    "- pandas\n",
    "- nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install ratelimit\n",
    "!pip install spacy\n",
    "# You need the old version of this package due to issues setting titles with the latest version\n",
    "!pip install ipywidgets==\"7.6.2\"\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# spaCy is used for a pipeline of NLP functions\n",
    "import spacy\n",
    "\n",
    "# ipywidgets is used for user interactive interfaces\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Imports for the map API requests and output formatting\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "from ratelimit import limits, sleep_and_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you can see as much of the output as possible within the Jupyter Notebook screen\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation <a class=\"anchor\" id=\"section-datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also want to set up some directories for the import and output of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the data directories\n",
    "# This presumes that 'notebooks' is the current working directory\n",
    "text_directory = os.path.normpath(\"../texts/\")\n",
    "csv_directory = os.path.normpath(\"../output/\")\n",
    "reference_directory = os.path.normpath(\"../data\")\n",
    "\n",
    "# Create the data directories\n",
    "if not os.path.exists(text_directory):\n",
    "    os.makedirs(text_directory)\n",
    "if not os.path.exists(csv_directory):\n",
    "    os.makedirs(csv_directory)\n",
    "if not os.path.exists(reference_directory):\n",
    "    os.makedirs(reference_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, you will be examining the text of *For the Term of His Natural Life* (which will be abbreviated as FtToHNL), an 1874CE novel by Marcus Clarke that is in the public domain. This was obtained via the [Gutenburg Project Australia](https://gutenberg.net.au/ebooks/e00016.txt). It is an unformatted text file. For this workshop, it has been further simplified by reducing it to only standard ASCII characters, replacing any accented characters with their unaccented forms and the British Pound Sterling symbol with the word pounds.\n",
    "\n",
    "The novel is divided into four books, each based in different regions of the world. For instance, you might want to start with the second book, which is titled *BOOK II.\\-\\-MACQUARIE HARBOUR.  1833*. Each book can be processed in its entirety, or as individual chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_BOOK_2_CHAPTER_3.txt\"\n",
    "print(\"Reading:\", filename)\n",
    "\n",
    "# Set the specific path for the 'filename'\n",
    "text_location = os.path.normpath(os.path.join(text_directory, filename))\n",
    "text_filename = os.path.basename(text_location)\n",
    "\n",
    "text = open(text_location, encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now read Chapter 3 of Book 2 into memory. This is currently no more than a long string of characters. So far, you have done no processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 501 characters\n",
    "text[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Placenames in Text <a class=\"anchor\" id=\"section-identifyingplacenames\"></a>\n",
    "*This section provides tools on identifying placenames in textual data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition <a class=\"anchor\" id=\"section-ner\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts like the start of Chapter 3, Book 2, mention a lot of places. Some of them are a little vague, like _the house of Major Vickers_, _head-quarters_ and _the settlement_, but some are very explicit like _Maria Island_ and _Macquarie Harbour_. This notebook will show you how to use software automatically identify these proper noun phrases relating to placenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part of this notebook uses the Named Entity Recognition (NER) tool provided in the spaCy Python package. For an introduction to this tool and how it can be used to find Named Entities (NEs) like placenames in text, go to the [spaCy NER notebook](https://github.com/Australian-Text-Analytics-Platform/geolocation-tools-workshop/blob/7d92664ac44f86b90a0c098bb3159793a4fe6c16/Notebooks/spacy_ner_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This NER need not be done one file at a time. You can now put all of this together and find the placenames that are identified by spaCy in each chapter of the text. The results can all be collected in a single data structure, reviewed and saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe where we store the details about each instance of the placenames\n",
    "placenames_df = pd.DataFrame(columns=[\"Book\", \"Chapter\", \"NEIndex\", \"Placename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which chapters and books you want to examine\n",
    "chapters = [1, 2, 3]\n",
    "books = [1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a spaCy model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should define what spaCy processing you do or don't want in the pipeline. You mainly need the Tokenizer and the NER components. Others, like the Parser slow the processing down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabled_pipeline = [\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, not all NEs are placenames, so you will need to make a list of what categories regularly contain placenames. This notebook focusses on the Location, Geo-Political Entity, Facility and Organisation categories. While they may not cover all processing of all instances of all placenames, they have been observed to identify the majority of placenames without providing too many irrelevant NEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placename_categories = [\"LOC\", \"GPE\", \"FAC\", \"ORG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start processing the chapters from FtToHNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0  # Counter of the entities\n",
    "for book in books:\n",
    "    for chapter in chapters:\n",
    "        # Construct the filename for this book and chapter\n",
    "        filename = \"FtToHNL_BOOK_\" + str(book) + \"_CHAPTER_\" + str(chapter) + \".txt\"\n",
    "\n",
    "        # Set the specific path for the 'filename'\n",
    "        text_location = os.path.normpath(os.path.join(text_directory, filename))\n",
    "        text_filename = os.path.basename(text_location)\n",
    "\n",
    "        # Read this chapter\n",
    "        text = open(text_location, encoding=\"utf-8\").read()\n",
    "        print(\"Reading:\", filename)\n",
    "\n",
    "        # Run spaCy\n",
    "        doc = nlp(text, disable=disabled_pipeline)\n",
    "\n",
    "        # Document level\n",
    "        ents = [(entity.text, entity.start_char, entity.end_char, entity.label_) for entity in doc.ents]\n",
    "\n",
    "        # Token level\n",
    "        for entity in doc.ents:\n",
    "            # filter out MONEY, DATE, etc.\n",
    "            if entity.label_ in placename_categories:\n",
    "\n",
    "                # To help understand the context of the text, extract the occurrence\n",
    "                context_text = doc.text[entity.start_char - 30 : entity.end_char + 30].replace(\"\\n\", \" \")\n",
    "\n",
    "                # Add the placenames according to spaCy\n",
    "                new_placename = {\n",
    "                    \"Book\": book,  # The Book number\n",
    "                    \"Chapter\": chapter,  # The Chapter number\n",
    "                    \"NEIndex\": i,  # A reference number to the nth Named Entity\n",
    "                    \"Placename\": entity.text,  # The placename in the text\n",
    "                    \"Category\": entity.label_,  # The spaCy category\n",
    "                    \"Context\": context_text,  # The textual context where the placename was found\n",
    "                    \"Approval\": 1,  # A flag for whether this is a suitable placename\n",
    "                }\n",
    "                placenames_df = placenames_df.append(new_placename, ignore_index=True)\n",
    "\n",
    "            i = i + 1  # Entity counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Save a copy of this data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output what placenames have been found\n",
    "placenames_df[[\"Book\", \"Chapter\", \"NEIndex\", \"Placename\", \"Category\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a lot more placenames were found in Book 2 than Book 1. This makes sense since Book 1 is set on board an ocean voyage, whereas Book 2 is at Macquarie Harbour in Australia. You will also see that the same placename might be recognised multiple times but be categorised differently. This is because the spaCy model used for the NER is dependent on the linguistic content in which the NE instance appears in isolation. It does not assign the category based on multiple instances of an NE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Candidate Placenames <a class=\"anchor\" id=\"section-reviewplacenames\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a number of NEs that are unlikely to be placenames, regardless of what spaCy categorised them as. You want to be able to filter them out. Sometimes though it is hard to work out whether an NE is a person, organisation or a location. For instance, spaCy has said _Van Diemen's_ and _Tasman_ are both ORG NE. While both could be the names of people (Dutch explorer Abel Tasman and Governor-General of the Dutch East Indies Anthony van Dieman) but they are actually part of larger NEs _Van Diemen's Land_ (the former name for the state of Tasmania) and _Tasman's Head_ (a headland in Tasmania). For this, it is best to consider the context in which the terms were used. \n",
    "\n",
    "The following interfaces shows you each NE instance identified by spaCy's NER and the context in which they occure. Use the checkboxes to select which terms you consider to be placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that is used when a checkbox changes\n",
    "def changed(b):\n",
    "    k = b[\"new\"]\n",
    "\n",
    "# Lists of the data required for displaying the checkboxes\n",
    "placename_items = []\n",
    "context_items = []\n",
    "num_items = []\n",
    "\n",
    "# Make checkboxes for every placename for book in books:\n",
    "for book in books:\n",
    "    for chapter in chapters:\n",
    "\n",
    "        # Get the NEs from this book and chapter\n",
    "        placenames_book_chapter = placenames_df[(placenames_df[\"Book\"] == book) & (placenames_df[\"Chapter\"] == chapter)]\n",
    "\n",
    "        # Get the contextual data for each indexed NE\n",
    "        for i in placenames_book_chapter[\"NEIndex\"]:\n",
    "            context_text = placenames_book_chapter[placenames_book_chapter[\"NEIndex\"] == i][\"Context\"].values[0]\n",
    "            category = placenames_book_chapter[placenames_book_chapter[\"NEIndex\"] == i][\"Category\"].values[0]\n",
    "\n",
    "        # Make lists of the candidate placenames, context text and index numbers\n",
    "        # Only the placenames are given a checkbox.\n",
    "        placename_items = placename_items + [\n",
    "            widgets.Checkbox(True, description=i) for i in placenames_book_chapter[\"Placename\"]\n",
    "        ]\n",
    "        context_items = context_items + [\n",
    "            widgets.Label(placenames_book_chapter[placenames_book_chapter[\"NEIndex\"] == i][\"Context\"].values[0])\n",
    "            for i in placenames_book_chapter[\"NEIndex\"]\n",
    "        ]\n",
    "        num_items = num_items + [widgets.Label(str(i)) for i in placenames_book_chapter[\"NEIndex\"]]\n",
    "\n",
    "# Create a display\n",
    "num_placenames = len(placename_items)\n",
    "left_box = widgets.VBox(placename_items)\n",
    "right_box = widgets.VBox(context_items)\n",
    "num_box = widgets.VBox(num_items)\n",
    "whole_box = widgets.HBox([num_box, left_box, right_box])\n",
    "\n",
    "print(\"Unselect any Named Entities (NEs) that you do not consider to be placenames.\")\n",
    "print(\"Each instance of an NE is listed, with the textual context in which it appeared.\")\n",
    "\n",
    "display(whole_box)\n",
    "\n",
    "for n in range(num_placenames):\n",
    "    placename_items[n].observe(changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now copy all the values from the checkboxes to the data, so you know which placenames you have approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the status of each checklist item to the data\n",
    "for n in range(num_placenames):\n",
    "\n",
    "    NEIndex_num = int(num_items[n].value)\n",
    "    approval_flag = placename_items[n].value\n",
    "\n",
    "    # Set the flag to match the checklist\n",
    "    for placename in placenames_df[\"NEIndex\"]:\n",
    "        if placename - NEIndex_num == 0:\n",
    "            placenames_df.loc[placenames_df[\"NEIndex\"] == NEIndex_num, \"Approval\"] = approval_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now visualise the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df[[\"NEIndex\", \"Placename\", \"Approval\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can extract the final list of distinct placenames that you have approved. While the names aren't sorted (though they could be), if you missed unselecting an NE on the checklist, this will help find it. All you need to do is go back to the checklist, unselect it, then run all other steps from there to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a unique list of the approved placenames\n",
    "approved_placenames = placenames_df[placenames_df[\"Approval\"] == True][\"Placename\"].unique()\n",
    "print(approved_placenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save this new data to a csv file. You have already defined the directory for your data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_placenames.csv\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving placename data to\", save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list, using savetxt from the numpy module\n",
    "np.savetxt(save_location, approved_placenames, delimiter=\", \", fmt=\"% s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a copy of what you consider to be a list of placenames from a select set of FtToHNL chapters. Each placename found and approved is only included once. For this notebook, this data does not include contextual data like how many times a placename was found in the original text, nor where it was found.\n",
    "\n",
    "You can easily change the notebook commands to search for placenames in different chapters or even look at a different single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Locations for the Placenames <a class=\"anchor\" id=\"section-findinglocs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a list of placenames from the text, the next step is to work out their location on Earth. For this you can use a combination of specialised lists of locations, gazzetteers and heuristics. The objective is to match every placename with the coordinates of a known location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to read the file of your placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_placenames.csv\"\n",
    "print(\"Reading:\", filename)\n",
    "\n",
    "# Set the specific path for the 'filename'\n",
    "data_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "data_filename = os.path.basename(data_location)\n",
    "\n",
    "# Read the csv file using pandas. This will place it in a dataframe format.\n",
    "placenames_df = pd.read_csv(data_location, encoding=\"utf-8\", header=None)\n",
    "placenames_df = placenames_df.rename(columns={placenames_df.columns[0]: \"Placename\"})\n",
    "placenames_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying States and Capitals <a class=\"anchor\" id=\"section-statescapitals\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some placenames, like *High Street* or *Maryborough*, may be very common across the world, or even in Australia. However, certain placenames refer to significant locations, like states, territories, large geographic features or capital cities. As such, if they are mentioned in a text, the placename is more likely to refer to the major location than a small town or village."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These significant locations are a finite set. They can be defined in a reference file that can be reused when reviewing the placenames of any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good point for you to start with is a file about locations like modern capital cities and countries. You can also add any historical locations of significance that are commonly relevant to your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"reference_location_data.csv\"\n",
    "print(\"Reading:\", filename)\n",
    "\n",
    "# Set the specific path for the 'filename'\n",
    "reference_location = os.path.normpath(os.path.join(reference_directory, filename))\n",
    "reference_filename = os.path.basename(reference_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Change this to a dictionary?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the reference data in a dataframe\n",
    "locref_df = pd.read_csv(reference_location, encoding=\"utf-8\", header=0)\n",
    "locref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each location, it is given a category (like _Continent_, _Country_, _Capital_ or _City_), the latitude and longitude coordinates and some indication of where in the world it is located. This _PartOf_ value may be a continent, a country or a region within an country (like a state or province). While you may only want to know the coordinates of any matching placename, the extra infomation will be vital in working out what is a relevant match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this allows some bias to be introduced into the data to suit your geolocation needs. For instance, _Perth_ is entered in this file as a city in the state of Western Australia, rather than one in Scotland. _Victoria_ is recorded as a state of Australia, rather than the capital of the Seychelles, or the capital of British Columbia, Canada. These locations are chosen because FtToHNL is mainly set in 18th century Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are researching historical texts, then you might have to consider adding various locations to this list to accomodate changes that may have occured over time. For instance\n",
    "* change of names, e.g., _New Amsterdam_ vs [_Gotham_](https://www.nypl.org/blog/2011/01/25/so-why-do-we-call-it-gotham-anyway) vs _New York_, _Constantinople_ vs _Istanbul_, _Ceylon_ vs _Sri Lanka_.\n",
    "* change of spelling, e.g., _Peking_ is a [romanized form](https://en.wikipedia.org/wiki/Chinese_postal_romanization) of _Beijing_\n",
    "* change of significance, like which city is a capital, e.g., _Bonn_ vs _Berlin_\n",
    "* names of previous geopolitical realms, like countries, kingdoms and empires, e.g., the _British Empire_, the _Zulu Kingdom_\n",
    "\n",
    "Obviously, you can't record every location everywhere, but the role of this reference file is to provide you with information on the major locations that might be relevant to your research, regardless of what textual document you are analysing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to see if any of the placenames from our selected chapters of FtToHNL match these locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each placename, we will be looking for the location that is the best match. If we find a location from our reference file with a name that matches a placename, then we record the geolocation data for the matching location as the best match for this placename. These records are kept independent from the reference file. Otherwise, don't record any geolocation data so we know to keep looking for the placename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the data about placenames and locations, once linked, as a list of dataframes\n",
    "geolocdata = []\n",
    "\n",
    "for placename in placenames_df[\"Placename\"]:\n",
    "\n",
    "    # Create a new geoloc entry about this placename\n",
    "    # [TO DO] Formally declare this as a dataframe?\n",
    "    new_geolocdata = {}\n",
    "\n",
    "    # Start a record for a placename\n",
    "    new_geolocdata[\"placename\"] = placename\n",
    "    new_geolocdata[\"locations\"] = {}  # Start with no location details\n",
    "    new_geolocdata[\"locations\"][\"best_match\"] = []  # Start with no matching location\n",
    "\n",
    "    # Match found in the reference data\n",
    "    if placename in list(locref_df[\"LocationName\"]):\n",
    "        # Copy the details from the reference file entry\n",
    "        new_geolocdata[\"locations\"][\"best_match\"] = locref_df[locref_df[\"LocationName\"] == placename]\n",
    "\n",
    "        print(\"+++ Found\", placename)\n",
    "    else:\n",
    "        print(\"--- Still looking for\", placename)\n",
    "\n",
    "    # Add the new placename data to the list\n",
    "    geolocdata.append(new_geolocdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, of the first ten placenames (depending on which ones you earlier approved), only _London_ found a matching location in the reference file. It turns out that it is the capital of the United Kingdom! No confusion there! However, you are still trying to match locations for the other placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the resulting geolocation records for the first 10 placenames\n",
    "geolocdata[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing complex about this matching. It expects the spelling of the location and the placename to be exactly the same. It doesn't try to accomodate differences in spelling or whether the placename is upper or lower case. Since these locations are supposed to be regarded as unambiguous, you don't want to try and accomodate such variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What locations did you end up finding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = [\n",
    "    # Convert each best_match dataframe into a single csv string\n",
    "    place[\"locations\"][\"best_match\"].to_csv(index=False, header=False).strip('\\n')\n",
    "    for place in geolocdata\n",
    "    if len(place[\"locations\"][\"best_match\"]) > 0\n",
    "]\n",
    "matched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now discard the data from the reference file. All the geolocation details for any matches will stay with the placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del locref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching a Gazetteer for Locations <a class=\"anchor\" id=\"section-searchgazzeteer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another source of possible matching locations is a gazetteer. Gazetteers are databases of geolocation records that can commonly be searched through the use of Application Programming Interfaces (APIs), a series of software functions that enable external software developers to send query commands to the databases. The database will then respond with the requested information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Open Street Map (OSM)](https://www.openstreetmap.org/about) is one such gazetteer that freely provides such a service. This notebook uses the [Nominatim API](https://nominatim.org/release-docs/develop/api/Search/) to send queries to the OSM gazetteer. These API queries look like website addresses (URLs) but allow you to include information about what locations you are looking for and various parameters about how you want to search for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, you can limit how many matching locations you want to know about. The default for the OSM is 10 records per query ([a maximum of 50](https://nominatim.org/release-docs/develop/api/Search/)) but let's limit it to only 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many (max) results do we want for each name?\n",
    "# [TO DO] Make this a user setting, defaulting to 5\n",
    "OSM_limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also want to set up various functions that process the responses from the OSM, converting it into a more manageable form for your purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send rate-limited requests that stay within n requests per second\n",
    "# [TO DO] add link to webpage about this\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=1)\n",
    "def osm_call_api(url):\n",
    "    response = requests.get(url)\n",
    "    return response\n",
    "\n",
    "# Convert a postcode from a string into a state abbreviation\n",
    "def postcode_to_state(postcodestr):\n",
    "    postcode = int(postcodestr)\n",
    "\n",
    "    if (1000 <= postcode <= 2599) or (2619 <= postcode < 2899) or (2921 <= postcode <= 2999):\n",
    "        return \"NSW\"\n",
    "    elif (200 <= postcode <= 299) or (2600 <= postcode <= 2618) or (2900 <= postcode <= 2920):\n",
    "        return \"ACT\"\n",
    "    elif (3000 <= postcode <= 3999) or (8000 <= postcode <= 8999):\n",
    "        return \"VIC\"\n",
    "    elif (4000 <= postcode <= 4999) or (9000 <= postcode < 9999):\n",
    "        return \"QLD\"\n",
    "    elif 5000 <= postcode <= 5999:\n",
    "        return \"SA\"\n",
    "    elif (6000 <= postcode <= 6797) or (6800 <= postcode <= 6999):\n",
    "        return \"WA\"\n",
    "    elif 8000 <= postcode <= 8999:\n",
    "        return \"TAS\"\n",
    "    elif 7000 <= postcode <= 7999:\n",
    "        return \"TAS\"\n",
    "    elif 800 <= postcode <= 999:\n",
    "        return \"NT\"\n",
    "    # Some postcodes are special cases\n",
    "    elif postcode == 2899:\n",
    "        return \"Norfolk Island\"  # Coded as NSW\n",
    "    elif postcode == 6798:\n",
    "        return \"Christmas Island\"  # Coded as WA\n",
    "    elif postcode == 6799:\n",
    "        return \"Cocos (Keeling) Islands\"  # Coded as WA\n",
    "    elif postcode == 9999:\n",
    "        return \"North Pole\"  # Coded as VIC for Santa mail\n",
    "\n",
    "    # Fallback\n",
    "    return postcodestr\n",
    "\n",
    "# Format the api response to make comparison easier\n",
    "def osm_format_response(input):\n",
    "\n",
    "    # OSM tends to provide the full address as the name of a location\n",
    "    # Shorten the name and extract the country name, if any\n",
    "    hyper_location = None\n",
    "    short_location = input[\"display_name\"]  # default to full address\n",
    "\n",
    "    if input[\"display_name\"].find(\",\"):\n",
    "        # Break up the address\n",
    "        namesplit = input[\"display_name\"].split(\",\")\n",
    "        short_location = namesplit[0].lstrip().rstrip()\n",
    "        # Extract the rightmost term from the split\n",
    "        hyper_location = namesplit[len(namesplit) - 1].lstrip().rstrip()\n",
    "        # Change to an Australian state name, rather than Australia\n",
    "        if hyper_location == \"Australia\" and len(namesplit) > 2:\n",
    "            hyper_location = namesplit[len(namesplit) - 2].lstrip().rstrip()\n",
    "            # Change postcodes into states\n",
    "            if hyper_location.isdigit() and len(hyper_location) == 4:\n",
    "                hyper_location = postcode_to_state(hyper_location)\n",
    "\n",
    "    # Keep the field names consistent between gazetteer records\n",
    "    response = {\n",
    "        \"LocationName\": str(short_location),\n",
    "        \"Category\": str(input[\"type\"]),\n",
    "        \"Latitude\": input[\"lat\"],\n",
    "        \"Longitude\": input[\"lon\"],\n",
    "        \"PartOf\": str(hyper_location),\n",
    "        \"Gazetteer\": \"OSM\",\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now search for any unknown placename by sending the OSM a query.  You won't need to search for any placenames that you have already found an unambiguous location for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every placename in our list\n",
    "for place in geolocdata:\n",
    "\n",
    "    # Already found an unambiguous location, so skip to the next placename\n",
    "    if len(place[\"locations\"][\"best_match\"]) > 0:\n",
    "        continue\n",
    "\n",
    "    placename = place[\"placename\"]\n",
    "\n",
    "    # Query the OSM database\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?q={placename}&format=json&limit={OSM_limit}\"\n",
    "    response = osm_call_api(url)\n",
    "    response_dict = json.loads(response.text)\n",
    "\n",
    "    # Start a list of possible candidate locations for this placename\n",
    "    place[\"locations\"][\"candidates\"] = None\n",
    "\n",
    "    # [TO DO] Remove the print commands, or do we need it so the user has some idea that something is happening?\n",
    "    # If no results found, then skip to the next placename\n",
    "    if len(response_dict) == 0:\n",
    "        print(\"--- Still looking for\", placename)\n",
    "        continue\n",
    "        \n",
    "    print(\"+++ Found\", placename)\n",
    "        \n",
    "    # Save the candidate locations for later processing\n",
    "    new_candidates = []\n",
    "\n",
    "    # Handle results found\n",
    "    for response_record in response_dict:\n",
    "        # Tidy up the responses to the query, discarding any data you don't need\n",
    "        # Add the data to a dataframe\n",
    "        cleaned_response = pd.DataFrame([osm_format_response(response_record)])\n",
    "        \n",
    "        # The cleaned_response should be in the form you want to keep.\n",
    "        new_candidates.append(cleaned_response)\n",
    "\n",
    "    # Add the results for this placename to the geoloc dataframe\n",
    "    place[\"locations\"][\"candidates\"] = new_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OSM provides a lot of information in reponse to each query. Not all of it is needed, so you will need to prune it, only keeping the data required for later processing.\n",
    "\n",
    "For instance, this is the full response to the OSM query for the last placename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the full address is given as the name for each location listed. The fields use different names to your previously matched geolocation data. Not all this data is beneficial to you, so the notebook has only kept the following data about each candidate location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last location in the last OSM query response\n",
    "cleaned_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than keeping the entire address, the name of the matching location has been shortened to the leftmost phrase in the address. Likewise, the _PartOf_ value is generally extracted from the rightmost phrase in the address. However, for FtToHNL, you need to discriminate between locations in different states of Australia. For that reason, the notebook code will change to the second rightmost phrase in the address for Australian locations. This is normally the name of the state, but sometimes is an Australian postcode so the notebook needs to work out which state corresponds to which postcode and use that instead. For your research, you may need to similarly tailor how to handle the output from any gazetteer you use to your needs. \n",
    "\n",
    "You will also notice that the OSM output has a _class_ and a _type_ value that are both similar to the _Category_ value from our reference file and the NER, yet also have distinctly different values. This is a common issue. Just like there are no universal set of categories used by NER systems, there is no universal set for gazetteers. In fact, even the OSM doesn't have a clearly defined set of values. Some values are very general, e.g., _locality_ or _administrative_, but others are very specific, e.g., _cafe_ or  _place_of_worship_. However, it is important contextual information to use when evaluating what locations may be relevant. _type_ is more specific (or fine-grained) than _class_ so it is used as the _Category_ for any OSM matching locations.\n",
    "\n",
    "The OSM also provides an _importance_ value to each query response. This is not useful information because it relates to the importance of the location, based on [how often a related page is linked to in Wikipedia](https://github.com/osm-search/wikipedia-wikidata), rather than the quality of the search match. The ranking order of the OSM locations in a query reponse [can be customised](https://nominatim.org/release-docs/develop/customize/Ranking/), but this notebook just uses the defaults settings. \n",
    "\n",
    "It should also be noted that the Nominatim OSM queries don't try to find exact matches for the placenames. Nominatim isn't very clear about how exactly they match the placenames, but partial matches seem to be made. For instance, _Mount Royal_ matches with locations called _Mount Royal_ as well as _Mont-Royal_. Likewise, _South Cape_ matches with _Cape_. These indicate that when searching for multi-word expressions, the Nominatim API will match both the entire expression ans well as single words within the expression. There is no clear instructions on how you can narrow down the search to only look for exact matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How helpful was the OSM gazetteer for you? Where have you found locations for placenames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = [\n",
    "    place[\"placename\"]\n",
    "    for place in geolocdata\n",
    "    if len(place[\"locations\"][\"best_match\"]) > 0 or place[\"locations\"][\"candidates\"] != None\n",
    "]\n",
    "matched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What placenames have you still not found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_data = [\n",
    "    place[\"placename\"]\n",
    "    for place in geolocdata\n",
    "    if (\n",
    "        # No non-ambiguous location\n",
    "        len(place[\"locations\"][\"best_match\"]) == 0\n",
    "        # No candidate Locations\n",
    "        and (place[\"locations\"][\"candidates\"] is None or place[\"locations\"][\"candidates\"] == [])\n",
    "    )\n",
    "]\n",
    "unmatched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Open Street Map is a wonderful resource, it focusses on current names of geographic locations. If the original source of your placenames was not written in the recent decades, then the OSM may not know the appropriate names of locations for the time of the document. Furthermore, some of the locations may no longer exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to also look up a historical gazetteer, like that provided by [TLCMap](https://tlcmap.org/about/devstrategy.php#whatwhy), rather than just rely on the OSM. You can then look the top matching locations from both and decide if any seem appropriate for your placenames.\n",
    "\n",
    "TLCMap is designed to enable humanities researchers to use, create and integrate datasets relating to spatial data and thematic mapping. The focus is on using Australian cultural and historical data. A core part of that is the establishment of the [Gazetteer of Historical Australian Places (GHAP, formerly 'Placenames')](https://tlcmap.org/ghap/), which includes data based on the [Australian National Placename Survey (ANPS)](https://www.anps.org.au/) and layers of cultural information contributed by researchers, institutions and the community.\n",
    "\n",
    "Like the OSM API, the [TLCMap API](https://tlcmap.org/guides/ghap/#ws) has various options, like which type of search to use and whether to search any data in the database entered by the public, rather than only that which has been verified or entered by experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, you will look for exact matches between the placenames and the locations, and not consider any publicly entered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which order to do different searches for known locations? \n",
    "# Alt values: 'exact', 'fuzzy', 'contains'\n",
    "search_type = \"exact\"\n",
    "\n",
    "# Flag whether to use data provided by the public\n",
    "search_public_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for the OSM, you can limit how many results you want to examine. The TLCMap default for this notebook is 1 but for now, you should use the same limit as for the OSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many (max) results do we want for each name?\n",
    "# [TO DO] Make this a user setting, defaulting to 5\n",
    "TLCMap_limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for the OSM, you will need a few functions to query the the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a url to query the tlcmap/ghap API.\n",
    "# - placename: the place we're trying to locate\n",
    "# - search_type: what search type to use (accepts one of ['contains','fuzzy','exact'])\n",
    "# ref: https://www.tlcmap.org/guides/ghap/#ws\n",
    "def tlc_build_url(placename: str, search_type: str, search_public_data: bool = False) -> str:\n",
    "    \n",
    "    safe_placename = urllib.parse.quote(placename.strip().lower())\n",
    "\n",
    "    url = f\"https://tlcmap.org/ghap/search?\"\n",
    "\n",
    "    if search_type == \"fuzzy\":\n",
    "        url += f\"fuzzyname={safe_placename}\"\n",
    "    elif search_type == \"exact\":\n",
    "        url += f\"name={safe_placename}\"\n",
    "    elif search_type == \"contains\":\n",
    "        url += f\"containsname={safe_placename}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Search Australian National Placenames Survey provided data\n",
    "    url += \"&searchausgaz=on\"\n",
    "\n",
    "    # Search public provided data, this data could be unreliable\n",
    "    if search_public_data == True:\n",
    "        url += \"&searchpublicdatasets=on\"\n",
    "    else:\n",
    "        url += \"&searchpublicdatasets=off\"\n",
    "\n",
    "    # Retrieve data as JSON\n",
    "    url += \"&format=json\"\n",
    "\n",
    "    # Limit the number of results\n",
    "    url += \"&paging=\" + str(TLCMap_limit)\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "# Send rate-limited requests that stay within n requests per second\n",
    "# [TO DO] add link to webpage about this\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=1)\n",
    "def tlc_call_api(url):\n",
    "    r = requests.get(url)\n",
    "    if r.url == \"https://tlcmap.org/ghap/maxpaging\":\n",
    "        return None\n",
    "\n",
    "    # If the reply says the placename wasn't found, customise the JSON data for the reply\n",
    "    if r.content.decode() == \"No search results to display.\":\n",
    "        # This should have obviously just be an empty list of features, but TLCMap is badly behaved\n",
    "        response = json.loads('{\"type\": \"FeatureCollection\",\"metadata\": {},\"features\": []}')\n",
    "    # SUCCESS! Record the spatial data provided in the reply\n",
    "    elif r.ok:\n",
    "        response = r.json()  # get [lon, lat] for spatial matches\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Use TLCMap/GHAP API to check a placename\n",
    "def tlc_query_name(placename: str, search_type: str):\n",
    "    \n",
    "    url = tlc_build_url(placename, search_type, search_public_data)\n",
    "    if url:\n",
    "        return tlc_call_api(url)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the api response to make comparison easier\n",
    "def tlc_format_response(location_features):\n",
    "\n",
    "    locdata = {}  # formatted data\n",
    "\n",
    "    # Gather the locdata for one of the placename's locations\n",
    "    # If the value is missing, set a value or leave it empty.\n",
    "    if len(location_features):\n",
    "        if \"placename\" in location_features[\"properties\"]:\n",
    "            locdata[\"LocationName\"] = location_features[\"properties\"][\"placename\"].lstrip().rstrip()\n",
    "        else:\n",
    "            locdata[\"LocationName\"] = \"Unknown Location\"\n",
    "\n",
    "        if \"feature_term\" in location_features[\"properties\"]:\n",
    "            locdata[\"Category\"] = location_features[\"properties\"][\"feature_term\"].lstrip().rstrip()\n",
    "        else:\n",
    "            locdata[\"Category\"] = None\n",
    "\n",
    "        if \"longitude\" in location_features[\"properties\"]:\n",
    "            locdata[\"Longitude\"] = location_features[\"properties\"][\"longitude\"]\n",
    "        else:\n",
    "            locdata[\"Longitude\"] = \"\"\n",
    "\n",
    "        if \"latitude\" in location_features[\"properties\"]:\n",
    "            locdata[\"Latitude\"] = location_features[\"properties\"][\"latitude\"]\n",
    "        else:\n",
    "            locdata[\"Latitude\"] = \"\"\n",
    "\n",
    "        if \"state\" in location_features[\"properties\"]:\n",
    "            locdata[\"PartOf\"] = location_features[\"properties\"][\"state\"].lstrip().rstrip()\n",
    "        else:\n",
    "            locdata[\"PartOf\"] = None\n",
    "\n",
    "        locdata[\"Gazetteer\"] = \"TLCMap\"\n",
    "\n",
    "        # Keep the names consistent between gazetteer records\n",
    "        response = {\n",
    "            \"LocationName\": str(locdata[\"LocationName\"]),\n",
    "            \"Category\": str(locdata[\"Category\"]),\n",
    "            \"Latitude\": locdata[\"Latitude\"],\n",
    "            \"Longitude\": locdata[\"Longitude\"],\n",
    "            \"PartOf\": str(locdata[\"PartOf\"]),\n",
    "            \"Gazetteer\": \"TLCMap\",\n",
    "        }\n",
    "    else:\n",
    "        response = None\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now search the TLCMap for locations matching the same placenames you previously searched for in the OSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every placename in our list\n",
    "for place in geolocdata:\n",
    "\n",
    "    # Already found an unambiguous location, so skip to the next placename\n",
    "    if len(place[\"locations\"][\"best_match\"]) > 0:\n",
    "        continue\n",
    "\n",
    "    placename = place[\"placename\"]\n",
    "    \n",
    "    # Query the OSM database\n",
    "    response = tlc_query_name(placename, search_type)\n",
    "    \n",
    "    # [TO DO] Remove the print commands, or do we need it so the user has some idea that something is happening?\n",
    "    # Handle no results found, skip to the next placename\n",
    "    if (response is None or response[\"features\"] == []) :\n",
    "        print(\"--- Still looking for\", placename)\n",
    "        continue\n",
    "\n",
    "    print(\"+++ Found\", placename)\n",
    "    \n",
    "    # Save the possible locations for later processing\n",
    "    data_frames = []\n",
    "\n",
    "    # Handle results found\n",
    "    for response_record in response[\"features\"]:\n",
    "        #  Use this to look at a reduced set of data from the results\n",
    "        cleaned_response = pd.DataFrame([tlc_format_response(response_record)])\n",
    "\n",
    "        # Add the data to a dataframe\n",
    "        data_frames.append(cleaned_response)\n",
    "\n",
    "    # Add the results to the geoloc dataframe\n",
    "    # Review the outcomes later\n",
    "    # Make sure you don't write over any candidates previously added from another gazetteer.\n",
    "    if place[\"locations\"][\"candidates\"] is None:\n",
    "        place[\"locations\"][\"candidates\"] = data_frames\n",
    "    else:\n",
    "        place[\"locations\"][\"candidates\"] = place[\"locations\"][\"candidates\"] + data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Show the output and discuss from OSM, so the context of the processing can be understood__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the OSM, TLCMap provides a lot of information in reponse to each query. Not all of it is needed, so you will need to prune it, only keeping the data required for later processing.\n",
    "\n",
    "Each response has a set of metadata that is kept independent from the spatial data (called _features_) for each matched location. For instance, this is the datalocation response to the TLCMap query for the last matched placename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the notebook code cleans up the data, just keeping what is needed. The _PartOf_ value can be taken from _state_ field and the _feature_term_ is used as the _Category_ value. Once again, the TLCMap has its only set of _Category_ values. If you allow the search to include data entered by the general public, then this is prone to having spelling errors and being even more inconsistent. However, it is still helpful to provide context to you as a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last matched location in the last TLCMap query response\n",
    "cleaned_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How helpful have the gazetteers been for you? What placenames have you still not found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_data = [\n",
    "    place[\"placename\"]\n",
    "    for place in geolocdata\n",
    "    if (\n",
    "        # No non-ambiguous location\n",
    "        len(place[\"locations\"][\"best_match\"]) == 0\n",
    "        # No candidate Locations\n",
    "        and (place[\"locations\"][\"candidates\"] is None or place[\"locations\"][\"candidates\"] == [])\n",
    "    )\n",
    "]\n",
    "unmatched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have information about locations for most of the placenames. Some locations are not ambiguous and are already recorded as a best match. Others came from gazetteers and are just candidate locations - you haven't worked out yet if any are suitable.\n",
    "\n",
    "For instance, this what data you have found for the first ten matched placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which placenames have matched locations\n",
    "matched_data = [\n",
    "    place\n",
    "    for place in geolocdata\n",
    "    if (\n",
    "        # Non-ambiguous location\n",
    "        len(place[\"locations\"][\"best_match\"]) != 0\n",
    "        # Candidate Locations\n",
    "        or (place[\"locations\"][\"candidates\"] is not None and place[\"locations\"][\"candidates\"] != [])\n",
    "    )\n",
    "]\n",
    "\n",
    "# For each of the first ten placenames\n",
    "for place in matched_data[0:9]:\n",
    "    print(\"\\n+++ \", place[\"placename\"])\n",
    "\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        # Unambiguous best match already known\n",
    "        pprint(place[\"locations\"][\"best_match\"])\n",
    "    else:\n",
    "        # Any candidate locations you have found\n",
    "        if place[\"locations\"][\"candidates\"] is not None:\n",
    "            pprint(pd.concat(place[\"locations\"][\"candidates\"], ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Just keeping the complete output for now. Remove before the workshop.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = [\n",
    "    place\n",
    "    for place in geolocdata\n",
    "    if (\n",
    "        len(place[\"locations\"][\"best_match\"]) != 0\n",
    "        or (place[\"locations\"][\"candidates\"] is not None and place[\"locations\"][\"candidates\"] != [])\n",
    "    )\n",
    "]\n",
    "\n",
    "all_locations = []\n",
    "\n",
    "for place in matched_data:\n",
    "    placename = place[\"placename\"]\n",
    "    locations = []\n",
    "\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        # Unambiguous best match already known\n",
    "        locations = place[\"locations\"][\"best_match\"]\n",
    "    else:\n",
    "        # Any candidate locations you have found\n",
    "        if place[\"locations\"][\"candidates\"] is not None:\n",
    "            locations = pd.concat(place[\"locations\"][\"candidates\"], ignore_index=True)\n",
    "    print(\"===> \", placename)\n",
    "    print(locations)\n",
    "    all_locations = all_locations + [locations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is now to work out which of these candidate locations is most suitable.\n",
    "\n",
    "A few heuristics can be used to flag those candidate locations with key features that can be used to help rank and select the locations.\n",
    "\n",
    "One way is to acknowledge if multiple candidate locations have similar coordinates. However, sometimes there are no coordinates for a location, which can complicate the comparisons. The solution is to default to the value of 0 for missing data. It is not perfect but in most cases it is adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lists of coordinates can be sorted,\n",
    "# When given a coordinate as a string value, convert it to an integer\n",
    "# This is required when the value is sometimes not an number, like NA or \"\".\n",
    "def sorting_coord(coord):\n",
    "\n",
    "    # Make all numbers into integers\n",
    "    if type(coord) in [float, int]:\n",
    "        return int(coord)\n",
    "    # Set to 0 if missing data\n",
    "    elif type(coord) == str and coord == \"\":\n",
    "        return 0\n",
    "    elif type(coord) != str and coord.isna():\n",
    "        return 0\n",
    "    # Convert to a float then an integer\n",
    "    return int(float(coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two sets of coordinates\n",
    "# Return true if they are both within 1 point of each other for both Lat and Lon\n",
    "def compare_coords(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    # Cannot compare missing values\n",
    "    if lat1.item() == \"\" or lat2.item() == \"\" or lon1.item() == \"\" or lon2.item() == \"\":\n",
    "        return False\n",
    "\n",
    "    # Can only compare integers\n",
    "    lat1_int = int(float(lat1.item()))\n",
    "    lon1_int = int(float(lon1.item()))\n",
    "    lat2_int = int(float(lat2.item()))\n",
    "    lon2_int = int(float(lon2.item()))\n",
    "\n",
    "    # Is Coord1 within 1 point of Coord2?\n",
    "    # e.g., -47.5 is close to -46.5 and -48.5\n",
    "    return (lat1_int in range(lat2_int - 1, lat2_int + 2, 1)) and (lon1_int in range(lon2_int - 1, lon2_int + 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another consideration is to recognise which locations are in certain countries or regions which you know are relevant to the original document. This should be indicated in the _PartOf_ field of the data you have matched. Most of FtToHNL is set in Australia but references are made to parts of Great Britain, so that is what you should focus on. A list of places-of-interest can be made for Australia and one for Great Britain. While these are very finite sets of places, you need to make sure you match the spelling in the gazetteers you have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag locations in Australia or Britain\n",
    "aus_states = {\n",
    "    \"AUSTRALIA\",\n",
    "    \"NSW\",\n",
    "    \"VIC\",\n",
    "    \"QLD\",\n",
    "    \"TAS\",\n",
    "    \"WA\",\n",
    "    \"NT\",\n",
    "    \"SA\",\n",
    "    \"ACT\",\n",
    "    \"NEW SOUTH WALES\",\n",
    "    \"VICTORIA\",\n",
    "    \"QUEENSLAND\",\n",
    "    \"TASMANIA\",\n",
    "    \"WESTERN AUSTRALIA\",\n",
    "    \"SOUTH AUSTRALIA\",\n",
    "    \"NORTHERN TERRITORY\",\n",
    "    \"AUSTRALIAN CAPITAL TERRITORY\",\n",
    "}\n",
    "gb_states = {\n",
    "    \"BRITAIN\",\n",
    "    \"UK\",\n",
    "    \"GB\",\n",
    "    \"GREAT BRITAIN\",\n",
    "    \"UNITED KINGDOM\",\n",
    "    \"BRITISH ISLES\",\n",
    "    \"ENGLAND\",\n",
    "    \"WALES\",\n",
    "    \"CYMRU\",\n",
    "    \"SCOTLAND\",\n",
    "    \"IRELAND\",\n",
    "    \"NORTHERN IRELAND\",\n",
    "    \"IRE / IRELAND\",\n",
    "    \"EIRE\",\n",
    "    \"EIRE / IRELAND\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can go through each placename and its candidate locations and flag whether they correspond to any of these criteria. A distinction is made between whether any two locations with similar coordinates were found in the same gazetteer or a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in matched_data:\n",
    "    placename = place[\"placename\"]\n",
    "    #locations = []\n",
    "    if (\n",
    "        # No unambiguous location found\n",
    "        len(place[\"locations\"][\"best_match\"]) == 0\n",
    "        # Candidate locations found\n",
    "        and place[\"locations\"][\"candidates\"] is not None\n",
    "        and len(place[\"locations\"][\"candidates\"]) > 1\n",
    "    ):\n",
    "\n",
    "        # Sort the locations by Latitude & Longitude\n",
    "        sorted_candidates = sorted(\n",
    "            place[\"locations\"][\"candidates\"],\n",
    "            key=lambda x: [\n",
    "                sorting_coord(x[\"Latitude\"].item()),\n",
    "                sorting_coord(x[\"Longitude\"].item()),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        prev_location = {}\n",
    "\n",
    "        # Flag the candidates according to the heuristics\n",
    "        for candidate in sorted_candidates:\n",
    "            rank_flags = [] # List of all heuristic flags\n",
    "\n",
    "            # Flag locations in Australia or Britain\n",
    "            # [TO DO] Make these lines into a function to pull the Aus/GB code out of the main code.\n",
    "            partings = candidate[\"PartOf\"].item().upper()\n",
    "            if partings in aus_states:\n",
    "                rank_flags.append(\"Australia\")\n",
    "            elif partings in gb_states:\n",
    "                rank_flags.append(\"Britain\")\n",
    "\n",
    "            # Flag coords in multiple gazetteers\n",
    "            if len(prev_location) > 0:\n",
    "                # Is this candidate location near to the previous candidate location?\n",
    "                coord_flag = compare_coords(\n",
    "                    prev_location[\"Latitude\"],\n",
    "                    prev_location[\"Longitude\"],\n",
    "                    candidate[\"Latitude\"],\n",
    "                    candidate[\"Longitude\"],\n",
    "                )\n",
    "                # Dupl_Gaz2: Matching coords in candidate locations from 2 gazetteers\n",
    "                if coord_flag and prev_location[\"Gazetteer\"].item() != candidate[\"Gazetteer\"].item():\n",
    "                    rank_flags.append(\"Dupl_2Gaz\")\n",
    "                # Dupl_Gaz1: Matching coords in candidate locations from 1 gazetteer\n",
    "                elif coord_flag:\n",
    "                    rank_flags.append(\"Dupl_1Gaz\")\n",
    "\n",
    "            prev_location = candidate\n",
    "\n",
    "            # rank_flags has to be converted to a single string to print it, rather than a list, \n",
    "            # because candidate is a Pandas dataframe\n",
    "            candidate[\"RankFlags\"] = pd.Series(\",\".join(rank_flags))\n",
    "            print(\n",
    "                \" ** \",\n",
    "                candidate[\"LocationName\"].item(),\n",
    "                \",\",\n",
    "                partings,\n",
    "                \",[\",\n",
    "                candidate[\"RankFlags\"].item(),\n",
    "                \"]\",\n",
    "            )\n",
    "\n",
    "        # Update the geoloc data\n",
    "        place[\"locations\"][\"candidates\"] = sorted_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the heuristics flags to re-sort the candidates and select the best one. The first step is to rank the possible flag combinations in an order of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish how to rank the heuristic flags\n",
    "sortorder = [\n",
    "    \"Australia,Dupl_2Gaz\",  # In Australia, found in 2 gazetteers\n",
    "    \"Australia,Dupl_1Gaz\",  # In Australia, found more than once in 1 gazetteer\n",
    "    \"Australia\",  # In Australia, found only once\n",
    "    \"Britain,Dupl_2Gaz\",  # In Great Britain, found in 2 gazetteers\n",
    "    \"Dupl_2Gaz\",  # Not in Australia or Great Britain, found in 2 gazetteers\n",
    "    \"Britain,Dupl_1Gaz\",  # In Great Britain, found more than once in 1 gazetteer\n",
    "    \"Britain\",  # In Great Britain, found only once\n",
    "    \"Dupl_1Gaz\",  # Not in Australia or Great Britain, found more than once in 1 gazetteer\n",
    "    \"\",  # Not in Australia or Great Britain, found only once\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use this order to rank the candidate locations against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the candidates\n",
    "for place in geolocdata:\n",
    "    placename = place[\"placename\"]\n",
    "    #pprint(placename)\n",
    "\n",
    "    if (\n",
    "        # No unmabiguous location\n",
    "        len(place[\"locations\"][\"best_match\"]) == 0\n",
    "        # More than one candidate location\n",
    "        and place[\"locations\"][\"candidates\"] is not None\n",
    "        and len(place[\"locations\"][\"candidates\"]) > 1\n",
    "    ):\n",
    "        candidates = place[\"locations\"][\"candidates\"]\n",
    "\n",
    "        sorted_candidates = []\n",
    "        # for each combination of heuristic flags\n",
    "        for heuristic in sortorder:\n",
    "            # Get the candidate locations matching this ranking heuristic\n",
    "            for candidate in candidates:\n",
    "                if candidate[\"RankFlags\"].item() == heuristic:\n",
    "                    # Add the candidate to the sorted list\n",
    "                    sorted_candidates = sorted_candidates + [candidate]\n",
    "        # Prepare a version for printing\n",
    "        locations = pd.concat(sorted_candidates, ignore_index=True)\n",
    "        #pprint(locations)\n",
    "\n",
    "        # Update the geoloc data with the sorted candidates\n",
    "        place[\"locations\"][\"candidates\"] = sorted_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first ten placenames now have better ranked candidate locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which placenames have matched candidate locations\n",
    "matched_data = [\n",
    "    place\n",
    "    for place in geolocdata\n",
    "    if (\n",
    "        # No unambiguous location\n",
    "        len(place[\"locations\"][\"best_match\"]) == 0\n",
    "        # Candidate Locations\n",
    "        and (place[\"locations\"][\"candidates\"] is not None and place[\"locations\"][\"candidates\"] != [])\n",
    "    )\n",
    "]\n",
    "\n",
    "# For each of the first ten placenames\n",
    "for place in matched_data[0:9]:\n",
    "    print(\"\\n+++ \", place[\"placename\"])\n",
    "\n",
    "    # Any candidate locations you have found\n",
    "    if place[\"locations\"][\"candidates\"] is not None:\n",
    "        pprint(pd.concat(place[\"locations\"][\"candidates\"], ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the candidates are sorted, the best match can be selected. The most obvious choice is the highest ranked candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in geolocdata:\n",
    "    placename = place[\"placename\"]\n",
    "    locations = []\n",
    "    # Already have the best match\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        locations = place[\"locations\"][\"best_match\"]\n",
    "    else:\n",
    "        if place[\"locations\"][\"candidates\"] is not None and len(place[\"locations\"][\"candidates\"]) > 0:\n",
    "            # Presume the best match has the top rank\n",
    "            top_candidate = place[\"locations\"][\"candidates\"][0]\n",
    "            # Don't include the values for Gazetteer or RankFlags\n",
    "            place[\"locations\"][\"best_match\"] = top_candidate.loc[\n",
    "                :, ~top_candidate.columns.isin([\"Gazetteer\", \"RankFlags\"])\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have geolocation data for most of the placenames!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Tidy up this output by copying it into an array__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in geolocdata:\n",
    "    print(\"+++ \",place[\"placename\"])\n",
    "    locations = []\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        pprint(place[\"locations\"][\"best_match\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the top ranked candidate may still not be the best. The second ranked candidate may actually be the same rank as the top ranked one, but it simply might have a lesser position due to the earlier ranking based on the latitude and longitude values (to find the matching candidates for the ranking). For this reason, the final stage of the processing is left to the user, showing them the best candidate location, but allowing them to select an alternate candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways the user interface could be done. The first is very verbose and shows all the candidate locations of all placenames to the user. Using checkboxes, the user can then select what they think is the best match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the candidates could be presented as drop-down accordions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for place in geolocdata:\n",
    "    placename = place[\"placename\"]\n",
    "\n",
    "    # If there isn't a best match, then there aren't any candidates\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "\n",
    "        best_match = place[\"locations\"][\"best_match\"]\n",
    "        best_match_index = 0\n",
    "\n",
    "        # Must be at least one candidate\n",
    "        if (\n",
    "            \"candidates\" in place[\"locations\"].keys()\n",
    "            and place[\"locations\"][\"candidates\"] is not None\n",
    "            and len(place[\"locations\"][\"candidates\"]) >= 1\n",
    "        ):\n",
    "\n",
    "            # Create a RadioButton for each candidate location\n",
    "            candidates = place[\"locations\"][\"candidates\"]\n",
    "            candidate_buttons = [\n",
    "                widgets.RadioButtons(\n",
    "                    layout={\"width\": \"max-content\"},\n",
    "                    options=[\n",
    "                        candidate[\"LocationName\"].item()\n",
    "                        + \", \"\n",
    "                        + candidate[\"PartOf\"].item()\n",
    "                        + \" (\"\n",
    "                        + str(candidate[\"Latitude\"].item())\n",
    "                        + \",\"\n",
    "                        + str(candidate[\"Longitude\"].item())\n",
    "                        + \")\"\n",
    "                        for candidate in candidates\n",
    "                    ],\n",
    "                )\n",
    "            ]\n",
    "            # Add a button for \"None of the above\"\n",
    "            candidate_buttons[0].options = candidate_buttons[0].options + tuple([\"None of the above\"])\n",
    "            place[\"locations\"][\"candidate_buttons\"] = candidate_buttons\n",
    "\n",
    "            # Select the button for the current bestmatch\n",
    "            place[\"locations\"][\"candidate_buttons\"][0].index = best_match_index\n",
    "\n",
    "            # Create a HBox for the Placename\n",
    "            placename_box = widgets.HBox(\n",
    "                [\n",
    "                    widgets.Label(\n",
    "                        \"'\"\n",
    "                        + place[\"placename\"]\n",
    "                        + \"' matches best with: \"\n",
    "                        + best_match[\"LocationName\"].item()\n",
    "                        + \", \"\n",
    "                        + best_match[\"PartOf\"].item()\n",
    "                        + \" (\"\n",
    "                        + best_match[\"Latitude\"].item()\n",
    "                        + \", \"\n",
    "                        + best_match[\"Longitude\"].item()\n",
    "                        + \")\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create an Accordion (in a HBox) for the candidates\n",
    "            new_accordion = widgets.Accordion()\n",
    "            new_accordion.set_title(0, \"Expand to choose a different location\")\n",
    "            new_accordion.children = [widgets.HBox(place[\"locations\"][\"candidate_buttons\"])]\n",
    "            new_accordion.selected_index = None  # close the accordion at startup\n",
    "\n",
    "            # Put the HBoxes together in a VBox\n",
    "            whole_box = widgets.VBox([placename_box, new_accordion])\n",
    "\n",
    "            # Show it all!\n",
    "            # Note: this doesn't close one accordion if another is opened because they are in separate VBoxes\n",
    "            display(whole_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the selected candidates as the best matches. Account for placenames without any best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in geolocdata:\n",
    "\n",
    "    # If there isn't a best match, then there aren't any candidates\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "\n",
    "        best_match_index = 0\n",
    "\n",
    "        # Must be at least one candidate\n",
    "        if \"candidate_buttons\" in place[\"locations\"].keys():\n",
    "\n",
    "            # Extract the selection\n",
    "            best_match_index = place[\"locations\"][\"candidate_buttons\"][0].index\n",
    "            # Make sure the selection is a location\n",
    "            if best_match_index < len(place[\"locations\"][\"candidates\"]):\n",
    "                # Record the best match\n",
    "                top_candidate = place[\"locations\"][\"candidates\"][best_match_index]\n",
    "                # Just record the important columns for the best match\n",
    "                place[\"locations\"][\"best_match\"] = top_candidate.loc[\n",
    "                    :, ~top_candidate.columns.isin([\"Gazetteer\", \"RankFlags\"])\n",
    "                ]\n",
    "            else:\n",
    "                # None of the above\n",
    "                best_match = {\n",
    "                    \"LocationName\": \"No suitable location selected\",\n",
    "                    \"Category\": \"No suitable location selected\",\n",
    "                    \"Latitude\": \"\",\n",
    "                    \"Longitude\": \"\",\n",
    "                    \"PartOf\": \"No suitable location selected\",\n",
    "                }\n",
    "                place[\"locations\"][\"best_match\"] = pd.DataFrame([best_match])\n",
    "    else:\n",
    "        # Note that there is no best match\n",
    "        best_match = {\n",
    "            \"LocationName\": \"No location matched\",\n",
    "            \"Category\": \"No location matched\",\n",
    "            \"Latitude\": \"\",\n",
    "            \"Longitude\": \"\",\n",
    "            \"PartOf\": \"No location matched\",\n",
    "        }\n",
    "        place[\"locations\"][\"best_match\"] = pd.DataFrame([best_match])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the final geoloc data for output to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(geolocdata[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verbose output\n",
    "all_locations = []  # the processed data\n",
    "for place in geolocdata:\n",
    "    placename = place[\"placename\"]\n",
    "    pprint(placename)\n",
    "\n",
    "    # Reformat the data about the best match\n",
    "    best_locations = []\n",
    "    # All placenames should now have a best_match\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        for candidate in place[\"locations\"][\"best_match\"]:\n",
    "            if place[\"locations\"][\"best_match\"][candidate].item != \"\":\n",
    "                best_locations = best_locations + [place[\"locations\"][\"best_match\"][candidate].item()]\n",
    "\n",
    "    # Reformat the data about the candidate locations\n",
    "    locations = []\n",
    "    if \"candidates\" in place[\"locations\"].keys() and place[\"locations\"][\"candidates\"] != []:\n",
    "        short_candidates = []\n",
    "        for candidate in place[\"locations\"][\"candidates\"]:\n",
    "            # Select which columns to output\n",
    "            short_candidates = short_candidates + [candidate.loc[:, candidate.columns != \"RankFlags\"]]\n",
    "        # Merge the dataframe values into a more human-readible format for now,\n",
    "        # though this is not a comma-separated format\n",
    "        locations = pd.concat(short_candidates, ignore_index=True)\n",
    "\n",
    "    # Put this all together\n",
    "    new_record = [[\"placename\", placename], [\"best_match\", best_locations], [\"candidates\", locations]]\n",
    "    all_locations = all_locations + [new_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in all_locations[0:10]:\n",
    "    pprint(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the geolocdata as a messy combination of array rows and dataframnes\n",
    "filename = \"FtToHNL_matchedlocations.data\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving to location data to \", save_location)\n",
    "\n",
    "# Save the list\n",
    "np.savetxt(save_location, geolocdata, delimiter=\", \", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output\n",
    "# Only use selected columns from the best match in csv format\n",
    "\n",
    "# Make a new array of records\n",
    "geoloc_output = [[\"Placename\", \"PartOf\", \"Latitude\", \"Longitude\"]]\n",
    "for place in geolocdata:\n",
    "    # Only output those placenames with a best match location\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        placename = place[\"placename\"]\n",
    "        location = place[\"locations\"][\"best_match\"]\n",
    "        print(\"Formatting \" + placename)\n",
    "        pprint(location)\n",
    "\n",
    "        # Convert Lat/Long into floats, rather than strings\n",
    "        if type(location[\"Latitude\"]) == float:\n",
    "            latitude = location[\"Latitude\"].item()\n",
    "        elif location[\"Latitude\"].item() != \"\":\n",
    "            latitude = float(location[\"Latitude\"].item())\n",
    "        else:\n",
    "            latitude = \"\"\n",
    "        if type(location[\"Longitude\"]) == float:\n",
    "            longitude = location[\"Longitude\"].item()\n",
    "        elif location[\"Longitude\"].item() != \"\":\n",
    "            longitude = float(location[\"Longitude\"].item())\n",
    "        else:\n",
    "            longitude = \"\"\n",
    "\n",
    "        # Add this record to the list\n",
    "        geoloc_output.append([placename, location[\"PartOf\"].item(), latitude, longitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_geolocdata.csv\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving to location data to \", save_location)\n",
    "\n",
    "# Save the list\n",
    "np.savetxt(save_location, geoloc_output, delimiter=\", \", fmt=\"% s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
