{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATAP Notebook for the Geolocation project\n",
    "\n",
    "This notebook helps you access the Geolocation tools in a Python development environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "* [Premise](#section-premise)\n",
    "* [Requirements](#section-requirements) \n",
    "* [Data Preparation](#section-datapreparation)\n",
    "* [Identifying Placenames in Text](#section-identifyingplacenames)\n",
    " * [Named Entity Recognition](#section-ner)\n",
    " * [Reviewing Candidate Placenames](#section-reviewplacenames)\n",
    "* [Finding Locations for Placenames](#section-findinglocs)\n",
    " * [Identifying States and Capitals](#section-statescapitals)\n",
    " * [Searching a Gazzetteer for Locations](#section-searchgazetteer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise <a class=\"anchor\" id=\"section-premise\"></a>\n",
    "*This section explains the Geolocation project and tools.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geolocation project relates to doctoral research done by [Fiannuala Morgan](https://finnoscarmorgan.github.io/) at the Australian National University. It uses software to identify placenames in archived historical texts, then compares them to data about known locations to identify where the placenames may be located. \n",
    "\n",
    "This notebook is designed to allow you to perform similar operations on textual documents.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "It will teach you how to\n",
    "<ul>\n",
    "    <li>use the spaCy library to identify and classify Named Entities (NEs)</li>\n",
    "    <li>identify multi-word expressions (MWE) that are NEs</li>\n",
    "    <li>search for spatial data about specific locations or places in gazetteers of such data</li>\n",
    "    <li>determine which locations are referred to by placenames, based on the context in which they are used in a text</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements <a class=\"anchor\" id=\"section-requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses various Python libraries. Most will come with your Python installation, but the following are crucial:\n",
    "- pandas\n",
    "- nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install ratelimit\n",
    "!pip install spacy\n",
    "# You need the old version due to issues setting titles with the latest\n",
    "!pip install ipywidgets==\"7.6.2\"\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# spaCy is used for a pipeline of NLP functions\n",
    "import spacy\n",
    "\n",
    "# ipywidgets is used for user interactive interfaces\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Imports for the map API requests and output formatting\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "from ratelimit import limits, sleep_and_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you can see as much of the output as possible within the Jupyter Notebook screen\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation <a class=\"anchor\" id=\"section-datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also want to set up some directories for the import and output of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the data directories\n",
    "# This presumes that 'notebooks' is the current working directory\n",
    "text_directory = os.path.normpath(\"../texts/\")\n",
    "csv_directory = os.path.normpath(\"../output/\")\n",
    "reference_directory = os.path.normpath(\"../data\")\n",
    "\n",
    "# Create the data directories\n",
    "if not os.path.exists(text_directory):\n",
    "    os.makedirs(text_directory)\n",
    "if not os.path.exists(csv_directory):\n",
    "    os.makedirs(csv_directory)\n",
    "if not os.path.exists(reference_directory):\n",
    "    os.makedirs(reference_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we will be examining the text of *For the Term of His Natural Life*, an 1874CE novel by Marcus Clarke that is in the public domain. Our copy was obtained via the [Gutenburg Project Australia](https://gutenberg.net.au/ebooks/e00016.txt). It is an unformatted textfile. We have slightly simplified it further by reducing it to only standard ASCII characters, replacing any accented characters with their unaccented forms and the British Pound Sterling symbol with the word pounds. \n",
    "\n",
    "The novel is divided into four books, each based in different regions of the world. For instance, you might want to start with the second book, which is titled *BOOK II.\\-\\-MACQUARIE HARBOUR.  1833*. Each book can be processed in its entirety, or as individual chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_BOOK_2_CHAPTER_3.txt\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# Set the specific path for the 'filename'\n",
    "text_location = os.path.normpath(os.path.join(text_directory, filename))\n",
    "text_filename = os.path.basename(text_location)\n",
    "\n",
    "text = open(text_location, encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now read Chapter 3 of Book 2 into memory. This is currently no more than a long string of characters. So far, you have done no processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0:500]  # look at the first 501 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Placenames in Text <a class=\"anchor\" id=\"section-identifyingplacenames\"></a>\n",
    "*This section provides tools on identifying placenames in textual data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition <a class=\"anchor\" id=\"section-ner\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts like the start of Chapter 3, Book 2, mention a lot of places. Some of them are a little vague, like _the house of Major Vickers_, _head-quarters_ and _the settlement_, but some are very explicit like _Maria Island_ and _Macquarie Harbour_. This notebook will show you how to use software automatically identify these proper noun phrases relating to placenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part of this notebook uses the Named Entity Recognition (NER) tool provided in the spaCy Python package. For an introduction to this tool and how it can be used to find Named Entities (NEs) like placenames in text, go to the [spaCy NER notebook](https://github.com/Australian-Text-Analytics-Platform/geolocation-tools-workshop/blob/7d92664ac44f86b90a0c098bb3159793a4fe6c16/Notebooks/spacy_ner_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This NER need not be done one file at a time. You can now put all of this together and find the placenames that are identified by spaCy in each chapter of the text. The results can all be collected in a single data structure, reviewed and saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe where we store the details about each instance of the placenames\n",
    "placenames_df = pd.DataFrame(columns=[\"Book\", \"Chapter\", \"NEIndex\", \"Placename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which chapters and books you want to examine\n",
    "chapters = [1, 2, 3]\n",
    "books = [1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a spaCy model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should define what spaCy processing you do or don't want in the pipeline. You mainly need the Tokenizer and the NER components. Others, like the Parser slow the processing down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabled_pipeline = [\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, not all NEs are placenames, so you will need to make a list of what categories regularly contain placenames. This notebook focusses on the Location, Geo-Political Entity, Facility and Organisation categories. While they may not cover all processing of all instances of all placenames, they have been observed to identify the majority of placenames without providing too many irrelevant NEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placename_categories = [\"LOC\", \"GPE\", \"FAC\", \"ORG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start processing the chapters from FtToHNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0  # Counter of the entities\n",
    "for book in books:\n",
    "    for chapter in chapters:\n",
    "        # Construct the filename for this book and chapter\n",
    "        filename = \"FtToHNL_BOOK_\" + str(book) + \"_CHAPTER_\" + str(chapter) + \".txt\"\n",
    "\n",
    "        # Set the specific path for the 'filename'\n",
    "        text_location = os.path.normpath(os.path.join(text_directory, filename))\n",
    "        text_filename = os.path.basename(text_location)\n",
    "\n",
    "        # Read this chapter\n",
    "        text = open(text_location, encoding=\"utf-8\").read()\n",
    "        print(\"Working on |\", filename)\n",
    "\n",
    "        # Run spaCy\n",
    "        doc = nlp(text, disable=disabled_pipeline)\n",
    "\n",
    "        # Document level\n",
    "        ents = [(entity.text, entity.start_char, entity.end_char, entity.label_) for entity in doc.ents]\n",
    "\n",
    "        # Token level\n",
    "        for entity in doc.ents:\n",
    "            # filter out MONEY, DATE, etc.\n",
    "            if entity.label_ in placename_categories:\n",
    "\n",
    "                # To help understand the context of the text, extract the occurrence\n",
    "                context_text = doc.text[entity.start_char - 30 : entity.end_char + 30].replace(\"\\n\", \" \")\n",
    "\n",
    "                # Add the placenames according to spaCy\n",
    "                new_placename = {\n",
    "                    \"Book\": book,  # The Book number\n",
    "                    \"Chapter\": chapter,  # The Chapter number\n",
    "                    \"NEIndex\": i,  # A reference number to the nth Named Entity\n",
    "                    \"Placename\": entity.text,  # The placename in the text\n",
    "                    \"Category\": entity.label_,  # The spaCy category\n",
    "                    \"Context\": context_text,  # The textual context where the placename was found\n",
    "                    \"Approval\": 1,  # A flag for whether this is a suitable placename\n",
    "                }\n",
    "                placenames_df = placenames_df.append(new_placename, ignore_index=True)\n",
    "\n",
    "            i = i + 1  # Entity counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Save a copy of this data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output what placenames have been found\n",
    "placenames_df[[\"Book\", \"Chapter\", \"NEIndex\", \"Placename\", \"Category\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a lot more placenames were found in Book 2 than Book 1. This makes sense since Book 1 is set on board an ocean voyage, whereas Book 2 is at Macquarie Harbour in Australia. You will also see that the same placename might be recognised multiple times but be categorised differently. This is because the spaCy model used for the NER is dependent on the linguistic content in which the NE instance appears in isolation. It does not assign the category based on multiple instances of an NE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Candidate Placenames <a class=\"anchor\" id=\"section-reviewplacenames\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a number of NEs that are unlikely to be placenames, regardless of what spaCy categorised them as. You want to be able to filter them out. Sometimes though it is hard to work out whether an NE is a person, organisation or a location. For instance, spaCy has said _Van Diemen's_ and _Tasman_ are both ORG NE. While both could be the names of people (Dutch explorer Abel Tasman and Governor-General of the Dutch East Indies Anthony van Dieman) but they are actually part of larger NEs _Van Diemen's Land_ (the former name for the state of Tasmania) and _Tasman's Head_ (a headland in Tasmania). For this, it is best to consider the context in which the terms were used. \n",
    "\n",
    "The following interfaces shows you each NE instance identified by spaCy's NER and the context in which they occure. Use the checkboxes to select which terms you consider to be placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that is used when a checkbox changes\n",
    "def changed(b):\n",
    "    k = b[\"new\"]\n",
    "\n",
    "# Lists of the data required for displaying the checkboxes\n",
    "placename_items = []\n",
    "context_items = []\n",
    "num_items = []\n",
    "\n",
    "# Make checkboxes for every placename for book in books:\n",
    "for book in books:\n",
    "    for chapter in chapters:\n",
    "\n",
    "        # Get the NEs from this book and chapter\n",
    "        placenames_book_chapter = placenames_df[(placenames_df[\"Book\"] == book) & (placenames_df[\"Chapter\"] == chapter)]\n",
    "\n",
    "        # Get the contextual data for each indexed NE\n",
    "        for i in placenames_book_chapter[\"NEIndex\"]:\n",
    "            context_text = placenames_book_chapter[placenames_book_chapter[\"NEIndex\"] == i][\"Context\"].values[0]\n",
    "            category = placenames_book_chapter[placenames_book_chapter[\"NEIndex\"] == i][\"Category\"].values[0]\n",
    "\n",
    "        # Make lists of the candidate placenames, context text and index numbers\n",
    "        # Only the placenames are given a checkbox.\n",
    "        placename_items = placename_items + [\n",
    "            widgets.Checkbox(True, description=i) for i in placenames_book_chapter[\"Placename\"]\n",
    "        ]\n",
    "        context_items = context_items + [\n",
    "            widgets.Label(placenames_book_chapter[placenames_book_chapter[\"NEIndex\"] == i][\"Context\"].values[0])\n",
    "            for i in placenames_book_chapter[\"NEIndex\"]\n",
    "        ]\n",
    "        num_items = num_items + [widgets.Label(str(i)) for i in placenames_book_chapter[\"NEIndex\"]]\n",
    "\n",
    "# Create a display\n",
    "num_placenames = len(placename_items)\n",
    "left_box = widgets.VBox(placename_items)\n",
    "right_box = widgets.VBox(context_items)\n",
    "num_box = widgets.VBox(num_items)\n",
    "whole_box = widgets.HBox([num_box, left_box, right_box])\n",
    "\n",
    "print(\"Unselect any Named Entities (NEs) that you do not consider to be placenames.\")\n",
    "print(\"Each instance of an NE is listed, with the textual context in which it appeared.\")\n",
    "\n",
    "display(whole_box)\n",
    "\n",
    "for n in range(num_placenames):\n",
    "    placename_items[n].observe(changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now copy all the values from the checkboxes to the data, so you know which placenames you have approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the status of each checklist item to the data\n",
    "for n in range(num_placenames):\n",
    "\n",
    "    NEIndex_num = int(num_items[n].value)\n",
    "    approval_flag = placename_items[n].value\n",
    "\n",
    "    # Set the flag to match the checklist\n",
    "    for placename in placenames_df[\"NEIndex\"]:\n",
    "        if placename - NEIndex_num == 0:\n",
    "            placenames_df.loc[placenames_df[\"NEIndex\"] == NEIndex_num, \"Approval\"] = approval_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now visualise the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df[[\"NEIndex\", \"Placename\", \"Approval\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can extract the final list of distinct placenames that you have approved. While the names aren't sorted (though they could be), if you missed unselecting an NE on the checklist, this will help find it. All you need to do is go back to the checklist, unselect it, then run all other steps from there to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a unique list of the approved placenames\n",
    "approved_placenames = placenames_df[placenames_df[\"Approval\"] == True][\"Placename\"].unique()\n",
    "print(approved_placenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save this new data to a csv file. You have already defined the directory for your data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_placenames.csv\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving placename data to \", save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list, using savetxt from the numpy module\n",
    "np.savetxt(save_location, approved_placenames, delimiter=\", \", fmt=\"% s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a copy of what you consider to be a list of placenames from a select set of FtToHNL chapters. Each placename found and approved is only included once. For this notebook, this data does not include contextual data like how many times a placename was found in the original text, nor where it was found.\n",
    "\n",
    "You can easily change the notebook commands to select different chapters or even look at a different single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Locations for the Placenames <a class=\"anchor\" id=\"section-findinglocs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a list of placenames from the text, the next step is to work out their location on Earth. For this you can use a combination of specialised lists of locations, gazzetteers and heuristics. The objective is to match every placename with the coordinates of a known location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to read the file of your placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_placenames.csv\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# Set the specific path for the 'filename'\n",
    "data_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "data_filename = os.path.basename(data_location)\n",
    "\n",
    "# Read the csv file using pandas. This will place it in a dataframe format.\n",
    "placenames_df = pd.read_csv(data_location, encoding=\"utf-8\", header=None)\n",
    "placenames_df = placenames_df.rename(columns={placenames_df.columns[0]: \"Placename\"})\n",
    "placenames_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying States and Capitals <a class=\"anchor\" id=\"section-statescapitals\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some placenames, like *High Street* or *Maryborough*, may be very common across the world, or even in Australia. However, certain placenames refer to significant locations, like states, territories, large geographic features or capital cities. As such, if they are mentioned in a text, the placename is more likely to refer to the major location than a town or village in Tasmania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These significant locations are a finite set. They can be defined in a reference file that can be reused when reviewing the placenames of any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good point for you to start is a file about locations like modern capital cities and countries, combined with historical locations of significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"reference_location_data.csv\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# Set the specific path for the 'filename'\n",
    "reference_location = os.path.normpath(os.path.join(reference_directory, filename))\n",
    "reference_filename = os.path.basename(reference_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than reading this and then processing it, you can process each line as you read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Change this to a dictionary?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the reference data in a dataframe\n",
    "locref_df = pd.read_csv(reference_location, encoding=\"utf-8\", header=0)\n",
    "locref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this allows some bias to be introduced into the data to suit your geolocation needs. For instance, _Perth_ is entered in this file as a city in the state of Western Australia, rather than one in Scotland. _Victoria_ is recorded as a state of Australia, rather than the capital of the Seychelles, or the capital of British Columbia, Canada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] update this text chunk to suit the workshop__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if you are researching historical texts, then some of these contemporary locations may have had different names. Old New York was once New Amsterdam (and had the [nickname of Gotham](https://www.nypl.org/blog/2011/01/25/so-why-do-we-call-it-gotham-anyway), amongst others). Istanbul was Constantinople. Some locations had [romanized names](https://en.wikipedia.org/wiki/Chinese_postal_romanization), like Beijing being called Peking. They may be a long time gone but you might want to add them to the list of significant known locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another historical variant is changing which cities are the capitals. These may be due to political decisions, like the movement of the Australian parliament from Melbourne to the new city of Canberra, or they could be a necessity due to the results of war, like Bonn becoming the capital of West Germany after World War II. These older capitals may also have to be accomodated in your reference data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because FtToHNL is set in the 19th Century CE, the next step is to add various capital cities from then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also larger geopolitical regions that may have been associated with placenames and cultures, for instance empires, dynasties and colonies like the British Empire or the Zulu Kingdom. Again, the borders and applicability of these political entities changed over time, so a contemporary reference list may not include them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 19th Century CE was a time of many European Empires so for FtToHNL, you will need to add reference data associated with relevant entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing this reference file, you can add the old political entity, its capital (if known), the geographic region (like continent or part thereof) and the modern country it would be considered part of.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to see if any of the placenames from our selected chapters of FtToHNL match these locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Describe this without being technical__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we match a placename, copy the geolocation data for the matching location. Otherwise, keep it empty so we know to keep looking for the placename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the data about placenames and locations, once linked, as a list of dataframes\n",
    "geolocdata = []\n",
    "\n",
    "for placename in placenames_df[\"Placename\"]:\n",
    "\n",
    "    # Create a new geoloc entry about this placename\n",
    "    # [TO DO] Formally declare this as a dataframe?\n",
    "    new_geolocdata = {}\n",
    "\n",
    "    # Start a record for a placename\n",
    "    new_geolocdata[\"placename\"] = placename\n",
    "    new_geolocdata[\"locations\"] = {}  # Start with no location details\n",
    "    new_geolocdata[\"locations\"][\"best_match\"] = []  # Start with no matching location\n",
    "\n",
    "    # Match found in the reference data\n",
    "    if placename in list(locref_df[\"LocationName\"]):\n",
    "        # Copy the details from the reference file entry\n",
    "        new_geolocdata[\"locations\"][\"best_match\"] = locref_df[locref_df[\"LocationName\"] == placename]\n",
    "\n",
    "        print(\"+++ Found\", placename)\n",
    "    else:\n",
    "        print(\"--- Still looking for\", placename)\n",
    "\n",
    "    # Add the new placename data to the list\n",
    "    geolocdata.append(new_geolocdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you have recorded the matches (and mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocdata[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What locations did you end up finding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = [\n",
    "    place[\"locations\"][\"best_match\"].to_string(index=False, header=False)\n",
    "    for place in geolocdata\n",
    "    if len(place[\"locations\"][\"best_match\"]) > 0\n",
    "]\n",
    "matched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now forget about the dataframe with the complete set of reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del locref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching a Gazetteer for Locations <a class=\"anchor\" id=\"section-searchgazzeteer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search [Open Street Map (ODM)](https://nominatim.org/release-docs/develop/api/Search/) for locations that match the unknown placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many (max) results do we want for each name?\n",
    "# [TO DO] Make this a user setting, defaulting to 5\n",
    "# The normal is (Default: 10, Maximum: 50), according to https://nominatim.org/release-docs/develop/api/Search/\n",
    "OSM_limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Talk more__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send rate-limited requests that stay within n requests per second\n",
    "# [TO DO] add link to webpage about this\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=1)\n",
    "def osm_call_api(url):\n",
    "    response = requests.get(url)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Convert a postcode from a string into a state abbreviation\n",
    "def postcode_to_state(postcodestr):\n",
    "    postcode = int(postcodestr)\n",
    "\n",
    "    if (1000 <= postcode <= 2599) or (2619 <= postcode < 2899) or (2921 <= postcode <= 2999):\n",
    "        return \"NSW\"\n",
    "    elif (200 <= postcode <= 299) or (2600 <= postcode <= 2618) or (2900 <= postcode <= 2920):\n",
    "        return \"ACT\"\n",
    "    elif (3000 <= postcode <= 3999) or (8000 <= postcode <= 8999):\n",
    "        return \"VIC\"\n",
    "    elif (4000 <= postcode <= 4999) or (9000 <= postcode < 9999):\n",
    "        return \"QLD\"\n",
    "    elif 5000 <= postcode <= 5999:\n",
    "        return \"SA\"\n",
    "    elif (6000 <= postcode <= 6797) or (6800 <= postcode <= 6999):\n",
    "        return \"WA\"\n",
    "    elif 8000 <= postcode <= 8999:\n",
    "        return \"TAS\"\n",
    "    elif 7000 <= postcode <= 7999:\n",
    "        return \"TAS\"\n",
    "    elif 800 <= postcode <= 999:\n",
    "        return \"NT\"\n",
    "    # Some postcodes are special cases\n",
    "    elif postcode == 2899:\n",
    "        return \"Norfolk Island\"  # Coded as NSW\n",
    "    elif postcode == 6798:\n",
    "        return \"Christmas Island\"  # Coded as WA\n",
    "    elif postcode == 6799:\n",
    "        return \"Cocos (Keeling) Islands\"  # Coded as WA\n",
    "    elif postcode == 9999:\n",
    "        return \"North Pole\"  # Coded as VIC for Santa mail\n",
    "\n",
    "    # Fallback\n",
    "    return postcodestr\n",
    "\n",
    "\n",
    "# Format the api response to make comparison easier\n",
    "def osm_format_response(input):\n",
    "\n",
    "    # Shorten the name and extract the country name, if any\n",
    "    hyper_location = None\n",
    "    short_location = input[\"display_name\"]  # default to full address\n",
    "\n",
    "    if input[\"display_name\"].find(\",\"):\n",
    "        # Break up the address\n",
    "        namesplit = input[\"display_name\"].split(\",\")\n",
    "        short_location = namesplit[0].lstrip().rstrip()\n",
    "        # Extract the rightmost term from the split\n",
    "        hyper_location = namesplit[len(namesplit) - 1].lstrip().rstrip()\n",
    "        # Change to an Australian state name, rather than Australia\n",
    "        if hyper_location == \"Australia\" and len(namesplit) > 2:\n",
    "            hyper_location = namesplit[len(namesplit) - 2].lstrip().rstrip()\n",
    "            # Change postcodes into states\n",
    "            if hyper_location.isdigit() and len(hyper_location) == 4:\n",
    "                hyper_location = postcode_to_state(hyper_location)\n",
    "\n",
    "    # For now, keep the names consistent between gazetteer records\n",
    "    response = {\n",
    "        \"LocationName\": str(short_location),\n",
    "        \"Category\": str(input[\"type\"]),\n",
    "        \"Latitude\": input[\"lat\"],\n",
    "        \"Longitude\": input[\"lon\"],\n",
    "        \"PartOf\": str(hyper_location),\n",
    "        \"Gazetteer\": \"OSM\",\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now move to the data that is needed for the geolocation project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every placename in our list\n",
    "for place in geolocdata:\n",
    "\n",
    "    # Already found a matching location, so skip to the next placename\n",
    "    if len(place[\"locations\"][\"best_match\"]) > 0:\n",
    "        continue\n",
    "\n",
    "    placename = place[\"placename\"]\n",
    "    print(\"Looking for\", placename)\n",
    "\n",
    "    # Query the OSM database\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?q={placename}&format=json&limit={OSM_limit}\"\n",
    "    response = osm_call_api(url)\n",
    "    response_dict = json.loads(response.text)\n",
    "\n",
    "    place[\"locations\"][\"candidates\"] = None\n",
    "\n",
    "    # Handle no results found, skip to the next placename\n",
    "    if len(response_dict) == 0:\n",
    "        continue\n",
    "\n",
    "    # Save the possible locations for later processing\n",
    "    data_frames = []\n",
    "\n",
    "    # Handle results found\n",
    "    for response_record in response_dict:\n",
    "        #  Use this to look at a reduced set of data from the results\n",
    "        cleaned_response = pd.DataFrame([osm_format_response(response_record)])\n",
    "\n",
    "        # Add the data to a dataframe\n",
    "        # The cleaned_response should be in the form we want to keep.\n",
    "        data_frames.append(cleaned_response)\n",
    "\n",
    "    # Add the results to the geoloc dataframe\n",
    "    place[\"locations\"][\"candidates\"] = data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What placenames have you still not found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_data = [\n",
    "    place[\"placename\"]\n",
    "    for place in geolocdata\n",
    "    if len(place[\"locations\"][\"best_match\"]) == 0 and place[\"locations\"][\"candidates\"] == None\n",
    "]\n",
    "unmatched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where have you found locations for placenames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = [\n",
    "    place[\"placename\"]\n",
    "    for place in geolocdata\n",
    "    if len(place[\"locations\"][\"best_match\"]) > 0 or place[\"locations\"][\"candidates\"] != None\n",
    "]\n",
    "matched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Show the output and discuss from OSM, so the context of the processing can be understood__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the final response to an OSM query\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Open Street Map is a wonderful resource, it focusses on current names of geographic locations. If the original source of your placenames was not written in the recent decades, then the OSM may not know the appropriate names of locations for the time of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to also look up a historical gazetteer, like the TLC. \n",
    "\n",
    "Like the OSM API, the TLCMap API has various options, like which type of search to use and whether to search any data in the database enetred by the public, rather than that which has been verified or entered by experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Describe the TLCMap__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, you will look for exact matches between the placenames and the locations, and not consider any publicly entered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which order to do different searches for known locations? Alt values: 'exact', 'fuzzy', 'contains'\n",
    "search_type = \"exact\"\n",
    "\n",
    "# Flag whether to use data provided by the public\n",
    "search_public_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for the OSM, you can limit how many results you want to examine. The TLCMap default for this notebook is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TLCMap_limit = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for the OSM, you will need a few functions to query the the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlc_build_url(placename: str, search_type: str, search_public_data: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Build a url to query the tlcmap/ghap API.\n",
    "    placename: the place we're trying to locate\n",
    "    search_type: what search type to use (accepts one of ['contains','fuzzy','exact'])\n",
    "\n",
    "    ref: https://www.tlcmap.org/guides/ghap/#ws\n",
    "    \"\"\"\n",
    "    safe_placename = urllib.parse.quote(placename.strip().lower())\n",
    "\n",
    "    url = f\"https://tlcmap.org/ghap/search?\"\n",
    "\n",
    "    if search_type == \"fuzzy\":\n",
    "        url += f\"fuzzyname={safe_placename}\"\n",
    "    elif search_type == \"exact\":\n",
    "        url += f\"name={safe_placename}\"\n",
    "    elif search_type == \"contains\":\n",
    "        url += f\"containsname={safe_placename}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Search Australian National Placenames Survey provided data\n",
    "    url += \"&searchausgaz=on\"\n",
    "\n",
    "    # Search public provided data, this data could be unreliable\n",
    "    if search_public_data == True:\n",
    "        url += \"&searchpublicdatasets=on\"\n",
    "    else:\n",
    "        url += \"&searchpublicdatasets=off\"\n",
    "\n",
    "    # Retrieve data as JSON\n",
    "    url += \"&format=json\"\n",
    "\n",
    "    # Limit the number of results\n",
    "    url += \"&paging=\" + str(TLCMap_limit)\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "# Send rate-limited requests that stay within n requests per second\n",
    "# [TO DO] add link to webpage about this\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=1)\n",
    "def tlc_call_api(url):\n",
    "    r = requests.get(url)\n",
    "    if r.url == \"https://tlcmap.org/ghap/maxpaging\":\n",
    "        return None\n",
    "\n",
    "    # If the reply says the placename wasn't found, customise the JSON data for the reply\n",
    "    if r.content.decode() == \"No search results to display.\":\n",
    "        # This should have obviously just be an empty list of features, but TLCMap is badly behaved\n",
    "        response = json.loads('{\"type\": \"FeatureCollection\",\"metadata\": {},\"features\": []}')\n",
    "    # SUCCESS! Record the spatial data provided in the reply\n",
    "    elif r.ok:\n",
    "        response = r.json()  # get [lon, lat] for spatial matches\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def tlc_query_name(placename: str, search_type: str):\n",
    "    \"\"\"\n",
    "    Use tlcmap/ghap API to check a placename, implemented fuzzy search but will not handle non returns.\n",
    "    \"\"\"\n",
    "    url = tlc_build_url(placename, search_type, search_public_data)\n",
    "    if url:\n",
    "        return tlc_call_api(url)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the api response to make comparison easier\n",
    "def tlc_format_response(location_features):\n",
    "\n",
    "    locdata = {}  # formatted data\n",
    "\n",
    "    # Gather the locdata for one of the placename's locations\n",
    "    # If the value is missing, set a value or leave it empty.\n",
    "    if len(location_features):\n",
    "        if \"placename\" in location_features[\"properties\"]:\n",
    "            locdata[\"LocationName\"] = location_features[\"properties\"][\"placename\"].lstrip().rstrip()\n",
    "        else:\n",
    "            locdata[\"LocationName\"] = \"Unknown Location\"\n",
    "\n",
    "        if \"feature_term\" in location_features[\"properties\"]:\n",
    "            locdata[\"Category\"] = location_features[\"properties\"][\"feature_term\"].lstrip().rstrip()\n",
    "        else:\n",
    "            locdata[\"Category\"] = None\n",
    "\n",
    "        if \"longitude\" in location_features[\"properties\"]:\n",
    "            locdata[\"Longitude\"] = location_features[\"properties\"][\"longitude\"]\n",
    "        else:\n",
    "            locdata[\"Longitude\"] = \"\"\n",
    "\n",
    "        if \"latitude\" in location_features[\"properties\"]:\n",
    "            locdata[\"Latitude\"] = location_features[\"properties\"][\"latitude\"]\n",
    "        else:\n",
    "            locdata[\"Latitude\"] = \"\"\n",
    "\n",
    "        if \"state\" in location_features[\"properties\"]:\n",
    "            locdata[\"PartOf\"] = location_features[\"properties\"][\"state\"].lstrip().rstrip()\n",
    "        else:\n",
    "            locdata[\"PartOf\"] = None\n",
    "\n",
    "        locdata[\"Gazetteer\"] = \"TLCMap\"\n",
    "\n",
    "        # For now, keep the names consistent between gazetteer records\n",
    "        response = {\n",
    "            \"LocationName\": str(locdata[\"LocationName\"]),\n",
    "            \"Category\": str(locdata[\"Category\"]),\n",
    "            \"Latitude\": locdata[\"Latitude\"],\n",
    "            \"Longitude\": locdata[\"Longitude\"],\n",
    "            \"PartOf\": str(locdata[\"PartOf\"]),\n",
    "            \"Gazetteer\": \"TLCMap\",\n",
    "        }\n",
    "    else:\n",
    "        response = None\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now search the TLCMap for locations matching the same placenames you previously searched for in the OSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every placename in our list\n",
    "for place in geolocdata:\n",
    "\n",
    "    # Already found a location, so skip to the next placename\n",
    "    if len(place[\"locations\"][\"best_match\"]) > 0:\n",
    "        continue\n",
    "\n",
    "    placename = place[\"placename\"]\n",
    "    print(\"Looking for\", placename)\n",
    "\n",
    "    # Query the OSM database\n",
    "    response = tlc_query_name(placename, search_type)\n",
    "\n",
    "    # Handle no results found, skip to the next placename\n",
    "    if response is None:\n",
    "        continue\n",
    "\n",
    "    # Save the possible locations for later processing\n",
    "    data_frames = []\n",
    "\n",
    "    # Handle results found\n",
    "    for response_record in response[\"features\"]:\n",
    "        #  Use this to look at a reduced set of data from the results\n",
    "        cleaned_response = pd.DataFrame([tlc_format_response(response_record)])\n",
    "\n",
    "        # Add the data to a dataframe\n",
    "        data_frames.append(cleaned_response)\n",
    "\n",
    "    # Add the results to the geoloc dataframe\n",
    "    # Review the outcomes later\n",
    "    # Make sure you don't write over any candidates previously added from another gazetteer.\n",
    "    if place[\"locations\"][\"candidates\"] is None:\n",
    "        place[\"locations\"][\"candidates\"] = data_frames\n",
    "    else:\n",
    "        place[\"locations\"][\"candidates\"] = place[\"locations\"][\"candidates\"] + data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What placenames have you still not found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_data = [\n",
    "    place[\"placename\"]\n",
    "    for place in geolocdata\n",
    "    # Must have not found an unambiguous best match or any candidate locations\n",
    "    if (\n",
    "        len(place[\"locations\"][\"best_match\"]) == 0\n",
    "        and (place[\"locations\"][\"candidates\"] is None or place[\"locations\"][\"candidates\"]) == []\n",
    "    )\n",
    "]\n",
    "unmatched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compare all the locations you have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = [\n",
    "    place\n",
    "    for place in geolocdata\n",
    "    if (\n",
    "        len(place[\"locations\"][\"best_match\"]) != 0\n",
    "        or (place[\"locations\"][\"candidates\"] is not None and place[\"locations\"][\"candidates\"]) != []\n",
    "    )\n",
    "]\n",
    "\n",
    "all_locations = []\n",
    "\n",
    "for place in matched_data:\n",
    "    placename = place[\"placename\"]\n",
    "    locations = []\n",
    "\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        # Unambiguous best match already known\n",
    "        locations = place[\"locations\"][\"best_match\"]\n",
    "    else:\n",
    "        # Any candidate locations you have found\n",
    "        if place[\"locations\"][\"candidates\"] is not None:\n",
    "            locations = pd.concat(place[\"locations\"][\"candidates\"], ignore_index=True)\n",
    "    print(\"===> \", placename)\n",
    "    print(locations)\n",
    "    all_locations = all_locations + [locations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Not sure which version to show - the full \"pretty version\" or the short dirty one__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(all_locations[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Show the output and discuss from OSM, so the context of the processing can be understood__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is now to work out which of these locations is most suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few heuristics can be used to flag those locations with key features that can be used to help rank and select the locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way is to acknowledge if multiple locations have similar coordinates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two sets of coordinates\n",
    "# Return true if they are both within 1 point of each other for both Lat and Lon\n",
    "def compare_coords(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    if lat1.item() == \"\" or lat2.item() == \"\" or lon1.item() == \"\" or lon2.item() == \"\":\n",
    "        return False\n",
    "\n",
    "    lat1_int = int(float(lat1.item()))\n",
    "    lon1_int = int(float(lon1.item()))\n",
    "    lat2_int = int(float(lat2.item()))\n",
    "    lon2_int = int(float(lon2.item()))\n",
    "\n",
    "    # Is Coord1 within 1 point of Coord2?\n",
    "    # e.g., -47.5 is close to -46.5 and -48.5\n",
    "    return (lat1_int in range(lat2_int - 1, lat2_int + 2, 1)) and (lon1_int in range(lon2_int - 1, lon2_int + 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, sometimes there are no coordinates for a location, which can complicate the comparisons. The solution is to default to the value of 0 for missing data. It is not perfect but in most cases it is adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a float as a string value, convert it to an integer\n",
    "# This is required because of NA or \"\".\n",
    "def sorting_coord(coord):\n",
    "\n",
    "    if type(coord) in [float, int]:\n",
    "        return int(coord)\n",
    "    # set to 0 if missing data\n",
    "    elif type(coord) == str and coord == \"\":\n",
    "        return 0\n",
    "    elif type(coord) != str and coord.isna():\n",
    "        return 0\n",
    "    else:\n",
    "        return int(float(coord))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another consideration is to recognise which locations are in certain countries which you know are relevant to the original document. For this you can focus on what values may be included in the PartOf field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag locations in Australia or Britain\n",
    "aus_states = {\n",
    "    \"AUSTRALIA\",\n",
    "    \"NSW\",\n",
    "    \"VIC\",\n",
    "    \"QLD\",\n",
    "    \"TAS\",\n",
    "    \"WA\",\n",
    "    \"NT\",\n",
    "    \"SA\",\n",
    "    \"ACT\",\n",
    "    \"NEW SOUTH WALES\",\n",
    "    \"VICTORIA\",\n",
    "    \"QUEENSLAND\",\n",
    "    \"TASMANIA\",\n",
    "    \"WESTERN AUSTRALIA\",\n",
    "    \"SOUTH AUSTRALIA\",\n",
    "    \"NORTHERN TERRITORY\",\n",
    "    \"AUSTRALIAN CAPITAL TERRITORY\",\n",
    "}\n",
    "gb_states = {\n",
    "    \"BRITAIN\",\n",
    "    \"UK\",\n",
    "    \"GB\",\n",
    "    \"GREAT BRITAIN\",\n",
    "    \"UNITED KINGDOM\",\n",
    "    \"BRITISH ISLES\",\n",
    "    \"ENGLAND\",\n",
    "    \"WALES\",\n",
    "    \"SCOTLAND\",\n",
    "    \"IRELAND\",\n",
    "    \"NORTHERN IRELAND\",\n",
    "    \"ÉIRE / IRELAND\",\n",
    "    \"EIRE\",\n",
    "    \"EIRE / IRELAND\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can go through each location and its candidate locations and flag whether they correspond to any of these criteria. A distinction is made between whether any two locations with similar coordinates were found in the same gazetteer or a different one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in matched_data:\n",
    "    placename = place[\"placename\"]\n",
    "    locations = []\n",
    "    if (\n",
    "        len(place[\"locations\"][\"best_match\"]) == 0\n",
    "        and place[\"locations\"][\"candidates\"] is not None\n",
    "        and len(place[\"locations\"][\"candidates\"]) > 1\n",
    "    ):\n",
    "\n",
    "        # Sort the locations by Latitude & Longitude\n",
    "        sorted_candidates = sorted(\n",
    "            place[\"locations\"][\"candidates\"],\n",
    "            key=lambda x: [\n",
    "                sorting_coord(x[\"Latitude\"].item()),\n",
    "                sorting_coord(x[\"Longitude\"].item()),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        prev_location = {}\n",
    "\n",
    "        # Flag the candidates according to the heuristics\n",
    "        for candidate in sorted_candidates:\n",
    "            rank_flags = []\n",
    "            partof_flags = \"\"\n",
    "\n",
    "            # Flag locations in Australia or Britain\n",
    "            # [TO DO] Make these lines into a function to pull the Aus/GB code out of the main code.\n",
    "            partings = candidate[\"PartOf\"].item().upper()\n",
    "            if partings in aus_states:\n",
    "                partof_flags = \"Australia\"\n",
    "            if partings in gb_states:\n",
    "                partof_flags = \"Britain\"\n",
    "            if partof_flags != \"\":\n",
    "                rank_flags.append(partof_flags)\n",
    "\n",
    "            # Flag coords in multiple gazetteers\n",
    "            if len(prev_location) > 0:\n",
    "                coord_flag = compare_coords(\n",
    "                    prev_location[\"Latitude\"],\n",
    "                    prev_location[\"Longitude\"],\n",
    "                    candidate[\"Latitude\"],\n",
    "                    candidate[\"Longitude\"],\n",
    "                )\n",
    "                # Dupl_Gaz2: Matching coords in locations from 2 gazetteers\n",
    "                if coord_flag and prev_location[\"Gazetteer\"].item() != candidate[\"Gazetteer\"].item():\n",
    "                    rank_flags.append(\"Dupl_2Gaz\")\n",
    "                # Dupl_Gaz1: Matching coords in locations from 1 gazetteer\n",
    "                elif coord_flag:\n",
    "                    rank_flags.append(\"Dupl_1Gaz\")\n",
    "\n",
    "            prev_location = candidate\n",
    "\n",
    "            # rank_flags has to be converted to a single string, rather than a list, \n",
    "            # because candidate is a Pandas dataframe\n",
    "            candidate[\"RankFlags\"] = pd.Series(\",\".join(rank_flags))\n",
    "            print(\n",
    "                \" ** \",\n",
    "                candidate[\"LocationName\"].item(),\n",
    "                \",\",\n",
    "                partings,\n",
    "                \",[\",\n",
    "                candidate[\"RankFlags\"].item(),\n",
    "                \"]\",\n",
    "            )\n",
    "\n",
    "        # Update the geoloc data\n",
    "        place[\"locations\"][\"candidates\"] = sorted_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the heuristics flags to re-sort the candidates and select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish how to rank the heuristic flags\n",
    "sortorder = [\n",
    "    \"Australia,Dupl_2Gaz\",  # In Australia, found in 2 gazetteers\n",
    "    \"Australia,Dupl_1Gaz\",  # In Australia, found more than once in 1 gazetteer\n",
    "    \"Australia\",  # In Australia, found only once\n",
    "    \"Britain,Dupl_2Gaz\",  # In Great Britain, found in 2 gazetteers\n",
    "    \"Dupl_2Gaz\",  # Not in Australia or Great Britain, found in 2 gazetteers\n",
    "    \"Britain,Dupl_1Gaz\",  # In Great Britain, found more than once in 1 gazetteer\n",
    "    \"Britain\",  # In Great Britain, found only once\n",
    "    \"Dupl_1Gaz\",  # Not in Australia or Great Britain, found more than once in 1 gazetteer\n",
    "    \"\",  # Not in Australia or Great Britain, found only once\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the candidates\n",
    "for place in geolocdata:\n",
    "    placename = place[\"placename\"]\n",
    "    pprint(placename)\n",
    "\n",
    "    if (\n",
    "        len(place[\"locations\"][\"best_match\"]) == 0\n",
    "        and place[\"locations\"][\"candidates\"] is not None\n",
    "        and len(place[\"locations\"][\"candidates\"]) > 1\n",
    "    ):\n",
    "        candidates = place[\"locations\"][\"candidates\"]\n",
    "\n",
    "        sorted_candidates = []\n",
    "        for heuristic in sortorder:\n",
    "            matched_candidates = []\n",
    "            # Get the candidates matching this ranking heuristic\n",
    "            for candidate in candidates:\n",
    "                if candidate[\"RankFlags\"].item() == heuristic:\n",
    "                    # Add the candidate to the sorted list\n",
    "                    sorted_candidates = sorted_candidates + [candidate]\n",
    "        # Prepare a version for printing\n",
    "        locations = pd.concat(sorted_candidates, ignore_index=True)\n",
    "        pprint(locations)\n",
    "\n",
    "        # Update the geoloc data with the sorted candidates\n",
    "        place[\"locations\"][\"candidates\"] = sorted_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting without the RankFlags column\n",
    "matched_data = [\n",
    "    place\n",
    "    for place in geolocdata\n",
    "    if (\n",
    "        len(place[\"locations\"][\"best_match\"]) != 0\n",
    "        or (place[\"locations\"][\"candidates\"] != None and place[\"locations\"][\"candidates\"]) != []\n",
    "    )\n",
    "]\n",
    "all_locations = []\n",
    "for place in matched_data:\n",
    "    placename = place[\"placename\"]\n",
    "    locations = []\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        # Unambiguous best match already known\n",
    "        locations = place[\"locations\"][\"best_match\"]\n",
    "    else:\n",
    "        # Any candidate locations you have found\n",
    "        if place[\"locations\"][\"candidates\"] != None:\n",
    "            short_candidates = []\n",
    "            for candidate in place[\"locations\"][\"candidates\"]:\n",
    "                # For output, ignore the RankFlags column\n",
    "                short_candidates = short_candidates + [candidate.loc[:, candidate.columns != \"RankFlags\"]]\n",
    "            locations = pd.concat(short_candidates, ignore_index=True)\n",
    "    print(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the candidates are sorted, the best match can be selected. The most obvious choice is the highest ranked candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in geolocdata:\n",
    "    placename = place[\"placename\"]\n",
    "    locations = []\n",
    "    # Already have the best match\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        locations = place[\"locations\"][\"best_match\"]\n",
    "    else:\n",
    "        if place[\"locations\"][\"candidates\"] is not None and len(place[\"locations\"][\"candidates\"]) > 0:\n",
    "            # Presume the best match has the top rank\n",
    "            top_candidate = place[\"locations\"][\"candidates\"][0]\n",
    "            place[\"locations\"][\"best_match\"] = top_candidate.loc[\n",
    "                :, ~top_candidate.columns.isin([\"Gazetteer\", \"RankFlags\"])\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputting just the best matching locations without the RankFlags column\n",
    "matched_data = [\n",
    "    place\n",
    "    for place in geolocdata\n",
    "    if (\n",
    "        len(place[\"locations\"][\"best_match\"]) != 0\n",
    "        or (place[\"locations\"][\"candidates\"] is not None and place[\"locations\"][\"candidates\"]) != []\n",
    "    )\n",
    "]\n",
    "all_locations = []\n",
    "for place in matched_data:\n",
    "    placename = place[\"placename\"]\n",
    "    locations = []\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        locations = place[\"locations\"][\"best_match\"]\n",
    "    else:\n",
    "        # This shouldn't run because all entries with matches should now have best matches, but just in case\n",
    "        if place[\"locations\"][\"candidates\"] is not None:\n",
    "            short_candidates = []\n",
    "            for candidate in place[\"locations\"][\"candidates\"]:\n",
    "                short_candidates = short_candidates + [candidate.loc[:, candidate.columns != \"RankFlags\"]]\n",
    "            locations = pd.concat(short_candidates, ignore_index=True)\n",
    "    print(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the top ranked candidate may still not be the best. The second ranked candidate may actually be the same rank as the top ranked one, but it simply might have a lesser position due to the earlier ranking based on the latitude and longitude values (to find the matching candidates for the ranking). For this reason, the final stage of the processing is left to the user, showing them the best candidate location, but allowing them to select an alternate candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways the user interface could be done. The first is very verbose and shows all the candidate locations of all placenames to the user. Using checkboxes, the user can then select what they think is the best match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the candidates could be presented as drop-down accordions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for place in geolocdata:\n",
    "    placename = place[\"placename\"]\n",
    "\n",
    "    # If there isn't a best match, then there aren't any candidates\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "\n",
    "        best_match = place[\"locations\"][\"best_match\"]\n",
    "        best_match_index = 0\n",
    "\n",
    "        # Must be at least one candidate\n",
    "        if (\n",
    "            \"candidates\" in place[\"locations\"].keys()\n",
    "            and place[\"locations\"][\"candidates\"] is not None\n",
    "            and len(place[\"locations\"][\"candidates\"]) >= 1\n",
    "        ):\n",
    "\n",
    "            # Create a RadioButton for each candidate location\n",
    "            candidates = place[\"locations\"][\"candidates\"]\n",
    "            candidate_buttons = [\n",
    "                widgets.RadioButtons(\n",
    "                    layout={\"width\": \"max-content\"},\n",
    "                    options=[\n",
    "                        candidate[\"LocationName\"].item()\n",
    "                        + \", \"\n",
    "                        + candidate[\"PartOf\"].item()\n",
    "                        + \" (\"\n",
    "                        + str(candidate[\"Latitude\"].item())\n",
    "                        + \",\"\n",
    "                        + str(candidate[\"Longitude\"].item())\n",
    "                        + \")\"\n",
    "                        for candidate in candidates\n",
    "                    ],\n",
    "                )\n",
    "            ]\n",
    "            # Add a button for \"None of the above\"\n",
    "            candidate_buttons[0].options = candidate_buttons[0].options + tuple([\"None of the above\"])\n",
    "            place[\"locations\"][\"candidate_buttons\"] = candidate_buttons\n",
    "\n",
    "            # Select the button for the current bestmatch\n",
    "            place[\"locations\"][\"candidate_buttons\"][0].index = best_match_index\n",
    "\n",
    "            # Create a HBox for the Placename\n",
    "            placename_box = widgets.HBox(\n",
    "                [\n",
    "                    widgets.Label(\n",
    "                        \"'\"\n",
    "                        + place[\"placename\"]\n",
    "                        + \"' matches best with: \"\n",
    "                        + best_match[\"LocationName\"].item()\n",
    "                        + \", \"\n",
    "                        + best_match[\"PartOf\"].item()\n",
    "                        + \" (\"\n",
    "                        + best_match[\"Latitude\"].item()\n",
    "                        + \", \"\n",
    "                        + best_match[\"Longitude\"].item()\n",
    "                        + \")\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create an Accordion (in a HBox) for the candidates\n",
    "            new_accordion = widgets.Accordion()\n",
    "            new_accordion.set_title(0, \"Expand to choose a different location\")\n",
    "            new_accordion.children = [widgets.HBox(place[\"locations\"][\"candidate_buttons\"])]\n",
    "            new_accordion.selected_index = None  # close the accordion at startup\n",
    "\n",
    "            # Put the HBoxes together in a VBox\n",
    "            whole_box = widgets.VBox([placename_box, new_accordion])\n",
    "\n",
    "            # Show it all!\n",
    "            # Note: this doesn't close one accordion if another is opened because they are in separate VBoxes\n",
    "            display(whole_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the selected candidates as the best matches. Account for placenames without any best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in geolocdata:\n",
    "\n",
    "    # If there isn't a best match, then there aren't any candidates\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "\n",
    "        best_match_index = 0\n",
    "\n",
    "        # Must be at least one candidate\n",
    "        if \"candidate_buttons\" in place[\"locations\"].keys():\n",
    "\n",
    "            # Extract the selection\n",
    "            best_match_index = place[\"locations\"][\"candidate_buttons\"][0].index\n",
    "            # Make sure the selection is a location\n",
    "            if best_match_index < len(place[\"locations\"][\"candidates\"]):\n",
    "                # Record the best match\n",
    "                top_candidate = place[\"locations\"][\"candidates\"][best_match_index]\n",
    "                # Just record the important columns for the best match\n",
    "                place[\"locations\"][\"best_match\"] = top_candidate.loc[\n",
    "                    :, ~top_candidate.columns.isin([\"Gazetteer\", \"RankFlags\"])\n",
    "                ]\n",
    "            else:\n",
    "                # None of the above\n",
    "                best_match = {\n",
    "                    \"LocationName\": \"No suitable location selected\",\n",
    "                    \"Category\": \"No suitable location selected\",\n",
    "                    \"Latitude\": \"\",\n",
    "                    \"Longitude\": \"\",\n",
    "                    \"PartOf\": \"No suitable location selected\",\n",
    "                }\n",
    "                place[\"locations\"][\"best_match\"] = pd.DataFrame([best_match])\n",
    "    else:\n",
    "        # Note that there is no best match\n",
    "        best_match = {\n",
    "            \"LocationName\": \"No location matched\",\n",
    "            \"Category\": \"No location matched\",\n",
    "            \"Latitude\": \"\",\n",
    "            \"Longitude\": \"\",\n",
    "            \"PartOf\": \"No location matched\",\n",
    "        }\n",
    "        place[\"locations\"][\"best_match\"] = pd.DataFrame([best_match])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the final geoloc data for output to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(geolocdata[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verbose output\n",
    "all_locations = []  # the processed data\n",
    "for place in geolocdata:\n",
    "    placename = place[\"placename\"]\n",
    "    pprint(placename)\n",
    "\n",
    "    # Reformat the data about the best match\n",
    "    best_locations = []\n",
    "    # All placenames should now have a best_match\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        for candidate in place[\"locations\"][\"best_match\"]:\n",
    "            if place[\"locations\"][\"best_match\"][candidate].item != \"\":\n",
    "                best_locations = best_locations + [place[\"locations\"][\"best_match\"][candidate].item()]\n",
    "\n",
    "    # Reformat the data about the candidate locations\n",
    "    locations = []\n",
    "    if \"candidates\" in place[\"locations\"].keys() and place[\"locations\"][\"candidates\"] != []:\n",
    "        short_candidates = []\n",
    "        for candidate in place[\"locations\"][\"candidates\"]:\n",
    "            # Select which columns to output\n",
    "            short_candidates = short_candidates + [candidate.loc[:, candidate.columns != \"RankFlags\"]]\n",
    "        # Merge the dataframe values into a more human-readible format for now,\n",
    "        # though this is not a comma-separated format\n",
    "        locations = pd.concat(short_candidates, ignore_index=True)\n",
    "\n",
    "    # Put this all together\n",
    "    new_record = [[\"placename\", placename], [\"best_match\", best_locations], [\"candidates\", locations]]\n",
    "    all_locations = all_locations + [new_record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in all_locations[0:10]:\n",
    "    pprint(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the geolocdata as a messy combination of array rows and dataframnes\n",
    "filename = \"FtToHNL_matchedlocations.data\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving to location data to \", save_location)\n",
    "\n",
    "# Save the list\n",
    "np.savetxt(save_location, geolocdata, delimiter=\", \", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output\n",
    "# Only use selected columns from the best match in csv format\n",
    "\n",
    "# Make a new array of records\n",
    "geoloc_output = [[\"Placename\", \"PartOf\", \"Latitude\", \"Longitude\"]]\n",
    "for place in geolocdata:\n",
    "    # Only output those placenames with a best match location\n",
    "    if len(place[\"locations\"][\"best_match\"]) != 0:\n",
    "        placename = place[\"placename\"]\n",
    "        location = place[\"locations\"][\"best_match\"]\n",
    "        print(\"Formatting \" + placename)\n",
    "        pprint(location)\n",
    "\n",
    "        # Convert Lat/Long into floats, rather than strings\n",
    "        if type(location[\"Latitude\"]) == float:\n",
    "            latitude = location[\"Latitude\"].item()\n",
    "        elif location[\"Latitude\"].item() != \"\":\n",
    "            latitude = float(location[\"Latitude\"].item())\n",
    "        else:\n",
    "            latitude = \"\"\n",
    "        if type(location[\"Longitude\"]) == float:\n",
    "            longitude = location[\"Longitude\"].item()\n",
    "        elif location[\"Longitude\"].item() != \"\":\n",
    "            longitude = float(location[\"Longitude\"].item())\n",
    "        else:\n",
    "            longitude = \"\"\n",
    "\n",
    "        # Add this record to the list\n",
    "        geoloc_output.append([placename, location[\"PartOf\"].item(), latitude, longitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_geolocdata.csv\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving to location data to \", save_location)\n",
    "\n",
    "# Save the list\n",
    "np.savetxt(save_location, geoloc_output, delimiter=\", \", fmt=\"% s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
