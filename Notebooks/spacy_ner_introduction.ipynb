{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Named Entity Recognition with spaCy \n",
    "\n",
    "This notebook helps you access the Named Entity Recognition (NER) tools in the spaCy Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "* [Premise](#section-premise)\n",
    "* [Requirements](#section-requirements) \n",
    "* [Data Preparation](#section-datapreparation)\n",
    "* [Named Entity Recognition](#section-ner)\n",
    "  * [Looking for Named Entities](#section-nes)\n",
    "  * [Categorising Named Entities](#section-categories) \n",
    "  * [Named Entities as Multi-Word Expressions](#section-mwes)\n",
    "  * [Improving the spaCy processing](#section-improvingspacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise <a class=\"anchor\" id=\"section-premise\"></a>\n",
    "*This section explains purpose of this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to introduce you to the spaCy Python package and show you how toi use it to recognise certain proper noun phrases or \"Named Entities\" in electronic text. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "It will teach you how to\n",
    "<ul>\n",
    "    <li>use the spaCy library to identify and classify Named Entities (NEs)</li>\n",
    "    <li>identify multi-word expressions (MWE) that are NEs</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements <a class=\"anchor\" id=\"section-requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "This notebook uses various Python libraries. Most will come with your Python installation, but the following are crucial:\n",
    "<ul>\n",
    "    <li> pandas</li> \n",
    "    <li> numpy</li> \n",
    "    <li> spacy</li> \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will load the required libraries into the notebook. If your computer is unable to do so, you may have to add an install line for the required package or seek technical advice.\n",
    "\n",
    "For example:\n",
    "<pre><code>import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# spaCy is used for a pipeline of NLP functions\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you can see as much of the output as possible within the Jupyter Notebook screen\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation <a class=\"anchor\" id=\"section-datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will use the text of *For the Term of His Natural Life* (*FtToHNL*), an 1874CE novel by Marcus Clarke that is in the public domain. Our copy was obtained via the [Gutenburg Project Australia](https://gutenberg.net.au/ebooks/e00016.txt) and is an unformatted textfile. We have slightly simplified it further by reducing it to only standard ASCII characters, replacing any accented characters with their unaccented forms and the British Pound Sterling symbol with the word pounds. \n",
    "\n",
    "The novel is divided into four books, each based in different regions of the world. You can start Chapter 3 of of the second book, which is titled *BOOK II.\\-\\-MACQUARIE HARBOUR.  1833*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to read the text from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This presumes that Notebooks/ is the current working directory  \n",
    "text_directory = os.path.normpath(\"../Texts/\")\n",
    "filename=\"FtToHNL_BOOK_2_CHAPTER_3.txt\"\n",
    "print(\"Reading: \", filename)\n",
    "\n",
    "# Set the specific path for the 'filename' \n",
    "text_location = os.path.normpath(os.path.join(text_directory, filename))\n",
    "text_filename = os.path.basename(text_location)\n",
    "\n",
    "# Read the text from the file\n",
    "text = open(text_location, encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no more than a long string of characters. So far, you have done no processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 501 characters\n",
    "text[0:500] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition <a class=\"anchor\" id=\"section-ner\"></a>\n",
    "*This section provides tools on identifying named entities in textual data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for NEs <a class=\"anchor\" id=\"section-nes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entities (NEs) are proper noun phrases within text, like the names of places, people or organisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above text from Book 2 of FtToHNL, you can see there are the names of places, characters, and a ship.  While you could mannually extract them from the text, Natural Language Processing (NLP) technology allows this process to be semi-automated through software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various packages that can include Named Enity Recognition (NER) tools, e.g., the [Stanza CoreNLP](https://colab.research.google.com/github/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb), the Stanford NER, and the spaCy library. They often combine machine learning and a rule-based system to identify NEs and classify them into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you will be using the [spaCy NER](https://spacy.io/usage/linguistic-features#named-entities).  This is available as part of the spaCy Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy allows you to load a [language model](https://spacy.io/models/en#en_core_web_sm) that has been trained on various examples of the language of interest. This training allows it to use statistical methods to evaluate the presence of various patterns in related texts that can be associated to linguistic behaviours.\n",
    "\n",
    "__[TO DO] Explain a language model *better* in one sentence.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy will use the model in various levels of natural language processing, that is called a processing pipeline. This pipeline includes dividing the text into individual tokens or terms, like words, values and punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The options for the spaCy pipeline include:\n",
    "    <p>&nbsp;</p>\n",
    "<table>\n",
    "    <tr><th>NAME</th>\t<th>COMPONENT</th>\t\t<th>\tDESCRIPTION</th>\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>tokenizer</strong></td><td>\tTokenizer</td><td>\tSegment text into tokens.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>tagger</strong></td><td>\tTagger</td><td>\tAssign part-of-speech tags.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>parser</strong></td><td>\tDependencyParser</td><td>\tAssign dependency labels.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>ner</strong></td><td>\tEntityRecognizer</td><td>\tDetect and label named entities.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>lemmatizer</strong></td><td>\tLemmatizer</td><td>\tAssign base forms.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>textcat</strong></td><td>\tTextCategorizer</td><td>\tAssign document labels.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>custom</strong></td><td>\tcustom components</td><td>\tAssign custom attributes, methods or properties.</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "    \n",
    "   A full explanation can be found at <a href=\"https://spacy.io/usage/processing-pipelines\">https://spacy.io/usage/processing-pipelines</a>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spaCy Language Procesing Pipeline](./spaCy_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, by default the spaCy pipeline contains the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the processes in the current pipeline\n",
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you send the FtToHNL text to the spaCy model, it will be processed by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 501 characters of the text using the model and pipeline\n",
    "doc = nlp(text[0:500])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not look very different to what we read from the file, but the output from the pipeline is now available in the output structure of the spaCy model. Each word is regarded as a token and each token has various linguistic features, based on the lexical and grammatical aspects that were identified by the pipeline, especially the parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "From <a href=\"https://spacy.io/usage/linguistic-features\">https://spacy.io/usage/linguistic-features</a>:\n",
    "        <ul><li> <strong>Text:</strong> The original token text.</li>\n",
    "<li> <strong>Dep:</strong> The syntactic relation connecting child to head.</li>\n",
    "<li> <strong>Head text:</strong> The original text of the token head.</li>\n",
    "<li> <strong>Head POS:</strong> The part-of-speech tag of the token head.</li>\n",
    "<li> <strong>Children:</strong> The immediate syntactic dependents of the token.</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each token in the first two sentences,\n",
    "for token in doc[9:59]:\n",
    "    # print the linguistic features of the token identified by the pipeline\n",
    "    print(token.text,\" - \", \n",
    "          \"\\n   Dep: \",token.dep_,       \n",
    "          \"\\n   Head: \",token.head.text, \n",
    "          \"\\n   Pos: \",token.head.pos_,  \n",
    "          \"\\n   Child: \",[child for child in token.children]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you might not want some of this pipeline processing as it may not be beneficial to your analysis. Any unneeded processing will also slow the system down and place a greater demand on the memory. This is particularly true of the parser. Luckily, it is easy to stipulate what you want excluded from the spaCy pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 501 characters of the text with a shortened pipeline\n",
    "doc=nlp(text[0:500], disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each token in the first two sentences, \n",
    "for token in doc[9:59]:\n",
    "    # print the linguistic features of the tokenidentified by the shorter pipeline\n",
    "    print(token.text,\" - \", \n",
    "          \"\\n   Dep: \",token.dep_,        \n",
    "          \"\\n   Head: \",token.head.text,  \n",
    "          \"\\n   Pos: \",token.head.pos_,   \n",
    "          \"\\n   Child: \",[child for child in token.children])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, for this notebook what you are interested in is [spaCy's NER](https://spacy.io/usage/linguistic-features#named-entities). Any text sent down the pipeline with the NER will get a list of entities that have been found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each Named Entity recognised by the NER,\n",
    "for entity in doc.ents:\n",
    "    # output the relevant tokens and NE category \n",
    "    print(entity.text+\" [\"+entity.label_+\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorising Named Entities <a class=\"anchor\" id=\"section-categories\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each entity is labelled with a category. The categories are defined by the model as it is trained to recognise them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    The NER categories classified by this spaCy model include:\n",
    "   <ul>\n",
    "       <li><strong>CARDINAL:</strong> Numerals that do not fall under another type</li>\n",
    "<li><strong>DATE:</strong> Absolute or relative dates or periods</li>\n",
    "<li><strong>EVENT:</strong> Named hurricanes, battles, wars, sports events, etc.</li>\n",
    "<li><strong>FAC:</strong> Buildings, airports, highways, bridges, etc.</li>\n",
    "<li><strong>GPE:</strong> Countries, cities, states</li>\n",
    "<li><strong>LANGUAGE:</strong> Any named language</li>\n",
    "<li><strong>LAW:</strong> Named documents made into laws.</li>\n",
    "<li><strong>LOC:</strong> Non-GPE locations, mountain ranges, bodies of water</li>\n",
    "<li><strong>MONEY:</strong> Monetary values, including unit</li>\n",
    "<li><strong>NORP:</strong> Nationalities or religious or political groups</li>\n",
    "<li><strong>ORDINAL:</strong> \"first\", \"second\", etc.</li>\n",
    "<li><strong>ORG:</strong> Companies, agencies, institutions, etc.</li>\n",
    "<li><strong>PERCENT:</strong> Percentage, including \"%\"</li>\n",
    "<li><strong>PERSON:</strong> People, including fictional</li>\n",
    "<li><strong>PRODUCT:</strong> Objects, vehicles, foods, etc. (not services)</li>\n",
    "<li><strong>QUANTITY:</strong> Measurements, as of weight or distance</li>\n",
    "<li><strong>TIME:</strong> Times smaller than a day</li>\n",
    "<li><strong>WORK_OF_ART:</strong> Titles of books, songs, etc.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These categories can be very helpful when you are trying to identify certain types of NEs. However, they are not perfect. Because the NER is based on the language model, words are given categories based on what the language model has seen in the training data. This means that the same NE can be given different categories, dependent on the linguistic context in which it appears. It should also be noted that spaCy's NER only labels each NE with one category and does not normally provide any measure of how certain it is about that category. Other NLP systems will have similar NE categories, but there is no universal ontology. Some are more fine-grained than others. If you need even more fine-grained or alternate categories in spaCy, you will have to train and use a suitable language model or add some post-processing, which will be briefly discussed [later in this notebook](#section-improvingspacy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities as Multi-Word Expressions <a class=\"anchor\" id=\"section-mwes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that some of the NEs recognised by spaCy in the text include more than one word. The ability to recognise Multi-Word Expressions (MWEs) as NEs is important, as is understanding the context of word usage. Luckily, the data for the entities includes the character position for the start and the end of the NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each Named Entity recognised by the NER,\n",
    "for entity in doc.ents:\n",
    "    # output the start and end characters for the relevant tokens\n",
    "    print(entity.text, \"(\"+str(entity.start_char)+\",\"+str(entity.end_char)+\") [\"+entity.label_+\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token will also have a value that indicates whether it is part of an NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each token in the first two sentences\n",
    "for token in doc[9:59]:\n",
    "    # output whether it has an NE category\n",
    "    print(token.text+\" [\"+token.ent_type_+\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contextual information is very helpful as it helps you evaluate the scope of terms regarded as a part of an NE and whether they are appropriate, given the linguistic use of the terms. For instance, in your example sentences, you can see that both of the PERSON NEs are directly preceded by military titles, e.g., _Major Vickers_ and _Lieutenant Maurice Frere_. It also shows that _Maria_ is not regarded as a Person NE because it is part of a LOC NE. This is one of the consequences of spaCy only labelling one NE category per token. If a token is regarded to be part of a MWE, then this subsumes any possible categories it may be regarded as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the spaCy processing  <a class=\"anchor\" id=\"section-improvingspacy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have also recognised that the spaCy NER didn't recognise that _Macquarie Harbour_ was a Named Entity. This might have happened if the training data for the language model you used didn't have any example text that included the words _Macquarie_ or _harbour_, especially as part of a proper noun phrase. The most obvious way to correct this is to train a new model that does have such examples. It is beyond the scope of this notebook to discuss but [spaCy has various guides](https://spacy.io/usage/training) on how this can be done, especially for NER.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to train models, then you can always set up some post-processing Python code to add some new NEs or change their categories. This is a good option if you don't have a wide range of changes to make, or if the linguistic contexts in which it would apply are very specific, e.g., only for a small closed set of words or distinct grammatical strutures. Again, this is beyond the scope of this workshop, as it requires an understanding of the [spaCy IOB and BILUO schema](https://spacy.io/usage/linguistic-features#accessing-ner) if you want to [set your own entities](https://spacy.io/usage/linguistic-features#setting-entities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as previously mentioned, some of the natural language processing in the spaCy pipeline can be resource-hungry, taking up CPU usage and/or memory. For this reason, if you are wanting to process a lot of data, like a lot of documents or sentences, you might be better processing them in batches. This will split up the processing by [piping the data to the pipeline](https://spacy.io/usage/processing-pipelines#processing) as a stream rather than processing it altogether at each stage. This is relatively easy to do but it does change some of the data structures of some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope this notebook has given you a basic understanding of how you can use the spaCy Named Entity Recgontion tool. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
