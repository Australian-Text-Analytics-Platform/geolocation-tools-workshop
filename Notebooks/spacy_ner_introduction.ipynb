{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Named Entity Recognition with spaCy \n",
    "\n",
    "This notebook helps you access the Named Entity Recognition (NER) tools in the spaCy Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "* [Premise](#section-premise)\n",
    "* [Requirements](#section-requirements) \n",
    "* [Data Preparation](#section-datapreparation)\n",
    "* [Named Entity Recognition](#section-ner)\n",
    " * [Look for NEs](#section-nes)\n",
    " * [Reviewing Candidate Placenames](#section-reviewplacenames)\n",
    "* [Finding Locations for Placenames](#section-findinglocs)\n",
    " * [Identifying States and Capitals](#section-statescapitals)\n",
    " * [Searching a Gazzetteer for Locations](#section-searchgazetteer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise <a class=\"anchor\" id=\"section-premise\"></a>\n",
    "*This section explains the Geolocation project and tools.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geolocation project relates to doctoral research done by [Fiannuala Morgan](https://finnoscarmorgan.github.io/) at the Australian National University. It uses software to identify placenames in archived historical texts, then compares them to data about known locations to identify where the placenames may be located. \n",
    "\n",
    "This notebook is designed to allow you to perform similar operations on textual documents.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "It will teach you how to\n",
    "<ul>\n",
    "    <li>use the spaCy library to identify and classify Named Entities (NEs)</li>\n",
    "    <li>identify multi-word expressions (MWE) that are NEs</li>\n",
    "    <li>search for spatial data about specific locations or places in gazetteers of such data</li>\n",
    "    <li>determine which locations are referred to by placenames, based on the context in which they are used in a text</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements <a class=\"anchor\" id=\"section-requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "This notebook uses various Python libraries. Most will come with your Python installation, but the following are crucial:\n",
    "<ul>\n",
    "    <li> pandas</li> \n",
    "    <li> json</li> \n",
    "    <li> nltk</li> \n",
    "    <li> geopandas</li> \n",
    "    <li> shapely  </li> \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TO DO] UPDATE\n",
    "# Many of these are probably not needed.\n",
    "\n",
    "import os\n",
    "from pickle import NONE\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geopandas is used to work with spatial data\n",
    "# If you have issues installing it on a MAcOS, \n",
    "# see https://stackoverflow.com/questions/71137617/error-installing-geopandas-in-python-on-mac-m1\n",
    "#import geopandas as gpd\n",
    "#from geopandas import GeoDataFrame\n",
    "\n",
    "# NLTK is used to work with textual data \n",
    "#from nltk.tag import StanfordNERTagger\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "# spaCy is used for a pipeline of NLP functions\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "# Shapely is used to work with geometric shapes\n",
    "#from shapely.geometry import Point\n",
    "\n",
    "# Fuzzywuzzy is used for fuzzy searches\n",
    "#from fuzzywuzzy import fuzz\n",
    "\n",
    "# used for the checklist\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the OSM\n",
    "# [TO DOI] Work oput what is still required\n",
    "import requests\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "from pprint import pprint\n",
    "from ratelimit import limits, RateLimitException, sleep_and_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you can see as much of the output as possible within the Jupyter Notebook screen\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation <a class=\"anchor\" id=\"section-datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also want to set up some directories for the import and output of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare the data directories\n",
    "## This presumes that Notebooks/ is the current working directory  \n",
    "text_directory = os.path.normpath(\"../Texts/\")\n",
    "csv_directory = os.path.normpath(\"../ner_output/\")\n",
    "reference_directory = os.path.normpath(\"../Data\")\n",
    "\n",
    "## Create the data directories\n",
    "if not os.path.exists(text_directory):\n",
    "    os.makedirs(text_directory)\n",
    "if not os.path.exists(csv_directory):\n",
    "   os.makedirs(csv_directory)\n",
    "if not os.path.exists(reference_directory):\n",
    "   os.makedirs(reference_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we will be examining the text of *For the Term of His Natural Life*, an 1874CE novel by Marcus Clarke that is in the public domain. Our copy was obtained via the [Gutenburg Project Australia](https://gutenberg.net.au/ebooks/e00016.txt). It is an unformatted textfile. We have slightly simplified it further by reducing it to only standard ASCII characters, replacing any accented characters with their unaccented forms and the British Pound Sterling symbol with the word pounds. \n",
    "\n",
    "The novel is divided into four books, each based in different regions of the world. You can start with part of the second book, which is titled *BOOK II.\\-\\-MACQUARIE HARBOUR.  1833*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on |  FtToHNL_BOOK_2_CHAPTER_3.txt\n"
     ]
    }
   ],
   "source": [
    "filename=\"FtToHNL_BOOK_2_CHAPTER_3.txt\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' \n",
    "text_location = os.path.normpath(os.path.join(text_directory, filename))\n",
    "text_filename = os.path.basename(text_location)\n",
    "\n",
    "text = open(text_location, encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no more than a long string of characters. So far, you have done no processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER III.\\n\\nA SOCIAL EVENING.\\n\\n\\n\\nIn the house of Major Vickers, Commandant of Macquarie Harbour,\\nthere was, on this evening of December 3rd, unusual gaiety.\\n\\nLieutenant Maurice Frere, late in command at Maria Island, had unexpectedly\\ncome down with news from head-quarters.  The Ladybird, Government schooner,\\nvisited the settlement on ordinary occasions twice a year, and such visits\\nwere looked forward to with no little eagerness by the settlers.\\nTo the convicts the arrival of the Ladybird mean'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:500] # look at the first 501 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition <a class=\"anchor\" id=\"section-ner\"></a>\n",
    "*This section provides tools on identifying named entities in textual data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for NEs <a class=\"anchor\" id=\"section-nes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entities (NEs) are proper noun phrases within text, like the names of places, people or organisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above text from Book 2 of FtToHNL, you can see there are the names of places, characters, and a ship.  While you could mannually extract them from the text, Natural Language Processing (NLP) technology allows this process to be semi-automated through software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various packages that can include Named Enity Recognition (NER), e.g., the [Stanza CoreNLP](https://colab.research.google.com/github/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb), the Stanford NER, and the spaCy library. They often combine machine learning and a rule-based system to identify NEs and classify them into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you will be using the spaCy NER - https://spacy.io/usage/linguistic-features#morphology .  This is available as a Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy allows you to load a [language model](https://spacy.io/models/en#en_core_web_sm) that has been trained on various examples of the language of interest. \n",
    "\n",
    "__[TO DO] Explain a language model in one sentence.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy will automatically run the model through various levels of natural language processing, that is called a processing pipeline. This pipeline includes dividing the text into individual tokens or terms, like words, values and punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The options for the spaCy pipeline include:\n",
    "    <p>&nbsp;</p>\n",
    "<table>\n",
    "    <tr><th>NAME</th>\t<th>COMPONENT</th>\t\t<th>\tDESCRIPTION</th>\t\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>tokenizer</strong></td><td>\tTokenizer</td><td>\tSegment text into tokens.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>tagger</strong></td><td>\tTagger</td><td>\tAssign part-of-speech tags.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>parser</strong></td><td>\tDependencyParser</td><td>\tAssign dependency labels.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>ner</strong></td><td>\tEntityRecognizer</td><td>\tDetect and label named entities.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>lemmatizer</strong></td><td>\tLemmatizer</td><td>\tAssign base forms.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>textcat</strong></td><td>\tTextCategorizer</td><td>\tAssign document labels.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "\n",
    "<td><strong>custom</strong></td><td>\tcustom components</td><td>\tAssign custom attributes, methods or properties.</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "    \n",
    "   A full explanation can be found at <a href=\"https://spacy.io/usage/processing-pipelines\">https://spacy.io/usage/processing-pipelines</a>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spaCy Language Procesing Pipeline](./spaCy_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, by default the spaCy pipeline contains the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sent to the spaCy model will be processed by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CHAPTER III.\n",
       "\n",
       "A SOCIAL EVENING.\n",
       "\n",
       "\n",
       "\n",
       "In the house of Major Vickers, Commandant of Macquarie Harbour,\n",
       "there was, on this evening of December 3rd, unusual gaiety.\n",
       "\n",
       "Lieutenant Maurice Frere, late in command at Maria Island, had unexpectedly\n",
       "come down with news from head-quarters.  The Ladybird, Government schooner,\n",
       "visited the settlement on ordinary occasions twice a year, and such visits\n",
       "were looked forward to with no little eagerness by the settlers.\n",
       "To the convicts the arrival of the Ladybird mean"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text[0:500])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not look very different but the output from the pipeline is now available in the output structure of the spaCy model. Each word is regarded as a token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "From <a href=\"https://spacy.io/usage/linguistic-features\">https://spacy.io/usage/linguistic-features</a>:\n",
    "        <ul><li> <strong>Text:</strong> The original token text.</li>\n",
    "<li> <strong>Dep:</strong> The syntactic relation connecting child to head.</li>\n",
    "<li> <strong>Head text:</strong> The original text of the token head.</li>\n",
    "<li> <strong>Head POS:</strong> The part-of-speech tag of the token head.</li>\n",
    "<li> <strong>Children:</strong> The immediate syntactic dependents of the token.</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In  -  \n",
      "   Dep:  prep \n",
      "   Head:  was \n",
      "   Pos:  AUX \n",
      "   Child:  [house]\n",
      "the  -  \n",
      "   Dep:  det \n",
      "   Head:  house \n",
      "   Pos:  NOUN \n",
      "   Child:  []\n",
      "house  -  \n",
      "   Dep:  pobj \n",
      "   Head:  In \n",
      "   Pos:  ADP \n",
      "   Child:  [the, of]\n",
      "of  -  \n",
      "   Dep:  prep \n",
      "   Head:  house \n",
      "   Pos:  NOUN \n",
      "   Child:  [Vickers]\n",
      "Major  -  \n",
      "   Dep:  compound \n",
      "   Head:  Vickers \n",
      "   Pos:  PROPN \n",
      "   Child:  []\n",
      "Vickers  -  \n",
      "   Dep:  pobj \n",
      "   Head:  of \n",
      "   Pos:  ADP \n",
      "   Child:  [Major, ,, Commandant]\n",
      ",  -  \n",
      "   Dep:  punct \n",
      "   Head:  Vickers \n",
      "   Pos:  PROPN \n",
      "   Child:  []\n",
      "Commandant  -  \n",
      "   Dep:  appos \n",
      "   Head:  Vickers \n",
      "   Pos:  PROPN \n",
      "   Child:  [of]\n",
      "of  -  \n",
      "   Dep:  prep \n",
      "   Head:  Commandant \n",
      "   Pos:  PROPN \n",
      "   Child:  [Harbour]\n",
      "Macquarie  -  \n",
      "   Dep:  compound \n",
      "   Head:  Harbour \n",
      "   Pos:  PROPN \n",
      "   Child:  []\n",
      "Harbour  -  \n",
      "   Dep:  pobj \n",
      "   Head:  of \n",
      "   Pos:  ADP \n",
      "   Child:  [Macquarie]\n",
      ",  -  \n",
      "   Dep:  punct \n",
      "   Head:  was \n",
      "   Pos:  AUX \n",
      "   Child:  [\n",
      "]\n",
      "\n",
      "  -  \n",
      "   Dep:   \n",
      "   Head:  , \n",
      "   Pos:  PUNCT \n",
      "   Child:  []\n",
      "there  -  \n",
      "   Dep:  expl \n",
      "   Head:  was \n",
      "   Pos:  AUX \n",
      "   Child:  []\n",
      "was  -  \n",
      "   Dep:  ROOT \n",
      "   Head:  was \n",
      "   Pos:  AUX \n",
      "   Child:  [In, ,, there, ,, on, gaiety, .]\n",
      ",  -  \n",
      "   Dep:  punct \n",
      "   Head:  was \n",
      "   Pos:  AUX \n",
      "   Child:  []\n",
      "on  -  \n",
      "   Dep:  prep \n",
      "   Head:  was \n",
      "   Pos:  AUX \n",
      "   Child:  [evening]\n",
      "this  -  \n",
      "   Dep:  det \n",
      "   Head:  evening \n",
      "   Pos:  NOUN \n",
      "   Child:  []\n",
      "evening  -  \n",
      "   Dep:  pobj \n",
      "   Head:  on \n",
      "   Pos:  ADP \n",
      "   Child:  [this, of]\n",
      "of  -  \n",
      "   Dep:  prep \n",
      "   Head:  evening \n",
      "   Pos:  NOUN \n",
      "   Child:  [3rd]\n",
      "December  -  \n",
      "   Dep:  compound \n",
      "   Head:  3rd \n",
      "   Pos:  NOUN \n",
      "   Child:  []\n",
      "3rd  -  \n",
      "   Dep:  pobj \n",
      "   Head:  of \n",
      "   Pos:  ADP \n",
      "   Child:  [December]\n",
      ",  -  \n",
      "   Dep:  punct \n",
      "   Head:  gaiety \n",
      "   Pos:  NOUN \n",
      "   Child:  []\n",
      "unusual  -  \n",
      "   Dep:  amod \n",
      "   Head:  gaiety \n",
      "   Pos:  NOUN \n",
      "   Child:  []\n",
      "gaiety  -  \n",
      "   Dep:  attr \n",
      "   Head:  was \n",
      "   Pos:  AUX \n",
      "   Child:  [,, unusual]\n",
      ".  -  \n",
      "   Dep:  punct \n",
      "   Head:  was \n",
      "   Pos:  AUX \n",
      "   Child:  [\n",
      "\n",
      "]\n",
      "\n",
      "\n",
      "  -  \n",
      "   Dep:   \n",
      "   Head:  . \n",
      "   Pos:  PUNCT \n",
      "   Child:  []\n",
      "Lieutenant  -  \n",
      "   Dep:  compound \n",
      "   Head:  Maurice \n",
      "   Pos:  PROPN \n",
      "   Child:  []\n",
      "Maurice  -  \n",
      "   Dep:  compound \n",
      "   Head:  Frere \n",
      "   Pos:  PROPN \n",
      "   Child:  [Lieutenant]\n",
      "Frere  -  \n",
      "   Dep:  nsubj \n",
      "   Head:  come \n",
      "   Pos:  VERB \n",
      "   Child:  [Maurice]\n",
      ",  -  \n",
      "   Dep:  punct \n",
      "   Head:  had \n",
      "   Pos:  AUX \n",
      "   Child:  []\n",
      "late  -  \n",
      "   Dep:  advmod \n",
      "   Head:  at \n",
      "   Pos:  ADP \n",
      "   Child:  [in]\n",
      "in  -  \n",
      "   Dep:  prep \n",
      "   Head:  late \n",
      "   Pos:  ADV \n",
      "   Child:  [command]\n",
      "command  -  \n",
      "   Dep:  pobj \n",
      "   Head:  in \n",
      "   Pos:  ADP \n",
      "   Child:  []\n",
      "at  -  \n",
      "   Dep:  prep \n",
      "   Head:  had \n",
      "   Pos:  AUX \n",
      "   Child:  [late, Island]\n",
      "Maria  -  \n",
      "   Dep:  compound \n",
      "   Head:  Island \n",
      "   Pos:  PROPN \n",
      "   Child:  []\n",
      "Island  -  \n",
      "   Dep:  pobj \n",
      "   Head:  at \n",
      "   Pos:  ADP \n",
      "   Child:  [Maria]\n",
      ",  -  \n",
      "   Dep:  punct \n",
      "   Head:  had \n",
      "   Pos:  AUX \n",
      "   Child:  []\n",
      "had  -  \n",
      "   Dep:  aux \n",
      "   Head:  come \n",
      "   Pos:  VERB \n",
      "   Child:  [,, at, ,, unexpectedly]\n",
      "unexpectedly  -  \n",
      "   Dep:  advmod \n",
      "   Head:  had \n",
      "   Pos:  AUX \n",
      "   Child:  [\n",
      "]\n",
      "\n",
      "  -  \n",
      "   Dep:   \n",
      "   Head:  unexpectedly \n",
      "   Pos:  ADV \n",
      "   Child:  []\n",
      "come  -  \n",
      "   Dep:  ROOT \n",
      "   Head:  come \n",
      "   Pos:  VERB \n",
      "   Child:  [Frere, had, down, with, .]\n",
      "down  -  \n",
      "   Dep:  prt \n",
      "   Head:  come \n",
      "   Pos:  VERB \n",
      "   Child:  []\n",
      "with  -  \n",
      "   Dep:  prep \n",
      "   Head:  come \n",
      "   Pos:  VERB \n",
      "   Child:  [news]\n",
      "news  -  \n",
      "   Dep:  pobj \n",
      "   Head:  with \n",
      "   Pos:  ADP \n",
      "   Child:  [from]\n",
      "from  -  \n",
      "   Dep:  prep \n",
      "   Head:  news \n",
      "   Pos:  NOUN \n",
      "   Child:  [quarters]\n",
      "head  -  \n",
      "   Dep:  compound \n",
      "   Head:  quarters \n",
      "   Pos:  NOUN \n",
      "   Child:  []\n",
      "-  -  \n",
      "   Dep:  punct \n",
      "   Head:  quarters \n",
      "   Pos:  NOUN \n",
      "   Child:  []\n",
      "quarters  -  \n",
      "   Dep:  pobj \n",
      "   Head:  from \n",
      "   Pos:  ADP \n",
      "   Child:  [head, -]\n",
      ".  -  \n",
      "   Dep:  punct \n",
      "   Head:  come \n",
      "   Pos:  VERB \n",
      "   Child:  [ ]\n"
     ]
    }
   ],
   "source": [
    "for token in doc[9:59]:\n",
    "    print(token.text,\" - \", \n",
    "          \"\\n   Dep: \",token.dep_,       \n",
    "          \"\\n   Head: \",token.head.text, \n",
    "          \"\\n   Pos: \",token.head.pos_,  \n",
    "          \"\\n   Child: \",[child for child in token.children]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you might not want some of this pipeline processing as it may not be beneficial to your analysis. Any unneeded processing will also slow the system down and place a greater demand on the memory. This is particularly true of the parser. Luckily, it is easy to stipulate what you want excluded from the spaCy pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text[0:500], disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In  -  \n",
      "   Dep:   \n",
      "   Head:  In \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "the  -  \n",
      "   Dep:   \n",
      "   Head:  the \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "house  -  \n",
      "   Dep:   \n",
      "   Head:  house \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "of  -  \n",
      "   Dep:   \n",
      "   Head:  of \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Major  -  \n",
      "   Dep:   \n",
      "   Head:  Major \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Vickers  -  \n",
      "   Dep:   \n",
      "   Head:  Vickers \n",
      "   Pos:   \n",
      "   Child:  []\n",
      ",  -  \n",
      "   Dep:   \n",
      "   Head:  , \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Commandant  -  \n",
      "   Dep:   \n",
      "   Head:  Commandant \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "of  -  \n",
      "   Dep:   \n",
      "   Head:  of \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Macquarie  -  \n",
      "   Dep:   \n",
      "   Head:  Macquarie \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Harbour  -  \n",
      "   Dep:   \n",
      "   Head:  Harbour \n",
      "   Pos:   \n",
      "   Child:  []\n",
      ",  -  \n",
      "   Dep:   \n",
      "   Head:  , \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "\n",
      "  -  \n",
      "   Dep:   \n",
      "   Head:  \n",
      " \n",
      "   Pos:  SPACE \n",
      "   Child:  []\n",
      "there  -  \n",
      "   Dep:   \n",
      "   Head:  there \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "was  -  \n",
      "   Dep:   \n",
      "   Head:  was \n",
      "   Pos:   \n",
      "   Child:  []\n",
      ",  -  \n",
      "   Dep:   \n",
      "   Head:  , \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "on  -  \n",
      "   Dep:   \n",
      "   Head:  on \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "this  -  \n",
      "   Dep:   \n",
      "   Head:  this \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "evening  -  \n",
      "   Dep:   \n",
      "   Head:  evening \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "of  -  \n",
      "   Dep:   \n",
      "   Head:  of \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "December  -  \n",
      "   Dep:   \n",
      "   Head:  December \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "3rd  -  \n",
      "   Dep:   \n",
      "   Head:  3rd \n",
      "   Pos:   \n",
      "   Child:  []\n",
      ",  -  \n",
      "   Dep:   \n",
      "   Head:  , \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "unusual  -  \n",
      "   Dep:   \n",
      "   Head:  unusual \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "gaiety  -  \n",
      "   Dep:   \n",
      "   Head:  gaiety \n",
      "   Pos:   \n",
      "   Child:  []\n",
      ".  -  \n",
      "   Dep:   \n",
      "   Head:  . \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "\n",
      "\n",
      "  -  \n",
      "   Dep:   \n",
      "   Head:  \n",
      "\n",
      " \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Lieutenant  -  \n",
      "   Dep:   \n",
      "   Head:  Lieutenant \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Maurice  -  \n",
      "   Dep:   \n",
      "   Head:  Maurice \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Frere  -  \n",
      "   Dep:   \n",
      "   Head:  Frere \n",
      "   Pos:   \n",
      "   Child:  []\n",
      ",  -  \n",
      "   Dep:   \n",
      "   Head:  , \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "late  -  \n",
      "   Dep:   \n",
      "   Head:  late \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "in  -  \n",
      "   Dep:   \n",
      "   Head:  in \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "command  -  \n",
      "   Dep:   \n",
      "   Head:  command \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "at  -  \n",
      "   Dep:   \n",
      "   Head:  at \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Maria  -  \n",
      "   Dep:   \n",
      "   Head:  Maria \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "Island  -  \n",
      "   Dep:   \n",
      "   Head:  Island \n",
      "   Pos:   \n",
      "   Child:  []\n",
      ",  -  \n",
      "   Dep:   \n",
      "   Head:  , \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "had  -  \n",
      "   Dep:   \n",
      "   Head:  had \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "unexpectedly  -  \n",
      "   Dep:   \n",
      "   Head:  unexpectedly \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "\n",
      "  -  \n",
      "   Dep:   \n",
      "   Head:  \n",
      " \n",
      "   Pos:  SPACE \n",
      "   Child:  []\n",
      "come  -  \n",
      "   Dep:   \n",
      "   Head:  come \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "down  -  \n",
      "   Dep:   \n",
      "   Head:  down \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "with  -  \n",
      "   Dep:   \n",
      "   Head:  with \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "news  -  \n",
      "   Dep:   \n",
      "   Head:  news \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "from  -  \n",
      "   Dep:   \n",
      "   Head:  from \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "head  -  \n",
      "   Dep:   \n",
      "   Head:  head \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "-  -  \n",
      "   Dep:   \n",
      "   Head:  - \n",
      "   Pos:   \n",
      "   Child:  []\n",
      "quarters  -  \n",
      "   Dep:   \n",
      "   Head:  quarters \n",
      "   Pos:   \n",
      "   Child:  []\n",
      ".  -  \n",
      "   Dep:   \n",
      "   Head:  . \n",
      "   Pos:   \n",
      "   Child:  []\n"
     ]
    }
   ],
   "source": [
    "for token in doc[9:59]:\n",
    "    print(token.text,\" - \", \n",
    "          \"\\n   Dep: \",token.dep_,        \n",
    "          \"\\n   Head: \",token.head.text,  \n",
    "          \"\\n   Pos: \",token.head.pos_,   \n",
    "          \"\\n   Child: \",[child for child in token.children])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what you are interested in is the NER. Any text sent down the pipeline with the NER will get a list of entities that have been found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vickers [ PERSON ]\n",
      "this evening [ TIME ]\n",
      "December 3rd [ DATE ]\n",
      "Maurice Frere [ PERSON ]\n",
      "Maria Island [ LOC ]\n",
      "Ladybird [ ORG ]\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, \"[\",entity.label_,\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each entity is labelled with a category. The categories are defined by the model as it is trained to recognise them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] UPDATE__\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    The NER categories classified by this spaCy model include:\n",
    "   <ul>\n",
    "       <li><strong>CARDINAL:</strong> Numerals that do not fall under another type</li>\n",
    "<li><strong>DATE:</strong> Absolute or relative dates or periods</li>\n",
    "<li><strong>EVENT:</strong> Named hurricanes, battles, wars, sports events, etc.</li>\n",
    "<li><strong>FAC:</strong> Buildings, airports, highways, bridges, etc.</li>\n",
    "<li><strong>GPE:</strong> Countries, cities, states</li>\n",
    "<li><strong>LANGUAGE:</strong> Any named language</li>\n",
    "<li><strong>LAW:</strong> Named documents made into laws.</li>\n",
    "<li><strong>LOC:</strong> Non-GPE locations, mountain ranges, bodies of water</li>\n",
    "<li><strong>MONEY:</strong> Monetary values, including unit</li>\n",
    "<li><strong>NORP:</strong> Nationalities or religious or political groups</li>\n",
    "<li><strong>ORDINAL:</strong> \"first\", \"second\", etc.</li>\n",
    "<li><strong>ORG:</strong> Companies, agencies, institutions, etc.</li>\n",
    "<li><strong>PERCENT:</strong> Percentage, including \"%\"</li>\n",
    "<li><strong>PERSON:</strong> People, including fictional</li>\n",
    "<li><strong>PRODUCT:</strong> Objects, vehicles, foods, etc. (not services)</li>\n",
    "<li><strong>QUANTITY:</strong> Measurements, as of weight or distance</li>\n",
    "<li><strong>TIME:</strong> Times smaller than a day</li>\n",
    "<li><strong>WORK_OF_ART:</strong> Titles of books, songs, etc.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# [TO DO] Remove now that the info box is updated\n",
    "cats = ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
    "for c in cats:\n",
    "    e=spacy.explain(c)\n",
    "    print(\"<li><strong>\"+c+\":</strong> \"+e+\"</li>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Update__\n",
    "- one category per NE\n",
    "- not always correct\n",
    "- some NE are MWE\n",
    "- MWE subsume smaller NEs, e.g., Maria Island is only a LOC, not a PERSON and a LOC\n",
    "- If you need a more precise training/categories, you need a new model, or to post process\n",
    "\n",
    "Most tokenised terms in the sentence have O as their NER value (that is the letter O not the number 0). Some however have been categorised. For instance, Van and Diemen are both classified as being PERSON named entities, Tasman is an ORGANIZATION and Cape and Pillar are in the LOCATION category. These categories are specific to Stanza. There are two key levels of processing available - the normal level will only identify the categories LOCATION, ORGANIZATION and PERSON, but the high recall processing will also consider other specialised phrases like TIME and MONEY which are not really named entities. Stanza can also look for the categories used in the KBP competition like CITY, COUNTRY and NATIONALITY, but this fine-grained processing will be slower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some judgement is needed to work out which NEs correspond to placenames. Understanding the context of word usage is important. Luckily, the data for the entities includes the character position for the start and the end of the NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vickers (57,64) [ PERSON ]\n",
      "this evening (113,125) [ TIME ]\n",
      "December 3rd (129,141) [ DATE ]\n",
      "Maurice Frere (171,184) [ PERSON ]\n",
      "Maria Island (205,217) [ LOC ]\n",
      "Ladybird (487,495) [ ORG ]\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, \"(\"+str(entity.start_char)+\",\"+str(entity.end_char)+\") [\",entity.label_,\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token will also have a value that indicates whether it is part of an NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [  ]\n",
      "the [  ]\n",
      "house [  ]\n",
      "of [  ]\n",
      "Major [  ]\n",
      "Vickers [ PERSON ]\n",
      ", [  ]\n",
      "Commandant [  ]\n",
      "of [  ]\n",
      "Macquarie [  ]\n",
      "Harbour [  ]\n",
      ", [  ]\n",
      "\n",
      " [  ]\n",
      "there [  ]\n",
      "was [  ]\n",
      ", [  ]\n",
      "on [  ]\n",
      "this [ TIME ]\n",
      "evening [ TIME ]\n",
      "of [  ]\n",
      "December [ DATE ]\n",
      "3rd [ DATE ]\n",
      ", [  ]\n",
      "unusual [  ]\n",
      "gaiety [  ]\n",
      ". [  ]\n",
      "\n",
      "\n",
      " [  ]\n",
      "Lieutenant [  ]\n",
      "Maurice [ PERSON ]\n",
      "Frere [ PERSON ]\n",
      ", [  ]\n",
      "late [  ]\n",
      "in [  ]\n",
      "command [  ]\n",
      "at [  ]\n",
      "Maria [ LOC ]\n",
      "Island [ LOC ]\n",
      ", [  ]\n",
      "had [  ]\n",
      "unexpectedly [  ]\n",
      "\n",
      " [  ]\n",
      "come [  ]\n",
      "down [  ]\n",
      "with [  ]\n",
      "news [  ]\n",
      "from [  ]\n",
      "head [  ]\n",
      "- [  ]\n",
      "quarters [  ]\n",
      ". [  ]\n"
     ]
    }
   ],
   "source": [
    "for token in doc[9:59]:\n",
    "    print(token.text, \"[\", token.ent_type_, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Update__\n",
    "\n",
    "The annotations so far show that *Cape* and *Pillar* are both a LOCATION NE, but not that *Cape Pillar* is actually the complete name of the location. However Stanza does also recognise multi-word expressions (MWEs), even though it recognises that they are part of an NE. Each *Sentence* annotated by Stanza has a list of [NE *Mentions*](https://stanfordnlp.github.io/CoreNLP/entitymentions.html) which are also given NER categories as their *Type*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Remove any comments and code about hand-coding corrections for exceptions?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to hand-code entities after the NER has been done. This can help make up for any common irregularities with the NER for your input documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this model doesn't recognise that FB is a NE."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#sampletext = \"FB is hiring a new vice president of global policy\"\n",
    "\n",
    "#doc = nlp(sampletext)\n",
    "ents = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "print('Entities before:', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to create a new entry for the list of entities."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a spaCy span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(ent.text, ent.start, ent.end, ent.label_) for ent in doc.ents]\n",
    "print('Entities after:', ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the data for the tokens is updated. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for token in doc[0:8]:\n",
    "    print(token.text, \"[\", token.ent_type_, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Decide whether to keep this part about batch processing. This may be too spaCy specific.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy also allows the input documents to be processed in batches. This helps better manage the processing demands of the system throughout the pipeline when there are multiple files or many sentences."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# multiple texts in a list\n",
    "sampletexts = [\"Autonomous cars shift insurance liability toward manufacturers\",\"This is a text\", \"These are lots of texts\", \"...\"]\n",
    "\n",
    "# remove what elements you don't need from the pipeline\n",
    "for doc in nlp.pipe(sampletexts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    print(\"Entities: \",[(ent.text, ent.label_) for ent in doc.ents])\n",
    "    for token in doc:\n",
    "        print(\"   \",token.text, \"[\", token.ent_type_, \"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can process the output of the piped pipeline straight away, you can't print it unless you convert it into a list."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docs = nlp.pipe(sampletexts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(list(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] This is about spaCy indicating the scope of a NE.\n",
    "Probably best to omit as it is too technical and about the spaCy rather than the problem.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    From <a href=\"https://spacy.io/usage/linguistic-features\">https://spacy.io/usage/linguistic-features</a>:\n",
    "\n",
    "<ul>\n",
    "    <li>IOB SCHEME\n",
    "        <ul>\n",
    "<li>I – Token is inside an entity.</li>\n",
    "<li>O – Token is outside an entity.</li>\n",
    "<li>B – Token is the beginning of an entity.</li>\n",
    "        </ul></li>\n",
    "<li>BILUO SCHEME\n",
    "    <ul>\n",
    "<li>B – Token is the beginning of a multi-token entity.</li>\n",
    "<li>I – Token is inside a multi-token entity.</li>\n",
    "<li>L – Token is the last token of a multi-token entity.</li>\n",
    "<li>U – Token is a single-token unit entity.</li>\n",
    "<li>O – Token is outside an entity.</li>\n",
    "    </ul></li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# [TO DO] move the import to the start of the Notebook\n",
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "doc = nlp(text[0:500])\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(\"Entities: \",ents)\n",
    "\n",
    "print(\"\\nBefore:\", doc.ents)  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = np.zeros((len(doc), len(header)), dtype=\"uint64\")\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[\"GPE\"]\n",
    "doc.from_array(header, attr_array)\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(\"\\nEntities: \",ents)\n",
    "\n",
    "print(\"\\nAfter\", doc.ents)  # [London]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] talk about extracting just the NER types we want.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Run through a single chapter (variable: text) before doing the entire collection?\n",
    "    This will allow all NEs to be shown, then the filter be introduced.\n",
    "    This will put it more on topic.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER III.\\n\\nA SOCIAL EVENING.\\n\\n\\n\\nIn the house of Major Vickers, Commandant of Macquarie Harbour,\\nthere was, on this evening of December 3rd, unusual gaiety.\\n\\nLieutenant Maurice Frere, late in command at Maria Island, had unexpectedly\\ncome down with news from head-quarters.  The Ladybird, Government schooner,\\nvisited the settlement on ordinary occasions twice a year, and such visits\\nwere looked forward to with no little eagerness by the settlers.\\nTo the convicts the arrival of the Ladybird mean'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1\t\tVickers                       \tPERSON\n",
      "    2\t\tthis evening                  \tTIME\n",
      "    3\t\tDecember 3rd                  \tDATE\n",
      "    4\t\tMaurice Frere                 \tPERSON\n",
      "    5\t\tMaria Island                  \tLOC\n",
      "    6\t\tLadybird                      \tORG\n",
      "    7\t\tLadybird                      \tPERSON\n",
      "    8\t\tLadybird                      \tORG\n",
      "    9\t\tTom                           \tPERSON\n",
      "   10\t\tDick                          \tPERSON\n",
      "   11\t\tHarry                         \tPERSON\n",
      "   12\t\tbush                          \tPERSON\n",
      "   13\t\tJack                          \tPERSON\n",
      "   14\t\tTown Gaol                     \tPERSON\n",
      "   15\t\tLadybird                      \tORG\n",
      "   16\t\tone                           \tCARDINAL\n",
      "   17\t\tCommandant                    \tORG\n",
      "   18\t\tVickers                       \tPERSON\n",
      "   19\t\tArthur                        \tPERSON\n",
      "   20\t\tHobart Town                   \tORG\n",
      "   21\t\tArthur                        \tPERSON\n",
      "   22\t\tTasman                        \tPERSON\n",
      "   23\t\tPeninsula                     \tLOC\n",
      "   24\t\tPort Arthur                   \tORG\n",
      "   25\t\tMaurice Frere                 \tPERSON\n",
      "   26\t\tVickers                       \tORG\n",
      "   27\t\tMacquarie Harbour             \tORG\n",
      "   28\t\tLieutenant Frere              \tPRODUCT\n",
      "   29\t\tNine years before             \tDATE\n",
      "   30\t\tArthur                        \tPERSON\n",
      "   31\t\tHonduras                      \tGPE\n",
      "   32\t\tSorrell                       \tPERSON\n",
      "   33\t\tfirst                         \tORDINAL\n",
      "   34\t\tArthur                        \tORG\n",
      "   35\t\tArthur                        \tPERSON\n",
      "   36\t\tthe 2nd November, 1829        \tDATE\n",
      "   37\t\tthirty-eight                  \tCARDINAL\n",
      "   38\t\tfifty-six                     \tQUANTITY\n",
      "   39\t\tthe 26th of September the same year\tDATE\n",
      "   40\t\tseven hundred and forty-five  \tQUANTITY\n",
      "   41\t\tSundays                       \tDATE\n",
      "   42\t\ttwenty                        \tCARDINAL\n",
      "   43\t\tSeven                         \tCARDINAL\n",
      "   44\t\t1826                          \tDATE\n",
      "   45\t\tHobart Town                   \tORG\n",
      "   46\t\tfirst                         \tORDINAL\n",
      "   47\t\tSaturday                      \tDATE\n",
      "   48\t\tsecond                        \tORDINAL\n",
      "   49\t\tthird                         \tORDINAL\n",
      "   50\t\tSaturday                      \tDATE\n",
      "   51\t\tafternoon                     \tTIME\n",
      "   52\t\tfourth                        \tORDINAL\n",
      "   53\t\tfifth                         \tORDINAL\n",
      "   54\t\tsixth                         \tORDINAL\n",
      "   55\t\tseventh                       \tORDINAL\n",
      "   56\t\tMaria Island                  \tLOC\n",
      "   57\t\tHells Gates                   \tORG\n",
      "   58\t\tone year                      \tDATE\n",
      "   59\t\teighty-five                   \tCARDINAL\n",
      "   60\t\tonly thirty                   \tCARDINAL\n",
      "   61\t\ttwenty-seven                  \tCARDINAL\n",
      "   62\t\teight                         \tCARDINAL\n",
      "   63\t\tthree                         \tCARDINAL\n",
      "   64\t\ttwelve                        \tCARDINAL\n",
      "   65\t\t1822                          \tDATE\n",
      "   66\t\tone hundred and sixty-nine    \tCARDINAL\n",
      "   67\t\tone hundred and eighty-two    \tCARDINAL\n",
      "   68\t\ttwo thousand                  \tCARDINAL\n",
      "   69\t\tthe ten years                 \tDATE\n",
      "   70\t\tone hundred and twelve        \tCARDINAL\n",
      "   71\t\tsixty-two                     \tQUANTITY\n",
      "   72\t\tArthur                        \tORG\n",
      "   73\t\tMaurice Frere                 \tPERSON\n",
      "   74\t\tone                           \tCARDINAL\n",
      "   75\t\tThe six years                 \tDATE\n",
      "   76\t\tEngland                       \tGPE\n",
      "   77\t\tfive years'                   \tDATE\n",
      "   78\t\tMaria Island                  \tLOC\n",
      "   79\t\tVickers                       \tPERSON\n",
      "   80\t\tVickers                       \tORG\n",
      "   81\t\tVickers                       \tPERSON\n",
      "   82\t\tsix years                     \tDATE\n",
      "   83\t\tJohn                          \tPERSON\n",
      "   84\t\tFrere                         \tPERSON\n",
      "   85\t\tSylvia                        \tGPE\n",
      "   86\t\tHobart Town                   \tORG\n",
      "   87\t\tJohn                          \tPERSON\n",
      "   88\t\tSylvia                        \tPERSON\n",
      "   89\t\tVickers                       \tORG\n",
      "   90\t\tPurfoy                        \tPERSON\n",
      "   91\t\tVickers                       \tPERSON\n",
      "   92\t\tFrere                         \tPERSON\n",
      "   93\t\tVickers                       \tPERSON\n",
      "   94\t\tRex                           \tPERSON\n",
      "   95\t\tSylvia                        \tGPE\n",
      "   96\t\tJohn                          \tPERSON\n",
      "   97\t\tFrere                         \tPERSON\n",
      "   98\t\tthe six months                \tDATE\n",
      "   99\t\tHobart Town                   \tPRODUCT\n",
      "  100\t\tSylvia                        \tPERSON\n",
      "  101\t\tFrere                         \tPERSON\n",
      "  102\t\tPurfoy                        \tPERSON\n",
      "  103\t\tRex                           \tPERSON\n",
      "  104\t\tRex                           \tPERSON\n",
      "  105\t\tEngland                       \tGPE\n",
      "  106\t\tFrere                         \tPERSON\n",
      "  107\t\tNew Town                      \tGPE\n",
      "  108\t\tRex                           \tPERSON\n",
      "  109\t\tFrere                         \tPERSON\n",
      "  110\t\tLaunceston                    \tPERSON\n",
      "  111\t\tSydney                        \tPERSON\n",
      "  112\t\tRex                           \tPERSON\n",
      "  113\t\tPine                          \tPERSON\n",
      "  114\t\tSylvia                        \tPERSON\n",
      "  115\t\tJohn                          \tPERSON\n",
      "  116\t\tVickers                       \tORG\n",
      "  117\t\tverandah.-She                 \tORG\n",
      "  118\t\tVickers                       \tPERSON\n",
      "  119\t\tVickers                       \tORG\n",
      "  120\t\tone                           \tCARDINAL\n",
      "  121\t\ttwo                           \tCARDINAL\n",
      "  122\t\tGeorge                        \tPERSON\n",
      "  123\t\tVickers                       \tPERSON\n",
      "  124\t\tBarton                        \tPERSON\n",
      "  125\t\ttwelve                        \tCARDINAL\n",
      "  126\t\tmore than fifty               \tCARDINAL\n",
      "  127\t\tGrummet Island                \tLOC\n",
      "  128\t\tweek                          \tDATE\n",
      "  129\t\tGrummet                       \tLOC\n",
      "  130\t\ta month or so                 \tDATE\n",
      "  131\t\tMaria                         \tPERSON\n",
      "  132\t\tVickers                       \tPERSON\n",
      "  133\t\tMalabar                       \tGPE\n",
      "  134\t\tthe first year                \tDATE\n",
      "  135\t\tBarton                        \tPERSON\n",
      "  136\t\tDawes                         \tORG\n",
      "  137\t\t'29                           \tDATE\n",
      "  138\t\tSaid                          \tPERSON\n",
      "  139\t\tOsprey                        \tPERSON\n",
      "  140\t\tFrere                         \tPERSON\n",
      "  141\t\tfifty                         \tCARDINAL\n",
      "  142\t\tAbout six weeks ago           \tDATE\n",
      "  143\t\tGabbett                       \tPERSON\n",
      "  144\t\tGabbett                       \tPERSON\n",
      "  145\t\tthree                         \tCARDINAL\n",
      "  146\t\tVickers                       \tORG\n",
      "  147\t\tMacquarie Harbour             \tORG\n",
      "  148\t\tthe end of the month          \tDATE\n",
      "  149\t\tVickers                       \tORG\n",
      "  150\t\tFrere                         \tORG\n",
      "  151\t\tfirst                         \tORDINAL\n",
      "  152\t\tMaria                         \tPERSON\n",
      "  153\t\tGeorge                        \tPERSON\n",
      "  154\t\tVickers                       \tORG\n",
      "  155\t\tFrere                         \tPERSON\n",
      "  156\t\tArthur                        \tPERSON\n",
      "  157\t\tGeorge                        \tPERSON\n",
      "  158\t\tVickers                       \tORG\n",
      "  159\t\tBatten                        \tPERSON\n",
      "  160\t\tVickers                       \tPERSON\n",
      "  161\t\tLadybird                      \tPERSON\n",
      "  162\t\tVickers                       \tPERSON\n",
      "  163\t\tOsprey                        \tPERSON\n",
      "  164\t\tSylvia                        \tPERSON\n",
      "  165\t\tFrere                         \tPERSON\n",
      "  166\t\tLadybird                      \tORG\n",
      "  167\t\tOsprey                        \tPERSON\n",
      "  168\t\tVickers                       \tPERSON\n",
      "  169\t\tVickers                       \tORG\n",
      "  170\t\tFrere                         \tORG\n",
      "  171\t\tSylvia                        \tPERSON\n",
      "  172\t\tVickers                       \tORG\n",
      "  173\t\tSylvia                        \tPERSON\n",
      "  174\t\tMalabar                       \tGPE\n",
      "  175\t\tsome eleven years old         \tDATE\n",
      "  176\t\tFrere                         \tPERSON\n",
      "  177\t\tSylvia                        \tPERSON\n",
      "  178\t\tFrere                         \tPERSON\n",
      "  179\t\tFrere                         \tPERSON\n",
      "  180\t\tFrere                         \tPERSON\n",
      "  181\t\tSylvia                        \tGPE\n",
      "  182\t\tSylvia                        \tPERSON\n",
      "  183\t\tFrere                         \tORG\n",
      "  184\t\tLieutenant Frere              \tWORK_OF_ART\n",
      "  185\t\tSylvia                        \tGPE\n",
      "  186\t\tRufus Dawes                   \tWORK_OF_ART\n",
      "  187\t\tDanny                         \tPERSON\n",
      "  188\t\tDanny                         \tPERSON\n",
      "  189\t\tFrere                         \tPERSON\n",
      "  190\t\tVickers                       \tORG\n",
      "  191\t\tSylvia                        \tPERSON\n",
      "  192\t\tDanny                         \tPERSON\n",
      "  193\t\tSylvia                        \tPERSON\n",
      "  194\t\tLondon                        \tGPE\n",
      "  195\t\tLondon                        \tGPE\n",
      "  196\t\tDanny                         \tPERSON\n",
      "  197\t\tFancy Danny's                 \tPERSON\n",
      "  198\t\tPapa                          \tWORK_OF_ART\n",
      "  199\t\tWill Danny                    \tPERSON\n",
      "  200\t\tSylvia                        \tPERSON\n",
      "  201\t\thalf an hour                  \tTIME\n",
      "  202\t\tVickers                       \tORG\n",
      "  203\t\tVickers                       \tORG\n",
      "  204\t\tthree or four years           \tDATE\n",
      "  205\t\tSydney                        \tGPE\n",
      "  206\t\tEngland                       \tGPE\n",
      "  207\t\tTroke                         \tORG\n",
      "  208\t\tGabbett                       \tPERSON\n",
      "  209\t\tFrere                         \tPERSON\n",
      "  210\t\tGabbett                       \tPERSON\n",
      "  211\t\tsix weeks                     \tDATE\n",
      "  212\t\tTroke                         \tORG\n",
      "  213\t\tVickers                       \tORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# document level\n",
    "entities = [(entity.text, entity.start_char, entity.end_char, entity.label_) for entity in doc.ents]\n",
    "    \n",
    "i=0 # entity counter\n",
    "# token level\n",
    "for entity in doc.ents:\n",
    "    print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,entity.text, entity.label_))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Expand on this explanation with example context.\n",
    "Talk about the issue with _Van Diemen's_ versus _Van Diemen's Land_ (and _Tasman's Head_)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These categories are assigned according to the context in which the NE is used. For this reason, _Van Diemen's_ is considered an *ORG*, a *PERSON* and a *FAC*, depending on its linguistic context. Note also that _VAN DIEMEN'S LAND_ in the title of the chapter isn't recognised as a NE, probably due to its unconventional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[TO DO] Expand on this explanation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, not all of these NE are suitable for placenames, so you will need to make a list of what categories regularly contain placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACENAME_CATEGORIES = [\"LOC\", \"GPE\", \"FAC\", \"ORG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
