{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATAP Notebook for the Geolocation project\n",
    "\n",
    "This notebook helps you access the Geolocation tools in a Python development environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "* [Premise](#section-premise)\n",
    "* [Requirements](#section-requirements) \n",
    "* [Data Preparation](#section-datapreparation)\n",
    "* [Named Entity Recognition](#section-ner)\n",
    " * [Look for NEs](#section-nes)\n",
    " * [Reviewing Candidate Placenames](#section-reviewplacenames)\n",
    "* [Finding Locations for Placenames](#section-findinglocs)\n",
    " * [Identifying States and Capitals](#section-statescapitals)\n",
    " * [Searching a Gazzetteer for Locations](#section-searchgazetteer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise <a class=\"anchor\" id=\"section-premise\"></a>\n",
    "*This section explains the Geolocation project and tools.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geolocation project relates to doctoral research done by [Fiannuala Morgan](https://finnoscarmorgan.github.io/) at the Australian National University. It uses software to identify placenames in archived historical texts, then compares them to data about known locations to identify where the placenames may be located. \n",
    "\n",
    "This notebook is designed to allow you to perform similar operations on textual documents.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "It will teach you how to\n",
    "<ul>\n",
    "    <li>use the spaCy library to identify and classify Named Entities (NEs)</li>\n",
    "    <li>identify multi-word expressions (MWE) that are NEs</li>\n",
    "    <li>search for spatial data about specific locations or places in gazetteers of such data</li>\n",
    "    <li>determine which locations are referred to by placenames, based on the context in which they are used in a text</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements <a class=\"anchor\" id=\"section-requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "This notebook uses various Python libraries. Most will come with your Python installation, but the following are crucial:\n",
    "<ul>\n",
    "    <li> pandas</li> \n",
    "    <li> json</li> \n",
    "    <li> nltk</li> \n",
    "    <li> geopandas</li> \n",
    "    <li> shapely  </li> \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UPDATE\n",
    "# Many of these are probably not needed.\n",
    "\n",
    "import os\n",
    "from pickle import NONE\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geopandas is used to work with spatial data\n",
    "# If you have issues installing it on a MAcOS, \n",
    "# see https://stackoverflow.com/questions/71137617/error-installing-geopandas-in-python-on-mac-m1\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# NLTK is used to work with textual data \n",
    "#from nltk.tag import StanfordNERTagger\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "# spaCy is used for a pipeline of NLP functions\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "# Shapely is used to work with geometric shapes\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Fuzzywuzzy is used for fuzzy searches\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# used for the checklist\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation <a class=\"anchor\" id=\"section-datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also want to set up some directories for the import and output of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare the data directories\n",
    "## This presumes that Notebooks/ is the current working directory  \n",
    "text_directory = os.path.normpath(\"../Texts/\")\n",
    "csv_directory = os.path.normpath(\"../ner_output/\")\n",
    "#maps_directory = os.path.normpath(\"../maps/\")\n",
    "\n",
    "## Create the data directories\n",
    "if not os.path.exists(text_directory):\n",
    "    os.makedirs(text_directory)\n",
    "if not os.path.exists(csv_directory):\n",
    "   os.makedirs(csv_directory)\n",
    "#if not os.path.exists(maps_directory):\n",
    "#    os.makedirs(maps_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we will be examining the text of *For the Term of His Natural Life*, an 1874CE novel by Marcus Clarke that is in the public domain. Our copy was obtained via the [Gutenburg Project Australia](https://gutenberg.net.au/ebooks/e00016.txt). It is an unformatted textfile. We have slightly simplified it further by reducing it to only standard ASCII characters, replacing any accented characters with their unaccented forms and the British Pound Sterling symbol with the word pounds. \n",
    "\n",
    "The novel is divided into four books, each based in different regions of the world. You can start with the second book, which is titled *BOOK II.\\-\\-MACQUARIE HARBOUR.  1833*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename=\"FtToHNL_BOOK_1.txt\"\n",
    "filename=\"FtToHNL_BOOK_2.txt\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "text_filename = os.path.basename(textlocation)\n",
    "\n",
    "text = open(textlocation, encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no more than a long string of characters. So far, you have done no processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0:499] # look at the first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition <a class=\"anchor\" id=\"section-ner\"></a>\n",
    "*This section provides tools on identifying named entities in textual data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for NEs <a class=\"anchor\" id=\"section-nes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entities (NEs) are proper noun phrases within text, like names of places, people or organisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various packages that can include Named Enity Recognition (NER), e.g., the [Stanza CoreNLP](https://colab.research.google.com/github/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb), the Stanford NER, and the spaCy library. They often combine machine learning and a rule-based system to identify NEs and classify them into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you will be using the spaCy NER - https://spacy.io/usage/linguistic-features#morphology .  This is available as a Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy allows you to load a language model that has been trained on various examples of the language of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy will automatically run the model through various levels of natural language processing. This pipeline includes tokenising the text into individual tokens or terms, like words, values and puncuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "It is simple to use the client. You just tell it to annotate the text. However, Stanza allows you to specify what to annotate the text with. For instance, you might want it to tokenise the terms, label their parts-of-speech and lemmatised forms as well as recognising any NEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The options for the Stanza client include:<ul>\n",
    "    <li> <strong>tokenize - </strong> split into words or terms </li> \n",
    "      <li>    <strong>ssplit - </strong> split into sentences or independent statements</li> \n",
    "     <li>     <strong>pos -  </strong> syntactic parts-of-speech</li> \n",
    "      <li>    <strong>lemma -  </strong> lemmatised form (not always a root form)</li> \n",
    "      <li>    <strong>ner - </strong> named entity recognition</li> \n",
    "      <li>    <strong>depparse -  </strong> parsing of dependencies</li> \n",
    "      <li>    <strong>coref - </strong> co-reference resolution</li> \n",
    "      <li>    <strong>kbppandas - </strong> KBP competition format</li> \n",
    "</ul>\n",
    "A full explanation can be found at <a href=\"https://stanfordnlp.github.io/CoreNLP/pipeline.html\">https://stanfordnlp.github.io/CoreNLP/pipeline.html</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the default line contains the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sent to the spaCy model will be processed by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "sampletext = \"Autonomous cars shift insurance liability toward manufacturers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the pipeline is then  available in the output structure of the spaCy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\" - \", \"Morph: \",token.morph, \n",
    "          \"\\n   Dep: \",token.dep_, \n",
    "          \"\\n   Head: \",token.head.text, \n",
    "          \"\\n   Pos: \",token.head.pos_,\n",
    "          \"\\n   Child: \",[child for child in token.children])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you might not want some of this pipeline processing as it may not be beneficial to your analysis. Any unneeded processing will also slow the system down and place a greater demand on the memory. This is particularly true of the parser. Luckily, it is easy to stipulate what you want excluded from the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(sampletext, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\" - \", \"Morph: \",token.morph, \n",
    "          \"\\n   Dep: \",token.dep_, \n",
    "          \"\\n   Head: \",token.head.text, \n",
    "          \"\\n   Pos: \",token.head.pos_,\n",
    "          \"\\n   Child: \",[child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what you are interested in is the NER. Each sentence sent down the pipeline with the ner will get a list of entities that have been found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"Apple is looking at buying a U.K. startup based in London for $1 billion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"[\",ent.label_,\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each entity is labelled with a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: UPDATE\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    The NER categories classified by Stanza include:\n",
    "   <ul>\n",
    "<li><strong>Default:</strong>\n",
    "LOCATION, ORGANIZATION, PERSON</li>\n",
    "<li><strong>High recall: </strong>\n",
    "DATE, LOCATION, MONEY, ORGANIZATION, PERCENT, PERSON, TIME, MISC</li>\n",
    "<li><strong>KBP fine-grained:</strong>\n",
    "CAUSE_OF_DEATH, CITY, COUNTRY, CRIMINAL_CHARGE, EMAIL, HANDLE,\n",
    "IDEOLOGY, NATIONALITY, RELIGION, STATE_OR_PROVINCE, TITLE, URL</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "Most tokenised terms in the sentence have O as their NER value (that is the letter O not the number 0). Some however have been categorised. For instance, Van and Diemen are both classified as being PERSON named entities, Tasman is an ORGANIZATION and Cape and Pillar are in the LOCATION category. These categories are specific to Stanza. There are two key levels of processing available - the normal level will only identify the categories LOCATION, ORGANIZATION and PERSON, but the high recall processing will also consider other specialised phrases like TIME and MONEY which are not really named entities. Stanza can also look for the categories used in the KBP competition like CITY, COUNTRY and NATIONALITY, but this fine-grained processing will be slower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the entities includes the character position for the start and the end of the NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token will also have a value that indicates whether it is part of an NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"[\", token.ent_type_, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "The annotations so far show that *Cape* and *Pillar* are both a LOCATION NE, but not that *Cape Pillar* is actually the complete name of the location. However Stanza does also recognise multi-word expressions (MWEs), even though it recognises that they are part of an NE. Each *Sentence* annotated by Stanza has a list of [NE *Mentions*](https://stanfordnlp.github.io/CoreNLP/entitymentions.html) which are also given NER categories as their *Type*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to hand-code entities after the NER has been done. This can help make up for any common irregularities with the NER for your input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this model doesn't recognise that FB is a NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"FB is hiring a new vice president of global policy\"\n",
    "\n",
    "doc = nlp(sampletext)\n",
    "ents = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "print('Entities before:', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to create a new entry for the list of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spaCy span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(ent.text, ent.start, ent.end, ent.label_) for ent in doc.ents]\n",
    "print('Entities after:', ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the data for the tokens is updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"[\", token.ent_type_, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy also allows the input documents to be processed in batches. This helps better manage the processing demands of the system throughout the pipeline when there are multiple files or many sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple texts in a list\n",
    "sampletexts = [\"Autonomous cars shift insurance liability toward manufacturers\",\"This is a text\", \"These are lots of texts\", \"...\"]\n",
    "\n",
    "# remove what elements you don't need from the pipeline\n",
    "for doc in nlp.pipe(sampletexts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    print(\"Entities: \",[(ent.text, ent.label_) for ent in doc.ents])\n",
    "    for token in doc:\n",
    "        print(\"   \",token.text, \"[\", token.ent_type_, \"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can process the output of the piped pipeline straight away, you can't print it unless you convert it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(sampletexts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Can't remember what this is trying to do. Think it has to do with the IOB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#doc = nlp.make_doc(\"London is a big city in the United Kingdom and New York is in the United States of America.\")\n",
    "doc = nlp(\"London is a big city in the United Kingdom and New York is in the United States of America.\")\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(\"Entities: \",ents)\n",
    "\n",
    "print(\"\\nBefore:\", doc.ents)  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = np.zeros((len(doc), len(header)), dtype=\"uint64\")\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[\"GPE\"]\n",
    "doc.from_array(header, attr_array)\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(\"\\nEntities: \",ents)\n",
    "\n",
    "print(\"\\nAfter\", doc.ents)  # [London]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Placeholder in-case we want to show of the displacy rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sampletext)\n",
    "\n",
    "# displacy from spaCy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: talk about extracting just the NER types we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('LOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('FAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('ORG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Run through a single chapter (variable: text) before doing the entire collection?\n",
    "    This will allow all NEs to be shown, then the filter be introduced.\n",
    "    This will put it more on topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0:499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "    \n",
    "i=0 # entity counter\n",
    "# token level\n",
    "for e in doc.ents:\n",
    "    print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,e.text, e.label_))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Expand on this explanation with example context.\n",
    "Talk about the issue with _Van Diemen's_ versus _Van Diemen's Land_ (and _Tasman's Head_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These categories are assigned according to the context in which the NE is used. For this reason, _Van Diemen's_ is considered an *ORG*, a *PERSON* and a *FAC*, depending on its linguistic context. Note also that _VAN DIEMEN'S LAND_ in the title of the chapter isn't recognised as a NE, probably due to its unconventional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Expand on this explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, not all of these NE are suitable for placenames, so you will need to make a list of what categories regularly contain placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACENAME_CATEGORIES = [\"LOC\", \"GPE\", \"FAC\", \"ORG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Candidate Placenames <a class=\"anchor\" id=\"section-reviewplacenames\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now put all of this together and find the placenames that are identified by spaCy in each chapter of the text. They can all be collected in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where we store the details about each instance of the placenames\n",
    "placenames_df = pd.DataFrame(columns=['Book','Chapter',\"NEIndex\",\"Placename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which chapters and books you want to annotate\n",
    "CHAPTERS=[1,2,3] \n",
    "BOOKS=[1,2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You should define what spaCy processing you do or don't want in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabledPipeline=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process FtToHNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "i=0 # counter of the entities\n",
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        filename = \"FtToHNL_BOOK_\"+str(b)+\"_CHAPTER_\"+str(c)+\".txt\" \n",
    "        # set the specific path for the 'filename'\n",
    "        textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "        text_filename = os.path.basename(textlocation)\n",
    "\n",
    "        # read this chapter\n",
    "        text = open(textlocation, encoding=\"utf-8\").read()\n",
    "        print(\"Working on |\",filename)\n",
    "        \n",
    "        # run spaCy    \n",
    "        doc = nlp(text,disable=disabledPipeline)\n",
    "\n",
    "        # document level\n",
    "        ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "\n",
    "        # token level\n",
    "        for e in doc.ents:\n",
    "            if e.label_ in PLACENAME_CATEGORIES: # filter out MONEY, DATE etc\n",
    "                print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,e.text, e.label_))\n",
    "                # To help understand the context of the text, extract the occurence\n",
    "                context_text=doc.text[e.start_char-30:e.end_char+30].replace(\"\\n\",\" \")\n",
    "                \n",
    "                # Code to render with displacy\n",
    "                #context_doc={\"text\":str(i+1)+\" \\t \"+context_text,\n",
    "                #             \"ents\":[{\"start\": len(str(i+1)+\"   \")+30, \n",
    "                #                      \"end\":   len(str(i+1)+\"   \"+context_text)-30, \n",
    "                #                      \"label\": e.label_}],\n",
    "                #             \"title\": None}\n",
    "                #print(context_doc)\n",
    "                #displacy.render(context_doc, style=\"ent\", manual=True, jupyter=True)\n",
    "\n",
    "                # find the placenames according to spaCy\n",
    "                new_placename = {'Book':b,              # The Book number\n",
    "                                'Chapter':c,            # The Chapter number\n",
    "                                'NEIndex':i,            # A reference number to the nth Named Entity \n",
    "                                'Placename':e.text,     # The placename in the text\n",
    "                                'Category':e.label_,    # The spaCy category\n",
    "                                'Context':context_text, # The textual context where the placename was found\n",
    "                                'Approval':1}        # A flag for whether this is a suitable placename\n",
    "                placenames_df = placenames_df.append(new_placename, ignore_index=True)\n",
    "                \n",
    "            i=i+1 # entity counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df[['Book','Chapter','NEIndex','Placename','Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a lot more placenames were found in Book 2 than Book 1. This makes sense since Book 1 is set on board an ocean voyage, whereas Book 2 is at Macquarie Harbour in Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a number of NEs that are unlikely to be placenames, regardless of what spaCy categoriesd them as. It is best to consider the context in which the terms were used. Use the checkboxes to select which terms you do consider to be placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ipywidgets as widgets\n",
    "\n",
    "def changed(b):\n",
    "    # The system sets the _property_lock_, changes the value, then releases the _property_lock_.\n",
    "    # The confusing thing is that there is a value for the checkbox and a value for the property lock.\n",
    "    # changed() is called three times for every change to the checkbox.\n",
    "\n",
    "    # If you want to see any change to the checkbox value, uncomment this print()) command\n",
    "    if b['name']=='value':\n",
    "        #print(b,\"\\n\")\n",
    "        #print(\"found value\")\n",
    "        k=b['new']\n",
    "        #print(\"    \",k)\n",
    "    #print(b,\"\\n\")\n",
    "\n",
    "placename_items=[]\n",
    "context_items=[]\n",
    "num_items=[]\n",
    "\n",
    "# Time to try to make checkboxes for every placename for b in BOOKS:\n",
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        # Get the NEs from this book and chapter\n",
    "        pbc=placenames_df[(placenames_df[\"Book\"]==b) & (placenames_df[\"Chapter\"]==c)]\n",
    "        for i in pbc[\"NEIndex\"]:\n",
    "            context_text=pbc[pbc[\"NEIndex\"]==i][\"Context\"].values[0]\n",
    "            category=pbc[pbc[\"NEIndex\"]==i][\"Category\"].values[0]\n",
    "            \n",
    "        # Make lists of the candidate placenames, context text and index numbers \n",
    "        # Only the placenames are given a checkbox. \n",
    "        placename_items = placename_items + [widgets.Checkbox(True,description=i) for i in pbc[\"Placename\"]]\n",
    "        context_items = context_items + [widgets.Label(pbc[pbc[\"NEIndex\"]==i][\"Context\"].values[0]) for i in pbc[\"NEIndex\"]]\n",
    "        num_items = num_items + [widgets.Label(str(i)) for i in pbc[\"NEIndex\"]]\n",
    "\n",
    "# create a display\n",
    "num_placenames=len(placename_items)\n",
    "left_box = widgets.VBox(placename_items)\n",
    "right_box = widgets.VBox(context_items)\n",
    "num_box = widgets.VBox(num_items)\n",
    "whole_box = widgets.HBox([num_box, left_box, right_box])\n",
    "        \n",
    "display(whole_box)\n",
    "\n",
    "# respond to any changes in the checkboxes\n",
    "for i in range(num_placenames):\n",
    "    placename_items[i].observe(changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now copy all the values from the checkboxes to the data, so you know which placenames you have approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the status of each checklist item to the data\n",
    "for i in range(num_placenames):\n",
    "    #print(num_items[i].value,placename_items[i].value,placename_items[i].description)\n",
    "\n",
    "    NEIndex_num = int(num_items[i].value)\n",
    "    approval_flag = placename_items[i].value\n",
    "    \n",
    "    #print(\"Looking for [\"+str(NEIndex_num)+\"]\")\n",
    "    \n",
    "    # set the flag to match the checklist\n",
    "    for p in placenames_df[\"NEIndex\"]:\n",
    "        if (p-NEIndex_num == 0):\n",
    "            placenames_df.loc[placenames_df[\"NEIndex\"] == NEIndex_num,\"Approval\"] = approval_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now visualise the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df[['NEIndex','Placename','Approval']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can extract the final list of distinct placenames that you have approved. While the names aren't sorted (though they could be), if you missed unselecting an NE on the checklist, this will help find it. All you need to do is go back to the checklist, unselect it, then run all other steps from there to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a unique list of the approved placenames\n",
    "approved_placenames = placenames_df[placenames_df[\"Approval\"]==True]['Placename'].unique()\n",
    "approved_placenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save this new data to a csv file. You have already defined the directory for your data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_placenames.txt\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving to placename data to \",save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list \n",
    "# using the savetxt from the numpy module\n",
    "np.savetxt(save_location, \n",
    "           approved_placenames,\n",
    "           delimiter =\", \", \n",
    "           fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Remove the MWE section as it is not needed for spaCy.\n",
    "Stopped the code form executing for now, but kept for reference while updating the above steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWEs as Named Entities  <a class=\"anchor\" id=\"section-mwes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotations so far show that *Cape* and *Pillar* are both a LOCATION NE, but not that *Cape Pillar* is actually the complete name of the location. However Stanza does also recognise multi-word expressions (MWEs), even though it recognises that they are part of an NE. Each *Sentence* annotated by Stanza has a list of [NE *Mentions*](https://stanfordnlp.github.io/CoreNLP/entitymentions.html) which are also given NER categories as their *Type*. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    print(\"{}\\t{:30s}\\t{}\".format(\"Sentence\", \"Mention\", \"Type\"))\n",
    "    for i, sent in enumerate(document.sentence):\n",
    "        for m in sent.mentions:\n",
    "            print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,m.entityMentionText, m.entityType))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this isn't perfect. While *Van Diemen* is recognised as a *PERSON* NE, *Van Diemen's Land* (i.e., the former name for Tasmania) isn't recognised as a *LOCATION*. This is because Stanza is trained to only recognise certain combinations of words and categories as a new MWE category. These rules can however be added to but this workshop won't explore this aspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token is also annotated with an index number corresponding to any Mention it is part of. Each token can only be part of one Mention. While the Mentions may be annotated per Sentence, the index number is actually considering all Sentences and Mentions in the annotated text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TotalNumMentions=0 # keep a running list of how many NEs are identified\n",
    "print(\"{:15s}\\t{:15s}\\t{:6s}\\t{:10s}\\t{:10s}\\t{}\".format(\"word\",\"lemma\",\"POS\",\"NER\",\"Mention Index\",\"Mention\"))\n",
    "\n",
    "for i, sent in enumerate(document.sentence):\n",
    "    NumMentions = len(sent.mentions) # count how many NEs annotated to this sentence\n",
    "    if NumMentions:\n",
    "        print(\"\\n[Sentence {}]\".format(i+1))\n",
    "        for t in sent.token:\n",
    "            # only output tokens that are in NEs\n",
    "            if t.ner != 'O': # Letter, not zero!\n",
    "                # get the full text for the NE\n",
    "                mentionStr=sent.mentions[t.entityMentionIndex -  TotalNumMentions].entityMentionText                \n",
    "                print(\"{:15s}\\t{:15s}\\t{:6s}\\t{:10s}\\t{:10}\\t{}\".format(t.word, \n",
    "                                                                 t.lemma, \n",
    "                                                                 t.pos, \n",
    "                                                                 t.ner, \n",
    "                                                                 t.entityMentionIndex, \n",
    "                                                                 mentionStr))\n",
    "    TotalNumMentions += NumMentions # add to the running total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you are mainly interested in the Named Entities relate to locations. The location-based NER categories used by Stanza are:\n",
    "* *LOCATION*\n",
    "* *CITY*\n",
    "* *COUNTRY*\n",
    "* *STATE_OR_PROVINCE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NATIONALITY* might also be considered but it may depend on whether you care about phrases like *student of English history* or *Frenchman's cap*, or not.\n",
    "It is easy to filter out all other NEs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NATIONALITY is ignored \n",
    "LOCATION_CATEGORIES = ['LOCATION','CITY','COUNTRY','STATE_OR_PROVINCE']\n",
    "\n",
    "# Iterate over all detected entity mentions\n",
    "print(\"{:30s}\\t{}\".format(\"Mention\", \"Type\"))\n",
    "\n",
    "STANZA_Locations = [] # list of locations Stanza recognises\n",
    "for sent in document.sentence:\n",
    "    for m in sent.mentions:\n",
    "        if (m.entityType in LOCATION_CATEGORIES): # ignore any nonLocation NEs\n",
    "            print(\"{:30s}\\t{}\".format(m.entityMentionText, m.entityType))\n",
    "            STANZA_Locations.append(m.entityMentionText)\n",
    "            \n",
    "print(\"\\n{}\".format(STANZA_Locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now put all of this together and find the placenames that are identified by Stanza in each chapter of the text. They can all be collected in a single dataframe."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "placenames = pd.DataFrame(columns=['Book','Chapter',\"NEIndex\",\"Placename\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# define which chapters and books you want to annotate\n",
    "CHAPTERS=[1,2,3] \n",
    "BOOKS=[1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\[MN Note\\]__ Change this to a background server, rather than a server on demand?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        filename = \"FtToHNL_BOOK_\"+str(b)+\"_CHAPTER_\"+str(c)+\".txt\" \n",
    "        # set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "        textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "        text_filename = os.path.basename(textlocation)\n",
    "\n",
    "        # read this chapter\n",
    "        text = open(textlocation, encoding=\"utf-8\").read()\n",
    "        print(\"Working on |\",filename)\n",
    "        \n",
    "        # run the Stanza server & client\n",
    "        # https://nlp.stanford.edu/software/regexner.html\n",
    "        # java -mx1g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP \n",
    "        #      -annotators 'tokenize,ssplit,pos,lemma,ner' -file JuliaGillard.txt\n",
    "        # java -mx1g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP \n",
    "        #      -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -file JuliaGillard.txt\n",
    "        #      -regexner.mapping jg-regexner.txt\n",
    "        #\n",
    "        # with CoreNLPClient(properties={\n",
    "        #  'annotators': 'tokenize,ssplit,pos',\n",
    "        #  'pos.model': '/path/to/custom-model.ser.gz'\n",
    "        # }) as client:\n",
    "        #with CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','regexner'], \n",
    "        #           memory='4G', \n",
    "        #           annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','regexner'], \n",
    "        #           endpoint='http://localhost:9001', \n",
    "        #           be_quiet=True) as client:\n",
    "        with CoreNLPClient(properties={\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,regexner', \n",
    "            'regexner.mapping':'jg-regexner.txt',\n",
    "            'memory':'4G', \n",
    "            'endpoint':'http://localhost:9001', \n",
    "            'be_quiet':True\n",
    "        }) as client:\n",
    "            # Annotate the text (presuming it is not too large)\n",
    "            document = client.annotate(text)\n",
    "        \n",
    "        # find the placenames according to Stanza\n",
    "        for sent in document.sentence:\n",
    "            for m in sent.mentions:\n",
    "                if (m.entityType in LOCATION_CATEGORIES):\n",
    "                    new_placename = {'Book':b,\n",
    "                                    'Chapter':c,\n",
    "                                    'NEIndex':m.entityMentionIndex,\n",
    "                                    'Placename':m.entityMentionText,\n",
    "                                    'Category':m.entityType}\n",
    "                    placenames = placenames.append(new_placename, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a lot more placenames were found in Book 2 than Book 1. This makes sense since Book 1 is set on board an ocean voyage, whereas Book 2 is at Macquarie Harbour in Australia.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save this new data to a csv file. You have already defined the directory for your data files."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filename = \"FtToHNL_placenames_v2.txt\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving placename data to \",save_location)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "placenames.to_csv(save_location)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Locations for the Placenames <a class=\"anchor\" id=\"section-findinglocs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a list of placenames from the text, the next step is to work out their location on Earth. For this you can use a combination of specialised lists of locations, gazzetteers and heuristics. The objective is to match every placename with the coordinates of a known location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to read the file of your placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename=\"FtToHNL_BOOK_1.txt\"\n",
    "filename=\"FtToHNL_placenames.txt\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "data_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "data_filename = os.path.basename(data_location)\n",
    "\n",
    "# Using pandas, read the csv file. This will place it in a dataframe format. \n",
    "placenames_df = pd.read_csv(data_location, encoding=\"utf-8\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying States and Capitals <a class=\"anchor\" id=\"section-statescapitals\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some placenames, like *High Street* or *Maryborough*, may be very common across the world, or even in Australia. However, certain placenames refer to significant locations, like states, territories, large geographic features or capital cities. As such, if they are mentioned in a text, the placename is more likely to refer to the major location than a town or village in Tasmania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These significant locations are a finite set. They can be defined in a reference file that can be reused when reviewing the placenames of any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching a Gazzetteer for Locations <a class=\"anchor\" id=\"section-searchgazzeteer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO BE DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
