{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATAP Notebook for the Geolocation project\n",
    "\n",
    "This notebook helps you access the Geolocation tools in a Python development environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "* [Premise](#section-premise)\n",
    "* [Requirements](#section-requirements) \n",
    "* [Data Preparation](#section-datapreparation)\n",
    "* [Named Entity Recognition](#section-ner)\n",
    " * [Look for NEs](#section-nes)\n",
    " * [Reviewing Candidate Placenames](#section-reviewplacenames)\n",
    "* [Finding Locations for Placenames](#section-findinglocs)\n",
    " * [Identifying States and Capitals](#section-statescapitals)\n",
    " * [Searching a Gazzetteer for Locations](#section-searchgazetteer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise <a class=\"anchor\" id=\"section-premise\"></a>\n",
    "*This section explains the Geolocation project and tools.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geolocation project relates to doctoral research done by [Fiannuala Morgan](https://finnoscarmorgan.github.io/) at the Australian National University. It uses software to identify placenames in archived historical texts, then compares them to data about known locations to identify where the placenames may be located. \n",
    "\n",
    "This notebook is designed to allow you to perform similar operations on textual documents.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "It will teach you how to\n",
    "<ul>\n",
    "    <li>use the spaCy library to identify and classify Named Entities (NEs)</li>\n",
    "    <li>identify multi-word expressions (MWE) that are NEs</li>\n",
    "    <li>search for spatial data about specific locations or places in gazetteers of such data</li>\n",
    "    <li>determine which locations are referred to by placenames, based on the context in which they are used in a text</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements <a class=\"anchor\" id=\"section-requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "This notebook uses various Python libraries. Most will come with your Python installation, but the following are crucial:\n",
    "<ul>\n",
    "    <li> pandas</li> \n",
    "    <li> json</li> \n",
    "    <li> nltk</li> \n",
    "    <li> geopandas</li> \n",
    "    <li> shapely  </li> \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UPDATE\n",
    "# Many of these are probably not needed.\n",
    "\n",
    "import os\n",
    "from pickle import NONE\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geopandas is used to work with spatial data\n",
    "# If you have issues installing it on a MAcOS, \n",
    "# see https://stackoverflow.com/questions/71137617/error-installing-geopandas-in-python-on-mac-m1\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# NLTK is used to work with textual data \n",
    "#from nltk.tag import StanfordNERTagger\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "# spaCy is used for a pipeline of NLP functions\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "# Shapely is used to work with geometric shapes\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Fuzzywuzzy is used for fuzzy searches\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# used for the checklist\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation <a class=\"anchor\" id=\"section-datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also want to set up some directories for the import and output of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare the data directories\n",
    "## This presumes that Notebooks/ is the current working directory  \n",
    "text_directory = os.path.normpath(\"../Texts/\")\n",
    "csv_directory = os.path.normpath(\"../ner_output/\")\n",
    "reference_directory = os.path.normpath(\"../Data\")\n",
    "#maps_directory = os.path.normpath(\"../maps/\")\n",
    "\n",
    "## Create the data directories\n",
    "if not os.path.exists(text_directory):\n",
    "    os.makedirs(text_directory)\n",
    "if not os.path.exists(csv_directory):\n",
    "   os.makedirs(csv_directory)\n",
    "if not os.path.exists(reference_directory):\n",
    "   os.makedirs(reference_directory)\n",
    "\n",
    "#if not os.path.exists(maps_directory):\n",
    "#    os.makedirs(maps_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we will be examining the text of *For the Term of His Natural Life*, an 1874CE novel by Marcus Clarke that is in the public domain. Our copy was obtained via the [Gutenburg Project Australia](https://gutenberg.net.au/ebooks/e00016.txt). It is an unformatted textfile. We have slightly simplified it further by reducing it to only standard ASCII characters, replacing any accented characters with their unaccented forms and the British Pound Sterling symbol with the word pounds. \n",
    "\n",
    "The novel is divided into four books, each based in different regions of the world. You can start with the second book, which is titled *BOOK II.\\-\\-MACQUARIE HARBOUR.  1833*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename=\"FtToHNL_BOOK_1.txt\"\n",
    "filename=\"FtToHNL_BOOK_2.txt\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "text_filename = os.path.basename(textlocation)\n",
    "\n",
    "text = open(textlocation, encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no more than a long string of characters. So far, you have done no processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0:499] # look at the first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition <a class=\"anchor\" id=\"section-ner\"></a>\n",
    "*This section provides tools on identifying named entities in textual data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for NEs <a class=\"anchor\" id=\"section-nes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entities (NEs) are proper noun phrases within text, like names of places, people or organisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various packages that can include Named Enity Recognition (NER), e.g., the [Stanza CoreNLP](https://colab.research.google.com/github/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb), the Stanford NER, and the spaCy library. They often combine machine learning and a rule-based system to identify NEs and classify them into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you will be using the spaCy NER - https://spacy.io/usage/linguistic-features#morphology .  This is available as a Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy allows you to load a language model that has been trained on various examples of the language of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy will automatically run the model through various levels of natural language processing. This pipeline includes tokenising the text into individual tokens or terms, like words, values and puncuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "It is simple to use the client. You just tell it to annotate the text. However, Stanza allows you to specify what to annotate the text with. For instance, you might want it to tokenise the terms, label their parts-of-speech and lemmatised forms as well as recognising any NEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The options for the Stanza client include:<ul>\n",
    "    <li> <strong>tokenize - </strong> split into words or terms </li> \n",
    "      <li>    <strong>ssplit - </strong> split into sentences or independent statements</li> \n",
    "     <li>     <strong>pos -  </strong> syntactic parts-of-speech</li> \n",
    "      <li>    <strong>lemma -  </strong> lemmatised form (not always a root form)</li> \n",
    "      <li>    <strong>ner - </strong> named entity recognition</li> \n",
    "      <li>    <strong>depparse -  </strong> parsing of dependencies</li> \n",
    "      <li>    <strong>coref - </strong> co-reference resolution</li> \n",
    "      <li>    <strong>kbppandas - </strong> KBP competition format</li> \n",
    "</ul>\n",
    "A full explanation can be found at <a href=\"https://stanfordnlp.github.io/CoreNLP/pipeline.html\">https://stanfordnlp.github.io/CoreNLP/pipeline.html</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the default line contains the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sent to the spaCy model will be processed by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "sampletext = \"Autonomous cars shift insurance liability toward manufacturers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the pipeline is then  available in the output structure of the spaCy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\" - \", \"Morph: \",token.morph, \n",
    "          \"\\n   Dep: \",token.dep_, \n",
    "          \"\\n   Head: \",token.head.text, \n",
    "          \"\\n   Pos: \",token.head.pos_,\n",
    "          \"\\n   Child: \",[child for child in token.children])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you might not want some of this pipeline processing as it may not be beneficial to your analysis. Any unneeded processing will also slow the system down and place a greater demand on the memory. This is particularly true of the parser. Luckily, it is easy to stipulate what you want excluded from the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(sampletext, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\" - \", \"Morph: \",token.morph, \n",
    "          \"\\n   Dep: \",token.dep_, \n",
    "          \"\\n   Head: \",token.head.text, \n",
    "          \"\\n   Pos: \",token.head.pos_,\n",
    "          \"\\n   Child: \",[child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what you are interested in is the NER. Each sentence sent down the pipeline with the ner will get a list of entities that have been found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"Apple is looking at buying a U.K. startup based in London for $1 billion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"[\",ent.label_,\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each entity is labelled with a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: UPDATE\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    The NER categories classified by Stanza include:\n",
    "   <ul>\n",
    "<li><strong>Default:</strong>\n",
    "LOCATION, ORGANIZATION, PERSON</li>\n",
    "<li><strong>High recall: </strong>\n",
    "DATE, LOCATION, MONEY, ORGANIZATION, PERCENT, PERSON, TIME, MISC</li>\n",
    "<li><strong>KBP fine-grained:</strong>\n",
    "CAUSE_OF_DEATH, CITY, COUNTRY, CRIMINAL_CHARGE, EMAIL, HANDLE,\n",
    "IDEOLOGY, NATIONALITY, RELIGION, STATE_OR_PROVINCE, TITLE, URL</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "Most tokenised terms in the sentence have O as their NER value (that is the letter O not the number 0). Some however have been categorised. For instance, Van and Diemen are both classified as being PERSON named entities, Tasman is an ORGANIZATION and Cape and Pillar are in the LOCATION category. These categories are specific to Stanza. There are two key levels of processing available - the normal level will only identify the categories LOCATION, ORGANIZATION and PERSON, but the high recall processing will also consider other specialised phrases like TIME and MONEY which are not really named entities. Stanza can also look for the categories used in the KBP competition like CITY, COUNTRY and NATIONALITY, but this fine-grained processing will be slower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the entities includes the character position for the start and the end of the NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token will also have a value that indicates whether it is part of an NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"[\", token.ent_type_, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "The annotations so far show that *Cape* and *Pillar* are both a LOCATION NE, but not that *Cape Pillar* is actually the complete name of the location. However Stanza does also recognise multi-word expressions (MWEs), even though it recognises that they are part of an NE. Each *Sentence* annotated by Stanza has a list of [NE *Mentions*](https://stanfordnlp.github.io/CoreNLP/entitymentions.html) which are also given NER categories as their *Type*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to hand-code entities after the NER has been done. This can help make up for any common irregularities with the NER for your input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this model doesn't recognise that FB is a NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"FB is hiring a new vice president of global policy\"\n",
    "\n",
    "doc = nlp(sampletext)\n",
    "ents = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "print('Entities before:', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to create a new entry for the list of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spaCy span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(ent.text, ent.start, ent.end, ent.label_) for ent in doc.ents]\n",
    "print('Entities after:', ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the data for the tokens is updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"[\", token.ent_type_, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy also allows the input documents to be processed in batches. This helps better manage the processing demands of the system throughout the pipeline when there are multiple files or many sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple texts in a list\n",
    "sampletexts = [\"Autonomous cars shift insurance liability toward manufacturers\",\"This is a text\", \"These are lots of texts\", \"...\"]\n",
    "\n",
    "# remove what elements you don't need from the pipeline\n",
    "for doc in nlp.pipe(sampletexts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    print(\"Entities: \",[(ent.text, ent.label_) for ent in doc.ents])\n",
    "    for token in doc:\n",
    "        print(\"   \",token.text, \"[\", token.ent_type_, \"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can process the output of the piped pipeline straight away, you can't print it unless you convert it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(sampletexts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Can't remember what this is trying to do. Think it has to do with the IOB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#doc = nlp.make_doc(\"London is a big city in the United Kingdom and New York is in the United States of America.\")\n",
    "doc = nlp(\"London is a big city in the United Kingdom and New York is in the United States of America.\")\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(\"Entities: \",ents)\n",
    "\n",
    "print(\"\\nBefore:\", doc.ents)  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = np.zeros((len(doc), len(header)), dtype=\"uint64\")\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[\"GPE\"]\n",
    "doc.from_array(header, attr_array)\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(\"\\nEntities: \",ents)\n",
    "\n",
    "print(\"\\nAfter\", doc.ents)  # [London]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Placeholder in-case we want to show of the displacy rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sampletext)\n",
    "\n",
    "# displacy from spaCy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: talk about extracting just the NER types we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('LOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('FAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('ORG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Run through a single chapter (variable: text) before doing the entire collection?\n",
    "    This will allow all NEs to be shown, then the filter be introduced.\n",
    "    This will put it more on topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0:499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "    \n",
    "i=0 # entity counter\n",
    "# token level\n",
    "for e in doc.ents:\n",
    "    print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,e.text, e.label_))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Expand on this explanation with example context.\n",
    "Talk about the issue with _Van Diemen's_ versus _Van Diemen's Land_ (and _Tasman's Head_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These categories are assigned according to the context in which the NE is used. For this reason, _Van Diemen's_ is considered an *ORG*, a *PERSON* and a *FAC*, depending on its linguistic context. Note also that _VAN DIEMEN'S LAND_ in the title of the chapter isn't recognised as a NE, probably due to its unconventional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Expand on this explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, not all of these NE are suitable for placenames, so you will need to make a list of what categories regularly contain placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACENAME_CATEGORIES = [\"LOC\", \"GPE\", \"FAC\", \"ORG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Candidate Placenames <a class=\"anchor\" id=\"section-reviewplacenames\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now put all of this together and find the placenames that are identified by spaCy in each chapter of the text. They can all be collected in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where we store the details about each instance of the placenames\n",
    "placenames_df = pd.DataFrame(columns=['Book','Chapter',\"NEIndex\",\"Placename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which chapters and books you want to annotate\n",
    "CHAPTERS=[1,2,3] \n",
    "BOOKS=[1,2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You should define what spaCy processing you do or don't want in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabledPipeline=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process FtToHNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "i=0 # counter of the entities\n",
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        filename = \"FtToHNL_BOOK_\"+str(b)+\"_CHAPTER_\"+str(c)+\".txt\" \n",
    "        # set the specific path for the 'filename'\n",
    "        textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "        text_filename = os.path.basename(textlocation)\n",
    "\n",
    "        # read this chapter\n",
    "        text = open(textlocation, encoding=\"utf-8\").read()\n",
    "        print(\"Working on |\",filename)\n",
    "        \n",
    "        # run spaCy    \n",
    "        doc = nlp(text,disable=disabledPipeline)\n",
    "\n",
    "        # document level\n",
    "        ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "\n",
    "        # token level\n",
    "        for e in doc.ents:\n",
    "            if e.label_ in PLACENAME_CATEGORIES: # filter out MONEY, DATE etc\n",
    "                print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,e.text, e.label_))\n",
    "                # To help understand the context of the text, extract the occurence\n",
    "                context_text=doc.text[e.start_char-30:e.end_char+30].replace(\"\\n\",\" \")\n",
    "                \n",
    "                # Code to render with displacy\n",
    "                #context_doc={\"text\":str(i+1)+\" \\t \"+context_text,\n",
    "                #             \"ents\":[{\"start\": len(str(i+1)+\"   \")+30, \n",
    "                #                      \"end\":   len(str(i+1)+\"   \"+context_text)-30, \n",
    "                #                      \"label\": e.label_}],\n",
    "                #             \"title\": None}\n",
    "                #print(context_doc)\n",
    "                #displacy.render(context_doc, style=\"ent\", manual=True, jupyter=True)\n",
    "\n",
    "                # find the placenames according to spaCy\n",
    "                new_placename = {'Book':b,              # The Book number\n",
    "                                'Chapter':c,            # The Chapter number\n",
    "                                'NEIndex':i,            # A reference number to the nth Named Entity \n",
    "                                'Placename':e.text,     # The placename in the text\n",
    "                                'Category':e.label_,    # The spaCy category\n",
    "                                'Context':context_text, # The textual context where the placename was found\n",
    "                                'Approval':1}        # A flag for whether this is a suitable placename\n",
    "                placenames_df = placenames_df.append(new_placename, ignore_index=True)\n",
    "                \n",
    "            i=i+1 # entity counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df[['Book','Chapter','NEIndex','Placename','Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a lot more placenames were found in Book 2 than Book 1. This makes sense since Book 1 is set on board an ocean voyage, whereas Book 2 is at Macquarie Harbour in Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a number of NEs that are unlikely to be placenames, regardless of what spaCy categoriesd them as. It is best to consider the context in which the terms were used. Use the checkboxes to select which terms you do consider to be placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ipywidgets as widgets\n",
    "\n",
    "def changed(b):\n",
    "    # The system sets the _property_lock_, changes the value, then releases the _property_lock_.\n",
    "    # The confusing thing is that there is a value for the checkbox and a value for the property lock.\n",
    "    # changed() is called three times for every change to the checkbox.\n",
    "\n",
    "    # If you want to see any change to the checkbox value, uncomment this print()) command\n",
    "    if b['name']=='value':\n",
    "        #print(b,\"\\n\")\n",
    "        #print(\"found value\")\n",
    "        k=b['new']\n",
    "        #print(\"    \",k)\n",
    "    #print(b,\"\\n\")\n",
    "\n",
    "placename_items=[]\n",
    "context_items=[]\n",
    "num_items=[]\n",
    "\n",
    "# Time to try to make checkboxes for every placename for b in BOOKS:\n",
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        # Get the NEs from this book and chapter\n",
    "        pbc=placenames_df[(placenames_df[\"Book\"]==b) & (placenames_df[\"Chapter\"]==c)]\n",
    "        for i in pbc[\"NEIndex\"]:\n",
    "            context_text=pbc[pbc[\"NEIndex\"]==i][\"Context\"].values[0]\n",
    "            category=pbc[pbc[\"NEIndex\"]==i][\"Category\"].values[0]\n",
    "            \n",
    "        # Make lists of the candidate placenames, context text and index numbers \n",
    "        # Only the placenames are given a checkbox. \n",
    "        placename_items = placename_items + [widgets.Checkbox(True,description=i) for i in pbc[\"Placename\"]]\n",
    "        context_items = context_items + [widgets.Label(pbc[pbc[\"NEIndex\"]==i][\"Context\"].values[0]) for i in pbc[\"NEIndex\"]]\n",
    "        num_items = num_items + [widgets.Label(str(i)) for i in pbc[\"NEIndex\"]]\n",
    "\n",
    "# create a display\n",
    "num_placenames=len(placename_items)\n",
    "left_box = widgets.VBox(placename_items)\n",
    "right_box = widgets.VBox(context_items)\n",
    "num_box = widgets.VBox(num_items)\n",
    "whole_box = widgets.HBox([num_box, left_box, right_box])\n",
    "        \n",
    "display(whole_box)\n",
    "\n",
    "# respond to any changes in the checkboxes\n",
    "for i in range(num_placenames):\n",
    "    placename_items[i].observe(changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now copy all the values from the checkboxes to the data, so you know which placenames you have approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the status of each checklist item to the data\n",
    "for i in range(num_placenames):\n",
    "    #print(num_items[i].value,placename_items[i].value,placename_items[i].description)\n",
    "\n",
    "    NEIndex_num = int(num_items[i].value)\n",
    "    approval_flag = placename_items[i].value\n",
    "    \n",
    "    #print(\"Looking for [\"+str(NEIndex_num)+\"]\")\n",
    "    \n",
    "    # set the flag to match the checklist\n",
    "    for p in placenames_df[\"NEIndex\"]:\n",
    "        if (p-NEIndex_num == 0):\n",
    "            placenames_df.loc[placenames_df[\"NEIndex\"] == NEIndex_num,\"Approval\"] = approval_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now visualise the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df[['NEIndex','Placename','Approval']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can extract the final list of distinct placenames that you have approved. While the names aren't sorted (though they could be), if you missed unselecting an NE on the checklist, this will help find it. All you need to do is go back to the checklist, unselect it, then run all other steps from there to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a unique list of the approved placenames\n",
    "approved_placenames = placenames_df[placenames_df[\"Approval\"]==True]['Placename'].unique()\n",
    "approved_placenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save this new data to a csv file. You have already defined the directory for your data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_placenames.csv\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving to placename data to \",save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list \n",
    "# using the savetxt from the numpy module\n",
    "np.savetxt(save_location, \n",
    "           approved_placenames,\n",
    "           delimiter =\", \", \n",
    "           fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Remove the MWE section as it is not needed for spaCy.\n",
    "Stopped the code form executing for now, but kept for reference while updating the above steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWEs as Named Entities  <a class=\"anchor\" id=\"section-mwes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotations so far show that *Cape* and *Pillar* are both a LOCATION NE, but not that *Cape Pillar* is actually the complete name of the location. However Stanza does also recognise multi-word expressions (MWEs), even though it recognises that they are part of an NE. Each *Sentence* annotated by Stanza has a list of [NE *Mentions*](https://stanfordnlp.github.io/CoreNLP/entitymentions.html) which are also given NER categories as their *Type*. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    print(\"{}\\t{:30s}\\t{}\".format(\"Sentence\", \"Mention\", \"Type\"))\n",
    "    for i, sent in enumerate(document.sentence):\n",
    "        for m in sent.mentions:\n",
    "            print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,m.entityMentionText, m.entityType))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this isn't perfect. While *Van Diemen* is recognised as a *PERSON* NE, *Van Diemen's Land* (i.e., the former name for Tasmania) isn't recognised as a *LOCATION*. This is because Stanza is trained to only recognise certain combinations of words and categories as a new MWE category. These rules can however be added to but this workshop won't explore this aspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token is also annotated with an index number corresponding to any Mention it is part of. Each token can only be part of one Mention. While the Mentions may be annotated per Sentence, the index number is actually considering all Sentences and Mentions in the annotated text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TotalNumMentions=0 # keep a running list of how many NEs are identified\n",
    "print(\"{:15s}\\t{:15s}\\t{:6s}\\t{:10s}\\t{:10s}\\t{}\".format(\"word\",\"lemma\",\"POS\",\"NER\",\"Mention Index\",\"Mention\"))\n",
    "\n",
    "for i, sent in enumerate(document.sentence):\n",
    "    NumMentions = len(sent.mentions) # count how many NEs annotated to this sentence\n",
    "    if NumMentions:\n",
    "        print(\"\\n[Sentence {}]\".format(i+1))\n",
    "        for t in sent.token:\n",
    "            # only output tokens that are in NEs\n",
    "            if t.ner != 'O': # Letter, not zero!\n",
    "                # get the full text for the NE\n",
    "                mentionStr=sent.mentions[t.entityMentionIndex -  TotalNumMentions].entityMentionText                \n",
    "                print(\"{:15s}\\t{:15s}\\t{:6s}\\t{:10s}\\t{:10}\\t{}\".format(t.word, \n",
    "                                                                 t.lemma, \n",
    "                                                                 t.pos, \n",
    "                                                                 t.ner, \n",
    "                                                                 t.entityMentionIndex, \n",
    "                                                                 mentionStr))\n",
    "    TotalNumMentions += NumMentions # add to the running total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you are mainly interested in the Named Entities relate to locations. The location-based NER categories used by Stanza are:\n",
    "* *LOCATION*\n",
    "* *CITY*\n",
    "* *COUNTRY*\n",
    "* *STATE_OR_PROVINCE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NATIONALITY* might also be considered but it may depend on whether you care about phrases like *student of English history* or *Frenchman's cap*, or not.\n",
    "It is easy to filter out all other NEs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NATIONALITY is ignored \n",
    "LOCATION_CATEGORIES = ['LOCATION','CITY','COUNTRY','STATE_OR_PROVINCE']\n",
    "\n",
    "# Iterate over all detected entity mentions\n",
    "print(\"{:30s}\\t{}\".format(\"Mention\", \"Type\"))\n",
    "\n",
    "STANZA_Locations = [] # list of locations Stanza recognises\n",
    "for sent in document.sentence:\n",
    "    for m in sent.mentions:\n",
    "        if (m.entityType in LOCATION_CATEGORIES): # ignore any nonLocation NEs\n",
    "            print(\"{:30s}\\t{}\".format(m.entityMentionText, m.entityType))\n",
    "            STANZA_Locations.append(m.entityMentionText)\n",
    "            \n",
    "print(\"\\n{}\".format(STANZA_Locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now put all of this together and find the placenames that are identified by Stanza in each chapter of the text. They can all be collected in a single dataframe."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "placenames = pd.DataFrame(columns=['Book','Chapter',\"NEIndex\",\"Placename\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# define which chapters and books you want to annotate\n",
    "CHAPTERS=[1,2,3] \n",
    "BOOKS=[1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\[MN Note\\]__ Change this to a background server, rather than a server on demand?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        filename = \"FtToHNL_BOOK_\"+str(b)+\"_CHAPTER_\"+str(c)+\".txt\" \n",
    "        # set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "        textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "        text_filename = os.path.basename(textlocation)\n",
    "\n",
    "        # read this chapter\n",
    "        text = open(textlocation, encoding=\"utf-8\").read()\n",
    "        print(\"Working on |\",filename)\n",
    "        \n",
    "        # run the Stanza server & client\n",
    "        # https://nlp.stanford.edu/software/regexner.html\n",
    "        # java -mx1g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP \n",
    "        #      -annotators 'tokenize,ssplit,pos,lemma,ner' -file JuliaGillard.txt\n",
    "        # java -mx1g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP \n",
    "        #      -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -file JuliaGillard.txt\n",
    "        #      -regexner.mapping jg-regexner.txt\n",
    "        #\n",
    "        # with CoreNLPClient(properties={\n",
    "        #  'annotators': 'tokenize,ssplit,pos',\n",
    "        #  'pos.model': '/path/to/custom-model.ser.gz'\n",
    "        # }) as client:\n",
    "        #with CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','regexner'], \n",
    "        #           memory='4G', \n",
    "        #           annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','regexner'], \n",
    "        #           endpoint='http://localhost:9001', \n",
    "        #           be_quiet=True) as client:\n",
    "        with CoreNLPClient(properties={\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,regexner', \n",
    "            'regexner.mapping':'jg-regexner.txt',\n",
    "            'memory':'4G', \n",
    "            'endpoint':'http://localhost:9001', \n",
    "            'be_quiet':True\n",
    "        }) as client:\n",
    "            # Annotate the text (presuming it is not too large)\n",
    "            document = client.annotate(text)\n",
    "        \n",
    "        # find the placenames according to Stanza\n",
    "        for sent in document.sentence:\n",
    "            for m in sent.mentions:\n",
    "                if (m.entityType in LOCATION_CATEGORIES):\n",
    "                    new_placename = {'Book':b,\n",
    "                                    'Chapter':c,\n",
    "                                    'NEIndex':m.entityMentionIndex,\n",
    "                                    'Placename':m.entityMentionText,\n",
    "                                    'Category':m.entityType}\n",
    "                    placenames = placenames.append(new_placename, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a lot more placenames were found in Book 2 than Book 1. This makes sense since Book 1 is set on board an ocean voyage, whereas Book 2 is at Macquarie Harbour in Australia.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save this new data to a csv file. You have already defined the directory for your data files."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filename = \"FtToHNL_placenames_v2.txt\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving placename data to \",save_location)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "placenames.to_csv(save_location)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Locations for the Placenames <a class=\"anchor\" id=\"section-findinglocs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a list of placenames from the text, the next step is to work out their location on Earth. For this you can use a combination of specialised lists of locations, gazzetteers and heuristics. The objective is to match every placename with the coordinates of a known location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to read the file of your placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on |  FtToHNL_placenames.csv\n"
     ]
    }
   ],
   "source": [
    "filename=\"FtToHNL_placenames.csv\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "data_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "data_filename = os.path.basename(data_location)\n",
    "\n",
    "# Using pandas, read the csv file. This will place it in a dataframe format. \n",
    "placenames_df = pd.read_csv(data_location, encoding=\"utf-8\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df = placenames_df.rename(columns={placenames_df.columns[0]: 'Placename'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Placename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the sleepy sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the Bay of Biscay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Van Diemen's Land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Grummet Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Grummet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Malabar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Dawes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Placename\n",
       "0      the sleepy sea\n",
       "1   the Bay of Biscay\n",
       "2               Heath\n",
       "3              London\n",
       "4   Van Diemen's Land\n",
       "..                ...\n",
       "75     Grummet Island\n",
       "76            Grummet\n",
       "77            Malabar\n",
       "78              Dawes\n",
       "79             Sydney\n",
       "\n",
       "[80 rows x 1 columns]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placenames_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying States and Capitals <a class=\"anchor\" id=\"section-statescapitals\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some placenames, like *High Street* or *Maryborough*, may be very common across the world, or even in Australia. However, certain placenames refer to significant locations, like states, territories, large geographic features or capital cities. As such, if they are mentioned in a text, the placename is more likely to refer to the major location than a town or village in Tasmania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These significant locations are a finite set. They can be defined in a reference file that can be reused when reviewing the placenames of any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good point for you to start is a file about locations like modern capital cities and countries, combined with historical locations of significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on |  reference_location_data.csv\n"
     ]
    }
   ],
   "source": [
    "filename=\"reference_location_data.csv\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "reference_location = os.path.normpath(os.path.join(reference_directory, filename))\n",
    "reference_filename = os.path.basename(reference_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than reading this and then processing it, you can process each line as you read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the reference data in a dataframe\n",
    "locref_df = pd.read_csv(reference_location, encoding=\"utf-8\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationName</th>\n",
       "      <th>Category</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>PartOf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abuja</td>\n",
       "      <td>Capital</td>\n",
       "      <td>9.083333</td>\n",
       "      <td>7.533333</td>\n",
       "      <td>Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accra</td>\n",
       "      <td>Capital</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adamstown</td>\n",
       "      <td>Capital</td>\n",
       "      <td>-25.066667</td>\n",
       "      <td>-130.083333</td>\n",
       "      <td>Pitcairn Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addis Ababa</td>\n",
       "      <td>Capital</td>\n",
       "      <td>9.033333</td>\n",
       "      <td>38.700000</td>\n",
       "      <td>Ethiopia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aegina</td>\n",
       "      <td>Capital</td>\n",
       "      <td>37.740882</td>\n",
       "      <td>23.501421</td>\n",
       "      <td>Greece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Capital</td>\n",
       "      <td>45.800000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>Croatia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>Country</td>\n",
       "      <td>-15.416667</td>\n",
       "      <td>28.283333</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Zanzibar City</td>\n",
       "      <td>Capital</td>\n",
       "      <td>-6.165193</td>\n",
       "      <td>39.198914</td>\n",
       "      <td>Tanzania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Country</td>\n",
       "      <td>-17.816667</td>\n",
       "      <td>31.033333</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Zomba</td>\n",
       "      <td>Capital</td>\n",
       "      <td>-15.376586</td>\n",
       "      <td>35.335652</td>\n",
       "      <td>Malawi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LocationName Category  Longitude    Latitude            PartOf\n",
       "0            Abuja  Capital   9.083333    7.533333           Nigeria\n",
       "1            Accra  Capital   5.550000   -0.216667             Ghana\n",
       "2        Adamstown  Capital -25.066667 -130.083333  Pitcairn Islands\n",
       "3      Addis Ababa  Capital   9.033333   38.700000          Ethiopia\n",
       "4           Aegina  Capital  37.740882   23.501421            Greece\n",
       "..             ...      ...        ...         ...               ...\n",
       "543         Zagreb  Capital  45.800000   16.000000           Croatia\n",
       "544         Zambia  Country -15.416667   28.283333            Africa\n",
       "545  Zanzibar City  Capital  -6.165193   39.198914          Tanzania\n",
       "546       Zimbabwe  Country -17.816667   31.033333            Africa\n",
       "547          Zomba  Capital -15.376586   35.335652            Malawi\n",
       "\n",
       "[548 rows x 5 columns]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[TODO\\]: update this text chunk to suit the workshop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if you are researching historical texts, then some of these contemporary locations may have had different names. Old New York was once New Amsterdam (and had the [nickname of Gotham](https://www.nypl.org/blog/2011/01/25/so-why-do-we-call-it-gotham-anyway), amongst others). Istanbul was Constantinople. Some locations had [romanized names](https://en.wikipedia.org/wiki/Chinese_postal_romanization), like Beijing being called Peking. They may be a long time gone but you might want to add them to the list of significant known locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another historical variant is changing which cities are the capitals. These may be due to political decisions, like the movement of the Australian parliament from Melbourne to the new city of Canberra, or they could be a necessity due to the results of war, like Bonn becoming the capital of West Germany after World War II. These older capitals may also have to be accomodated in your reference data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because FtToHNL is set in the 19th Century CE, the next step is to add various capital cities from then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also larger geopolitical regions that may have been associated with placenames and cultures, for instance empires, dynasties and colonies like the British Empire or the Zulu Kingdom. Again, the borders and applicability of these political entities changed over time, so a contemporary reference list may not include them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 19th Century CE was a time of many European Empires so for FtToHNL, you will need to add reference data associated with relevant entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing this reference file, you can add the old political entity, its capital (if known), the geographic region (like continent or part thereof) and the modern country it would be considered part of.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to see if any of the placenames from our selected chapters of FtToHNL match these locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO DO] Describe this without being technical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we match a placename, copy the geolocation data for the matching location. Otherwise, keep it empty so we know to keep looking for the placename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still looking for  the sleepy sea\n",
      "Still looking for  the Bay of Biscay\n",
      "Still looking for  Heath\n",
      "*** Found London [Exact match]\n",
      "{'placename': 'London', 'locations': {'best_match':     LocationName Category  Longitude  Latitude          PartOf\n",
      "266       London  Capital       51.5 -0.083333  United Kingdom}}\n",
      "Still looking for  Van Diemen's Land\n",
      "Still looking for  Vickers\n",
      "Still looking for  Sylvia\n",
      "Still looking for  Bath\n",
      "Still looking for  Julia Vickers's\n",
      "Still looking for  Frere\n",
      "Still looking for  Chatham\n",
      "Still looking for  CHAPTER II\n",
      "Still looking for  Surgeon Pine\n",
      "Still looking for  Coromandel\n",
      "Still looking for  Pine\n",
      "*** Found India [Exact match]\n",
      "{'placename': 'India', 'locations': {'best_match':     LocationName Category  Longitude  Latitude PartOf\n",
      "206        India  Country       28.6      77.2   Asia}}\n",
      "Still looking for  the Hydaspes for Calcutta\n",
      "Still looking for  the poop guard\n",
      "Still looking for  MONOTONY\n",
      "Still looking for  Three'll\n",
      "Still looking for  Van Diemen's\n",
      "Still looking for  Tasman\n",
      "Still looking for  Cape Pillar\n",
      "Still looking for  Pirates' Bay\n",
      "Still looking for  east\n",
      "Still looking for  west\n",
      "Still looking for  the Isle of Wight\n",
      "Still looking for  the South-West Cape\n",
      "Still looking for  Swan Port\n",
      "Still looking for  Mediterranean\n",
      "Still looking for  Maria Island\n",
      "Still looking for  the Three Thumbs\n",
      "Still looking for  Peninsula\n",
      "Still looking for  Storm Bay\n",
      "Still looking for  Storing Island\n",
      "*** Found Italy [Exact match]\n",
      "{'placename': 'Italy', 'locations': {'best_match':     LocationName Category  Longitude   Latitude  PartOf\n",
      "215        Italy  Country       41.9  12.483333  Europe}}\n",
      "Still looking for  Sorrell\n",
      "Still looking for  Bruny Island\n",
      "Still looking for  Mount Royal\n",
      "Still looking for  D'Entrecasteaux Channel\n",
      "Still looking for  Actaeon\n",
      "Still looking for  the South Cape\n",
      "Still looking for  New Norfolk\n",
      "Still looking for  Derwent\n",
      "Still looking for  the Southern Ocean\n",
      "Still looking for  Tamar\n",
      "*** Found Victoria [Exact match]\n",
      "{'placename': 'Victoria', 'locations': {'best_match':     LocationName Category  Longitude  Latitude      PartOf\n",
      "523     Victoria  Capital  -4.616667     55.45  Seychelles}}\n",
      "Still looking for  Port Philip Bay\n",
      "*** Found Wellington [Exact match]\n",
      "{'placename': 'Wellington', 'locations': {'best_match':     LocationName Category  Longitude    Latitude       PartOf\n",
      "531   Wellington  Capital      -41.3  174.783333  New Zealand}}\n",
      "Still looking for  Dromedary\n",
      "Still looking for  Mount Wellington\n",
      "Still looking for  Launceston\n",
      "Still looking for  Smyrna\n",
      "Still looking for  Pyramid Island\n",
      "Still looking for  Rocky Point\n",
      "Still looking for  Port Davey\n",
      "Still looking for  Mount Direction\n",
      "Still looking for  Macquarie Harbour\n",
      "Still looking for  Mount Heemskirk\n",
      "Still looking for  Mount Zeehan\n",
      "Still looking for  King's River\n",
      "Still looking for  Sarah Island\n",
      "Still looking for  Philip's Island\n",
      "Still looking for  Hobart Town\n",
      "Still looking for  earth\n",
      "Still looking for  south-east\n",
      "Still looking for  Ladybird\n",
      "Still looking for  Commandant\n",
      "Still looking for  Port Arthur\n",
      "*** Found Honduras [Exact match]\n",
      "{'placename': 'Honduras', 'locations': {'best_match':     LocationName Category  Longitude   Latitude           PartOf\n",
      "200     Honduras  Country       14.1 -87.216667  Central America}}\n",
      "Still looking for  Arthur\n",
      "Still looking for  Hells Gates\n",
      "Still looking for  England\n",
      "Still looking for  New Town\n",
      "Still looking for  verandah.-She\n",
      "Still looking for  Grummet Island\n",
      "Still looking for  Grummet\n",
      "Still looking for  Malabar\n",
      "Still looking for  Dawes\n",
      "Still looking for  Sydney\n"
     ]
    }
   ],
   "source": [
    "\n",
    "geolocdata = [] # all the data about placenames and locations, once linked\n",
    "\n",
    "for placename in placenames_df['Placename']:\n",
    "    \n",
    "    # create a new geoloc entry about this placename\n",
    "    new_geolocdata={} \n",
    "    ## start a record for a placename\n",
    "    new_geolocdata['placename'] = placename\n",
    "    new_geolocdata['locations'] = {} # start with no location details\n",
    "    new_geolocdata['locations']['best_match'] = [] # start with no match\n",
    "    \n",
    "    # normalise its case and remove any leading whitespace\n",
    "    # This will be needed later for the gazzetteer\n",
    "    #safe_placename = urllib.parse.quote(placename.strip().lower()) \n",
    "\n",
    "    # Exact match\n",
    "    if(placename in list(locref_df['LocationName'])):\n",
    "        \n",
    "        print(\"*** Found\", placename,\"[Exact match]\")\n",
    "        # Copy the details from the reference file entry\n",
    "        new_geolocdata['locations']['best_match'] = locref_df[locref_df['LocationName']==placename]\n",
    "        \n",
    "    else:\n",
    "        print(\"Still looking for \", placename)\n",
    "    \n",
    "    # If you have a match, show it\n",
    "    if (len(new_geolocdata['locations']['best_match']) > 0):\n",
    "        print(new_geolocdata)\n",
    "        \n",
    "    geolocdata.append(new_geolocdata) # add the new placename data to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you have recorded the matches (and mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'placename': 'the sleepy sea', 'locations': {'best_match': []}},\n",
       " {'placename': 'the Bay of Biscay', 'locations': {'best_match': []}},\n",
       " {'placename': 'Heath', 'locations': {'best_match': []}},\n",
       " {'placename': 'London',\n",
       "  'locations': {'best_match':     LocationName Category  Longitude  Latitude          PartOf\n",
       "   266       London  Capital       51.5 -0.083333  United Kingdom}},\n",
       " {'placename': \"Van Diemen's Land\", 'locations': {'best_match': []}},\n",
       " {'placename': 'Vickers', 'locations': {'best_match': []}},\n",
       " {'placename': 'Sylvia', 'locations': {'best_match': []}},\n",
       " {'placename': 'Bath', 'locations': {'best_match': []}},\n",
       " {'placename': \"Julia Vickers's\", 'locations': {'best_match': []}},\n",
       " {'placename': 'Frere', 'locations': {'best_match': []}}]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geolocdata[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What locations did you end up finding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['London Capital 51.5 -0.083333 United Kingdom',\n",
       " 'India Country 28.6 77.2 Asia',\n",
       " 'Italy Country 41.9 12.483333 Europe',\n",
       " 'Victoria Capital -4.616667 55.45 Seychelles',\n",
       " 'Wellington Capital -41.3 174.783333 New Zealand',\n",
       " 'Honduras Country 14.1 -87.216667 Central America']"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchdata = [p['locations']['best_match'].to_string(index=False,header=False) for p in geolocdata \n",
    "             if len(p['locations']['best_match'])>0]\n",
    "matchdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching a Gazzetteer for Locations <a class=\"anchor\" id=\"section-searchgazzeteer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO BE DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
