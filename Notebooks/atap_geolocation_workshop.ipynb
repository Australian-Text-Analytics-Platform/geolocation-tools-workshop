{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATAP Notebook for the Geolocation project\n",
    "\n",
    "This notebook helps you access the Geolocation tools in a Python development environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "* [Premise](#section-premise)\n",
    "* [Requirements](#section-requirements) \n",
    "* [Data Preparation](#section-datapreparation)\n",
    "* [Named Entity Recognition](#section-ner)\n",
    " * [Look for NEs](#section-nes)\n",
    " * [Reviewing Candidate Placenames](#section-reviewplacenames)\n",
    "* [Finding Locations for Placenames](#section-findinglocs)\n",
    " * [Identifying States and Capitals](#section-statescapitals)\n",
    " * [Searching a Gazzetteer for Locations](#section-searchgazetteer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise <a class=\"anchor\" id=\"section-premise\"></a>\n",
    "*This section explains the Geolocation project and tools.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geolocation project relates to doctoral research done by [Fiannuala Morgan](https://finnoscarmorgan.github.io/) at the Australian National University. It uses software to identify placenames in archived historical texts, then compares them to data about known locations to identify where the placenames may be located. \n",
    "\n",
    "This notebook is designed to allow you to perform similar operations on textual documents.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "It will teach you how to\n",
    "<ul>\n",
    "    <li>use the spaCy library to identify and classify Named Entities (NEs)</li>\n",
    "    <li>identify multi-word expressions (MWE) that are NEs</li>\n",
    "    <li>search for spatial data about specific locations or places in gazetteers of such data</li>\n",
    "    <li>determine which locations are referred to by placenames, based on the context in which they are used in a text</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements <a class=\"anchor\" id=\"section-requirements\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "This notebook uses various Python libraries. Most will come with your Python installation, but the following are crucial:\n",
    "<ul>\n",
    "    <li> pandas</li> \n",
    "    <li> json</li> \n",
    "    <li> nltk</li> \n",
    "    <li> geopandas</li> \n",
    "    <li> shapely  </li> \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UPDATE\n",
    "# Many of these are probably not needed.\n",
    "\n",
    "import os\n",
    "from pickle import NONE\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geopandas is used to work with spatial data\n",
    "# If you have issues installing it on a MAcOS, \n",
    "# see https://stackoverflow.com/questions/71137617/error-installing-geopandas-in-python-on-mac-m1\n",
    "#import geopandas as gpd\n",
    "#from geopandas import GeoDataFrame\n",
    "\n",
    "# NLTK is used to work with textual data \n",
    "#from nltk.tag import StanfordNERTagger\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "# spaCy is used for a pipeline of NLP functions\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "# Shapely is used to work with geometric shapes\n",
    "#from shapely.geometry import Point\n",
    "\n",
    "# Fuzzywuzzy is used for fuzzy searches\n",
    "#from fuzzywuzzy import fuzz\n",
    "\n",
    "# used for the checklist\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation <a class=\"anchor\" id=\"section-datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also want to set up some directories for the import and output of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare the data directories\n",
    "## This presumes that Notebooks/ is the current working directory  \n",
    "text_directory = os.path.normpath(\"../Texts/\")\n",
    "csv_directory = os.path.normpath(\"../ner_output/\")\n",
    "reference_directory = os.path.normpath(\"../Data\")\n",
    "#maps_directory = os.path.normpath(\"../maps/\")\n",
    "\n",
    "## Create the data directories\n",
    "if not os.path.exists(text_directory):\n",
    "    os.makedirs(text_directory)\n",
    "if not os.path.exists(csv_directory):\n",
    "   os.makedirs(csv_directory)\n",
    "if not os.path.exists(reference_directory):\n",
    "   os.makedirs(reference_directory)\n",
    "\n",
    "#if not os.path.exists(maps_directory):\n",
    "#    os.makedirs(maps_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we will be examining the text of *For the Term of His Natural Life*, an 1874CE novel by Marcus Clarke that is in the public domain. Our copy was obtained via the [Gutenburg Project Australia](https://gutenberg.net.au/ebooks/e00016.txt). It is an unformatted textfile. We have slightly simplified it further by reducing it to only standard ASCII characters, replacing any accented characters with their unaccented forms and the British Pound Sterling symbol with the word pounds. \n",
    "\n",
    "The novel is divided into four books, each based in different regions of the world. You can start with the second book, which is titled *BOOK II.\\-\\-MACQUARIE HARBOUR.  1833*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename=\"FtToHNL_BOOK_1.txt\"\n",
    "filename=\"FtToHNL_BOOK_2.txt\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "text_filename = os.path.basename(textlocation)\n",
    "\n",
    "text = open(textlocation, encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no more than a long string of characters. So far, you have done no processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0:499] # look at the first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition <a class=\"anchor\" id=\"section-ner\"></a>\n",
    "*This section provides tools on identifying named entities in textual data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for NEs <a class=\"anchor\" id=\"section-nes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entities (NEs) are proper noun phrases within text, like names of places, people or organisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various packages that can include Named Enity Recognition (NER), e.g., the [Stanza CoreNLP](https://colab.research.google.com/github/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb), the Stanford NER, and the spaCy library. They often combine machine learning and a rule-based system to identify NEs and classify them into categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, you will be using the spaCy NER - https://spacy.io/usage/linguistic-features#morphology .  This is available as a Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy allows you to load a language model that has been trained on various examples of the language of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy will automatically run the model through various levels of natural language processing. This pipeline includes tokenising the text into individual tokens or terms, like words, values and puncuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "It is simple to use the client. You just tell it to annotate the text. However, Stanza allows you to specify what to annotate the text with. For instance, you might want it to tokenise the terms, label their parts-of-speech and lemmatised forms as well as recognising any NEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The options for the Stanza client include:<ul>\n",
    "    <li> <strong>tokenize - </strong> split into words or terms </li> \n",
    "      <li>    <strong>ssplit - </strong> split into sentences or independent statements</li> \n",
    "     <li>     <strong>pos -  </strong> syntactic parts-of-speech</li> \n",
    "      <li>    <strong>lemma -  </strong> lemmatised form (not always a root form)</li> \n",
    "      <li>    <strong>ner - </strong> named entity recognition</li> \n",
    "      <li>    <strong>depparse -  </strong> parsing of dependencies</li> \n",
    "      <li>    <strong>coref - </strong> co-reference resolution</li> \n",
    "      <li>    <strong>kbppandas - </strong> KBP competition format</li> \n",
    "</ul>\n",
    "A full explanation can be found at <a href=\"https://stanfordnlp.github.io/CoreNLP/pipeline.html\">https://stanfordnlp.github.io/CoreNLP/pipeline.html</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the default line contains the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sent to the spaCy model will be processed by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "sampletext = \"Autonomous cars shift insurance liability toward manufacturers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the pipeline is then  available in the output structure of the spaCy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\" - \", \"Morph: \",token.morph, \n",
    "          \"\\n   Dep: \",token.dep_, \n",
    "          \"\\n   Head: \",token.head.text, \n",
    "          \"\\n   Pos: \",token.head.pos_,\n",
    "          \"\\n   Child: \",[child for child in token.children])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you might not want some of this pipeline processing as it may not be beneficial to your analysis. Any unneeded processing will also slow the system down and place a greater demand on the memory. This is particularly true of the parser. Luckily, it is easy to stipulate what you want excluded from the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(sampletext, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\" - \", \"Morph: \",token.morph, \n",
    "          \"\\n   Dep: \",token.dep_, \n",
    "          \"\\n   Head: \",token.head.text, \n",
    "          \"\\n   Pos: \",token.head.pos_,\n",
    "          \"\\n   Child: \",[child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what you are interested in is the NER. Each sentence sent down the pipeline with the ner will get a list of entities that have been found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"Apple is looking at buying a U.K. startup based in London for $1 billion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"[\",ent.label_,\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each entity is labelled with a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: UPDATE\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    The NER categories classified by Stanza include:\n",
    "   <ul>\n",
    "<li><strong>Default:</strong>\n",
    "LOCATION, ORGANIZATION, PERSON</li>\n",
    "<li><strong>High recall: </strong>\n",
    "DATE, LOCATION, MONEY, ORGANIZATION, PERCENT, PERSON, TIME, MISC</li>\n",
    "<li><strong>KBP fine-grained:</strong>\n",
    "CAUSE_OF_DEATH, CITY, COUNTRY, CRIMINAL_CHARGE, EMAIL, HANDLE,\n",
    "IDEOLOGY, NATIONALITY, RELIGION, STATE_OR_PROVINCE, TITLE, URL</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "Most tokenised terms in the sentence have O as their NER value (that is the letter O not the number 0). Some however have been categorised. For instance, Van and Diemen are both classified as being PERSON named entities, Tasman is an ORGANIZATION and Cape and Pillar are in the LOCATION category. These categories are specific to Stanza. There are two key levels of processing available - the normal level will only identify the categories LOCATION, ORGANIZATION and PERSON, but the high recall processing will also consider other specialised phrases like TIME and MONEY which are not really named entities. Stanza can also look for the categories used in the KBP competition like CITY, COUNTRY and NATIONALITY, but this fine-grained processing will be slower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the entities includes the character position for the start and the end of the NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sampletext)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token will also have a value that indicates whether it is part of an NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"[\", token.ent_type_, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update\n",
    "\n",
    "The annotations so far show that *Cape* and *Pillar* are both a LOCATION NE, but not that *Cape Pillar* is actually the complete name of the location. However Stanza does also recognise multi-word expressions (MWEs), even though it recognises that they are part of an NE. Each *Sentence* annotated by Stanza has a list of [NE *Mentions*](https://stanfordnlp.github.io/CoreNLP/entitymentions.html) which are also given NER categories as their *Type*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to hand-code entities after the NER has been done. This can help make up for any common irregularities with the NER for your input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this model doesn't recognise that FB is a NE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"FB is hiring a new vice president of global policy\"\n",
    "\n",
    "doc = nlp(sampletext)\n",
    "ents = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "print('Entities before:', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to create a new entry for the list of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spaCy span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(ent.text, ent.start, ent.end, ent.label_) for ent in doc.ents]\n",
    "print('Entities after:', ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the data for the tokens is updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"[\", token.ent_type_, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy also allows the input documents to be processed in batches. This helps better manage the processing demands of the system throughout the pipeline when there are multiple files or many sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple texts in a list\n",
    "sampletexts = [\"Autonomous cars shift insurance liability toward manufacturers\",\"This is a text\", \"These are lots of texts\", \"...\"]\n",
    "\n",
    "# remove what elements you don't need from the pipeline\n",
    "for doc in nlp.pipe(sampletexts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    print(\"Entities: \",[(ent.text, ent.label_) for ent in doc.ents])\n",
    "    for token in doc:\n",
    "        print(\"   \",token.text, \"[\", token.ent_type_, \"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can process the output of the piped pipeline straight away, you can't print it unless you convert it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp.pipe(sampletexts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Can't remember what this is trying to do. Think it has to do with the IOB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#doc = nlp.make_doc(\"London is a big city in the United Kingdom and New York is in the United States of America.\")\n",
    "doc = nlp(\"London is a big city in the United Kingdom and New York is in the United States of America.\")\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(\"Entities: \",ents)\n",
    "\n",
    "print(\"\\nBefore:\", doc.ents)  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = np.zeros((len(doc), len(header)), dtype=\"uint64\")\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[\"GPE\"]\n",
    "doc.from_array(header, attr_array)\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(\"\\nEntities: \",ents)\n",
    "\n",
    "print(\"\\nAfter\", doc.ents)  # [London]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Placeholder in-case we want to show of the displacy rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sampletext)\n",
    "\n",
    "# displacy from spaCy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: talk about extracting just the NER types we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('LOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('FAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('ORG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Run through a single chapter (variable: text) before doing the entire collection?\n",
    "    This will allow all NEs to be shown, then the filter be introduced.\n",
    "    This will put it more on topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0:499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "    \n",
    "i=0 # entity counter\n",
    "# token level\n",
    "for e in doc.ents:\n",
    "    print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,e.text, e.label_))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Expand on this explanation with example context.\n",
    "Talk about the issue with _Van Diemen's_ versus _Van Diemen's Land_ (and _Tasman's Head_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These categories are assigned according to the context in which the NE is used. For this reason, _Van Diemen's_ is considered an *ORG*, a *PERSON* and a *FAC*, depending on its linguistic context. Note also that _VAN DIEMEN'S LAND_ in the title of the chapter isn't recognised as a NE, probably due to its unconventional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Expand on this explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, not all of these NE are suitable for placenames, so you will need to make a list of what categories regularly contain placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACENAME_CATEGORIES = [\"LOC\", \"GPE\", \"FAC\", \"ORG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing Candidate Placenames <a class=\"anchor\" id=\"section-reviewplacenames\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now put all of this together and find the placenames that are identified by spaCy in each chapter of the text. They can all be collected in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where we store the details about each instance of the placenames\n",
    "placenames_df = pd.DataFrame(columns=['Book','Chapter',\"NEIndex\",\"Placename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which chapters and books you want to annotate\n",
    "CHAPTERS=[1,2,3] \n",
    "BOOKS=[1,2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You should define what spaCy processing you do or don't want in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disabledPipeline=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process FtToHNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "i=0 # counter of the entities\n",
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        filename = \"FtToHNL_BOOK_\"+str(b)+\"_CHAPTER_\"+str(c)+\".txt\" \n",
    "        # set the specific path for the 'filename'\n",
    "        textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "        text_filename = os.path.basename(textlocation)\n",
    "\n",
    "        # read this chapter\n",
    "        text = open(textlocation, encoding=\"utf-8\").read()\n",
    "        print(\"Working on |\",filename)\n",
    "        \n",
    "        # run spaCy    \n",
    "        doc = nlp(text,disable=disabledPipeline)\n",
    "\n",
    "        # document level\n",
    "        ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "\n",
    "        # token level\n",
    "        for e in doc.ents:\n",
    "            if e.label_ in PLACENAME_CATEGORIES: # filter out MONEY, DATE etc\n",
    "                print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,e.text, e.label_))\n",
    "                # To help understand the context of the text, extract the occurence\n",
    "                context_text=doc.text[e.start_char-30:e.end_char+30].replace(\"\\n\",\" \")\n",
    "                \n",
    "                # Code to render with displacy\n",
    "                #context_doc={\"text\":str(i+1)+\" \\t \"+context_text,\n",
    "                #             \"ents\":[{\"start\": len(str(i+1)+\"   \")+30, \n",
    "                #                      \"end\":   len(str(i+1)+\"   \"+context_text)-30, \n",
    "                #                      \"label\": e.label_}],\n",
    "                #             \"title\": None}\n",
    "                #print(context_doc)\n",
    "                #displacy.render(context_doc, style=\"ent\", manual=True, jupyter=True)\n",
    "\n",
    "                # find the placenames according to spaCy\n",
    "                new_placename = {'Book':b,              # The Book number\n",
    "                                'Chapter':c,            # The Chapter number\n",
    "                                'NEIndex':i,            # A reference number to the nth Named Entity \n",
    "                                'Placename':e.text,     # The placename in the text\n",
    "                                'Category':e.label_,    # The spaCy category\n",
    "                                'Context':context_text, # The textual context where the placename was found\n",
    "                                'Approval':1}        # A flag for whether this is a suitable placename\n",
    "                placenames_df = placenames_df.append(new_placename, ignore_index=True)\n",
    "                \n",
    "            i=i+1 # entity counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df[['Book','Chapter','NEIndex','Placename','Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a lot more placenames were found in Book 2 than Book 1. This makes sense since Book 1 is set on board an ocean voyage, whereas Book 2 is at Macquarie Harbour in Australia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a number of NEs that are unlikely to be placenames, regardless of what spaCy categoriesd them as. It is best to consider the context in which the terms were used. Use the checkboxes to select which terms you do consider to be placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ipywidgets as widgets\n",
    "\n",
    "def changed(b):\n",
    "    # The system sets the _property_lock_, changes the value, then releases the _property_lock_.\n",
    "    # The confusing thing is that there is a value for the checkbox and a value for the property lock.\n",
    "    # changed() is called three times for every change to the checkbox.\n",
    "\n",
    "    # If you want to see any change to the checkbox value, uncomment this print()) command\n",
    "    if b['name']=='value':\n",
    "        #print(b,\"\\n\")\n",
    "        #print(\"found value\")\n",
    "        k=b['new']\n",
    "        #print(\"    \",k)\n",
    "    #print(b,\"\\n\")\n",
    "\n",
    "placename_items=[]\n",
    "context_items=[]\n",
    "num_items=[]\n",
    "\n",
    "# Time to try to make checkboxes for every placename for b in BOOKS:\n",
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        # Get the NEs from this book and chapter\n",
    "        pbc=placenames_df[(placenames_df[\"Book\"]==b) & (placenames_df[\"Chapter\"]==c)]\n",
    "        for i in pbc[\"NEIndex\"]:\n",
    "            context_text=pbc[pbc[\"NEIndex\"]==i][\"Context\"].values[0]\n",
    "            category=pbc[pbc[\"NEIndex\"]==i][\"Category\"].values[0]\n",
    "            \n",
    "        # Make lists of the candidate placenames, context text and index numbers \n",
    "        # Only the placenames are given a checkbox. \n",
    "        placename_items = placename_items + [widgets.Checkbox(True,description=i) for i in pbc[\"Placename\"]]\n",
    "        context_items = context_items + [widgets.Label(pbc[pbc[\"NEIndex\"]==i][\"Context\"].values[0]) for i in pbc[\"NEIndex\"]]\n",
    "        num_items = num_items + [widgets.Label(str(i)) for i in pbc[\"NEIndex\"]]\n",
    "\n",
    "# create a display\n",
    "num_placenames=len(placename_items)\n",
    "left_box = widgets.VBox(placename_items)\n",
    "right_box = widgets.VBox(context_items)\n",
    "num_box = widgets.VBox(num_items)\n",
    "whole_box = widgets.HBox([num_box, left_box, right_box])\n",
    "        \n",
    "display(whole_box)\n",
    "\n",
    "# respond to any changes in the checkboxes\n",
    "for i in range(num_placenames):\n",
    "    placename_items[i].observe(changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now copy all the values from the checkboxes to the data, so you know which placenames you have approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the status of each checklist item to the data\n",
    "for i in range(num_placenames):\n",
    "    #print(num_items[i].value,placename_items[i].value,placename_items[i].description)\n",
    "\n",
    "    NEIndex_num = int(num_items[i].value)\n",
    "    approval_flag = placename_items[i].value\n",
    "    \n",
    "    #print(\"Looking for [\"+str(NEIndex_num)+\"]\")\n",
    "    \n",
    "    # set the flag to match the checklist\n",
    "    for p in placenames_df[\"NEIndex\"]:\n",
    "        if (p-NEIndex_num == 0):\n",
    "            placenames_df.loc[placenames_df[\"NEIndex\"] == NEIndex_num,\"Approval\"] = approval_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now visualise the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df[['NEIndex','Placename','Approval']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you can extract the final list of distinct placenames that you have approved. While the names aren't sorted (though they could be), if you missed unselecting an NE on the checklist, this will help find it. All you need to do is go back to the checklist, unselect it, then run all other steps from there to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a unique list of the approved placenames\n",
    "approved_placenames = placenames_df[placenames_df[\"Approval\"]==True]['Placename'].unique()\n",
    "approved_placenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save this new data to a csv file. You have already defined the directory for your data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FtToHNL_placenames.csv\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving to placename data to \",save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list \n",
    "# using the savetxt from the numpy module\n",
    "np.savetxt(save_location, \n",
    "           approved_placenames,\n",
    "           delimiter =\", \", \n",
    "           fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Remove the MWE section as it is not needed for spaCy.\n",
    "Stopped the code form executing for now, but kept for reference while updating the above steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWEs as Named Entities  <a class=\"anchor\" id=\"section-mwes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotations so far show that *Cape* and *Pillar* are both a LOCATION NE, but not that *Cape Pillar* is actually the complete name of the location. However Stanza does also recognise multi-word expressions (MWEs), even though it recognises that they are part of an NE. Each *Sentence* annotated by Stanza has a list of [NE *Mentions*](https://stanfordnlp.github.io/CoreNLP/entitymentions.html) which are also given NER categories as their *Type*. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    print(\"{}\\t{:30s}\\t{}\".format(\"Sentence\", \"Mention\", \"Type\"))\n",
    "    for i, sent in enumerate(document.sentence):\n",
    "        for m in sent.mentions:\n",
    "            print(\"{:5}\\t\\t{:30s}\\t{}\".format(i+1,m.entityMentionText, m.entityType))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this isn't perfect. While *Van Diemen* is recognised as a *PERSON* NE, *Van Diemen's Land* (i.e., the former name for Tasmania) isn't recognised as a *LOCATION*. This is because Stanza is trained to only recognise certain combinations of words and categories as a new MWE category. These rules can however be added to but this workshop won't explore this aspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token is also annotated with an index number corresponding to any Mention it is part of. Each token can only be part of one Mention. While the Mentions may be annotated per Sentence, the index number is actually considering all Sentences and Mentions in the annotated text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TotalNumMentions=0 # keep a running list of how many NEs are identified\n",
    "print(\"{:15s}\\t{:15s}\\t{:6s}\\t{:10s}\\t{:10s}\\t{}\".format(\"word\",\"lemma\",\"POS\",\"NER\",\"Mention Index\",\"Mention\"))\n",
    "\n",
    "for i, sent in enumerate(document.sentence):\n",
    "    NumMentions = len(sent.mentions) # count how many NEs annotated to this sentence\n",
    "    if NumMentions:\n",
    "        print(\"\\n[Sentence {}]\".format(i+1))\n",
    "        for t in sent.token:\n",
    "            # only output tokens that are in NEs\n",
    "            if t.ner != 'O': # Letter, not zero!\n",
    "                # get the full text for the NE\n",
    "                mentionStr=sent.mentions[t.entityMentionIndex -  TotalNumMentions].entityMentionText                \n",
    "                print(\"{:15s}\\t{:15s}\\t{:6s}\\t{:10s}\\t{:10}\\t{}\".format(t.word, \n",
    "                                                                 t.lemma, \n",
    "                                                                 t.pos, \n",
    "                                                                 t.ner, \n",
    "                                                                 t.entityMentionIndex, \n",
    "                                                                 mentionStr))\n",
    "    TotalNumMentions += NumMentions # add to the running total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you are mainly interested in the Named Entities relate to locations. The location-based NER categories used by Stanza are:\n",
    "* *LOCATION*\n",
    "* *CITY*\n",
    "* *COUNTRY*\n",
    "* *STATE_OR_PROVINCE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NATIONALITY* might also be considered but it may depend on whether you care about phrases like *student of English history* or *Frenchman's cap*, or not.\n",
    "It is easy to filter out all other NEs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NATIONALITY is ignored \n",
    "LOCATION_CATEGORIES = ['LOCATION','CITY','COUNTRY','STATE_OR_PROVINCE']\n",
    "\n",
    "# Iterate over all detected entity mentions\n",
    "print(\"{:30s}\\t{}\".format(\"Mention\", \"Type\"))\n",
    "\n",
    "STANZA_Locations = [] # list of locations Stanza recognises\n",
    "for sent in document.sentence:\n",
    "    for m in sent.mentions:\n",
    "        if (m.entityType in LOCATION_CATEGORIES): # ignore any nonLocation NEs\n",
    "            print(\"{:30s}\\t{}\".format(m.entityMentionText, m.entityType))\n",
    "            STANZA_Locations.append(m.entityMentionText)\n",
    "            \n",
    "print(\"\\n{}\".format(STANZA_Locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now put all of this together and find the placenames that are identified by Stanza in each chapter of the text. They can all be collected in a single dataframe."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "placenames = pd.DataFrame(columns=['Book','Chapter',\"NEIndex\",\"Placename\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# define which chapters and books you want to annotate\n",
    "CHAPTERS=[1,2,3] \n",
    "BOOKS=[1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\[MN Note\\]__ Change this to a background server, rather than a server on demand?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for b in BOOKS:\n",
    "    for c in CHAPTERS:\n",
    "        filename = \"FtToHNL_BOOK_\"+str(b)+\"_CHAPTER_\"+str(c)+\".txt\" \n",
    "        # set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "        textlocation = os.path.normpath(os.path.join(text_directory, filename))\n",
    "        text_filename = os.path.basename(textlocation)\n",
    "\n",
    "        # read this chapter\n",
    "        text = open(textlocation, encoding=\"utf-8\").read()\n",
    "        print(\"Working on |\",filename)\n",
    "        \n",
    "        # run the Stanza server & client\n",
    "        # https://nlp.stanford.edu/software/regexner.html\n",
    "        # java -mx1g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP \n",
    "        #      -annotators 'tokenize,ssplit,pos,lemma,ner' -file JuliaGillard.txt\n",
    "        # java -mx1g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP \n",
    "        #      -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -file JuliaGillard.txt\n",
    "        #      -regexner.mapping jg-regexner.txt\n",
    "        #\n",
    "        # with CoreNLPClient(properties={\n",
    "        #  'annotators': 'tokenize,ssplit,pos',\n",
    "        #  'pos.model': '/path/to/custom-model.ser.gz'\n",
    "        # }) as client:\n",
    "        #with CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','regexner'], \n",
    "        #           memory='4G', \n",
    "        #           annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','regexner'], \n",
    "        #           endpoint='http://localhost:9001', \n",
    "        #           be_quiet=True) as client:\n",
    "        with CoreNLPClient(properties={\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,regexner', \n",
    "            'regexner.mapping':'jg-regexner.txt',\n",
    "            'memory':'4G', \n",
    "            'endpoint':'http://localhost:9001', \n",
    "            'be_quiet':True\n",
    "        }) as client:\n",
    "            # Annotate the text (presuming it is not too large)\n",
    "            document = client.annotate(text)\n",
    "        \n",
    "        # find the placenames according to Stanza\n",
    "        for sent in document.sentence:\n",
    "            for m in sent.mentions:\n",
    "                if (m.entityType in LOCATION_CATEGORIES):\n",
    "                    new_placename = {'Book':b,\n",
    "                                    'Chapter':c,\n",
    "                                    'NEIndex':m.entityMentionIndex,\n",
    "                                    'Placename':m.entityMentionText,\n",
    "                                    'Category':m.entityType}\n",
    "                    placenames = placenames.append(new_placename, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a lot more placenames were found in Book 2 than Book 1. This makes sense since Book 1 is set on board an ocean voyage, whereas Book 2 is at Macquarie Harbour in Australia.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to save this new data to a csv file. You have already defined the directory for your data files."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filename = \"FtToHNL_placenames_v2.txt\"\n",
    "save_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "save_filename = os.path.basename(save_location)\n",
    "print(\"Saving placename data to \",save_location)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "placenames.to_csv(save_location)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Locations for the Placenames <a class=\"anchor\" id=\"section-findinglocs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a list of placenames from the text, the next step is to work out their location on Earth. For this you can use a combination of specialised lists of locations, gazzetteers and heuristics. The objective is to match every placename with the coordinates of a known location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to read the file of your placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on |  FtToHNL_placenames.csv\n"
     ]
    }
   ],
   "source": [
    "filename=\"FtToHNL_placenames.csv\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "data_location = os.path.normpath(os.path.join(csv_directory, filename))\n",
    "data_filename = os.path.basename(data_location)\n",
    "\n",
    "# Using pandas, read the csv file. This will place it in a dataframe format. \n",
    "placenames_df = pd.read_csv(data_location, encoding=\"utf-8\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "placenames_df = placenames_df.rename(columns={placenames_df.columns[0]: 'Placename'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Placename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the sleepy sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the Bay of Biscay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Van Diemen's Land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Grummet Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Grummet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Malabar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Dawes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Placename\n",
       "0      the sleepy sea\n",
       "1   the Bay of Biscay\n",
       "2               Heath\n",
       "3              London\n",
       "4   Van Diemen's Land\n",
       "..                ...\n",
       "75     Grummet Island\n",
       "76            Grummet\n",
       "77            Malabar\n",
       "78              Dawes\n",
       "79             Sydney\n",
       "\n",
       "[80 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placenames_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying States and Capitals <a class=\"anchor\" id=\"section-statescapitals\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some placenames, like *High Street* or *Maryborough*, may be very common across the world, or even in Australia. However, certain placenames refer to significant locations, like states, territories, large geographic features or capital cities. As such, if they are mentioned in a text, the placename is more likely to refer to the major location than a town or village in Tasmania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These significant locations are a finite set. They can be defined in a reference file that can be reused when reviewing the placenames of any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good point for you to start is a file about locations like modern capital cities and countries, combined with historical locations of significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on |  reference_location_data.csv\n"
     ]
    }
   ],
   "source": [
    "filename=\"reference_location_data.csv\"\n",
    "print(\"Working on | \", filename)\n",
    "\n",
    "# set the specific path for the 'filename' which is basically working through a list of everything that is in the folder\n",
    "reference_location = os.path.normpath(os.path.join(reference_directory, filename))\n",
    "reference_filename = os.path.basename(reference_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than reading this and then processing it, you can process each line as you read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the reference data in a dataframe\n",
    "locref_df = pd.read_csv(reference_location, encoding=\"utf-8\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationName</th>\n",
       "      <th>Category</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>PartOf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abuja</td>\n",
       "      <td>Capital</td>\n",
       "      <td>9.083333</td>\n",
       "      <td>7.533333</td>\n",
       "      <td>Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accra</td>\n",
       "      <td>Capital</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adamstown</td>\n",
       "      <td>Capital</td>\n",
       "      <td>-25.066667</td>\n",
       "      <td>-130.083333</td>\n",
       "      <td>Pitcairn Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addis Ababa</td>\n",
       "      <td>Capital</td>\n",
       "      <td>9.033333</td>\n",
       "      <td>38.700000</td>\n",
       "      <td>Ethiopia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aegina</td>\n",
       "      <td>Capital</td>\n",
       "      <td>37.740882</td>\n",
       "      <td>23.501421</td>\n",
       "      <td>Greece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Capital</td>\n",
       "      <td>45.800000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>Croatia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>Country</td>\n",
       "      <td>-15.416667</td>\n",
       "      <td>28.283333</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Zanzibar City</td>\n",
       "      <td>Capital</td>\n",
       "      <td>-6.165193</td>\n",
       "      <td>39.198914</td>\n",
       "      <td>Tanzania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Country</td>\n",
       "      <td>-17.816667</td>\n",
       "      <td>31.033333</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Zomba</td>\n",
       "      <td>Capital</td>\n",
       "      <td>-15.376586</td>\n",
       "      <td>35.335652</td>\n",
       "      <td>Malawi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LocationName Category  Longitude    Latitude            PartOf\n",
       "0            Abuja  Capital   9.083333    7.533333           Nigeria\n",
       "1            Accra  Capital   5.550000   -0.216667             Ghana\n",
       "2        Adamstown  Capital -25.066667 -130.083333  Pitcairn Islands\n",
       "3      Addis Ababa  Capital   9.033333   38.700000          Ethiopia\n",
       "4           Aegina  Capital  37.740882   23.501421            Greece\n",
       "..             ...      ...        ...         ...               ...\n",
       "543         Zagreb  Capital  45.800000   16.000000           Croatia\n",
       "544         Zambia  Country -15.416667   28.283333            Africa\n",
       "545  Zanzibar City  Capital  -6.165193   39.198914          Tanzania\n",
       "546       Zimbabwe  Country -17.816667   31.033333            Africa\n",
       "547          Zomba  Capital -15.376586   35.335652            Malawi\n",
       "\n",
       "[548 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[TODO\\]: update this text chunk to suit the workshop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if you are researching historical texts, then some of these contemporary locations may have had different names. Old New York was once New Amsterdam (and had the [nickname of Gotham](https://www.nypl.org/blog/2011/01/25/so-why-do-we-call-it-gotham-anyway), amongst others). Istanbul was Constantinople. Some locations had [romanized names](https://en.wikipedia.org/wiki/Chinese_postal_romanization), like Beijing being called Peking. They may be a long time gone but you might want to add them to the list of significant known locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another historical variant is changing which cities are the capitals. These may be due to political decisions, like the movement of the Australian parliament from Melbourne to the new city of Canberra, or they could be a necessity due to the results of war, like Bonn becoming the capital of West Germany after World War II. These older capitals may also have to be accomodated in your reference data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because FtToHNL is set in the 19th Century CE, the next step is to add various capital cities from then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also larger geopolitical regions that may have been associated with placenames and cultures, for instance empires, dynasties and colonies like the British Empire or the Zulu Kingdom. Again, the borders and applicability of these political entities changed over time, so a contemporary reference list may not include them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 19th Century CE was a time of many European Empires so for FtToHNL, you will need to add reference data associated with relevant entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing this reference file, you can add the old political entity, its capital (if known), the geographic region (like continent or part thereof) and the modern country it would be considered part of.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to see if any of the placenames from our selected chapters of FtToHNL match these locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO DO] Describe this without being technical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we match a placename, copy the geolocation data for the matching location. Otherwise, keep it empty so we know to keep looking for the placename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still looking for  the sleepy sea\n",
      "Still looking for  the Bay of Biscay\n",
      "Still looking for  Heath\n",
      "*** Found London [Exact match]\n",
      "{'placename': 'London', 'locations': {'best_match':     LocationName Category  Longitude  Latitude          PartOf\n",
      "266       London  Capital       51.5 -0.083333  United Kingdom}}\n",
      "Still looking for  Van Diemen's Land\n",
      "Still looking for  Vickers\n",
      "Still looking for  Sylvia\n",
      "Still looking for  Bath\n",
      "Still looking for  Julia Vickers's\n",
      "Still looking for  Frere\n",
      "Still looking for  Chatham\n",
      "Still looking for  CHAPTER II\n",
      "Still looking for  Surgeon Pine\n",
      "Still looking for  Coromandel\n",
      "Still looking for  Pine\n",
      "*** Found India [Exact match]\n",
      "{'placename': 'India', 'locations': {'best_match':     LocationName Category  Longitude  Latitude PartOf\n",
      "206        India  Country       28.6      77.2   Asia}}\n",
      "Still looking for  the Hydaspes for Calcutta\n",
      "Still looking for  the poop guard\n",
      "Still looking for  MONOTONY\n",
      "Still looking for  Three'll\n",
      "Still looking for  Van Diemen's\n",
      "Still looking for  Tasman\n",
      "Still looking for  Cape Pillar\n",
      "Still looking for  Pirates' Bay\n",
      "Still looking for  east\n",
      "Still looking for  west\n",
      "Still looking for  the Isle of Wight\n",
      "Still looking for  the South-West Cape\n",
      "Still looking for  Swan Port\n",
      "Still looking for  Mediterranean\n",
      "Still looking for  Maria Island\n",
      "Still looking for  the Three Thumbs\n",
      "Still looking for  Peninsula\n",
      "Still looking for  Storm Bay\n",
      "Still looking for  Storing Island\n",
      "*** Found Italy [Exact match]\n",
      "{'placename': 'Italy', 'locations': {'best_match':     LocationName Category  Longitude   Latitude  PartOf\n",
      "215        Italy  Country       41.9  12.483333  Europe}}\n",
      "Still looking for  Sorrell\n",
      "Still looking for  Bruny Island\n",
      "Still looking for  Mount Royal\n",
      "Still looking for  D'Entrecasteaux Channel\n",
      "Still looking for  Actaeon\n",
      "Still looking for  the South Cape\n",
      "Still looking for  New Norfolk\n",
      "Still looking for  Derwent\n",
      "Still looking for  the Southern Ocean\n",
      "Still looking for  Tamar\n",
      "*** Found Victoria [Exact match]\n",
      "{'placename': 'Victoria', 'locations': {'best_match':     LocationName Category  Longitude  Latitude      PartOf\n",
      "523     Victoria  Capital  -4.616667     55.45  Seychelles}}\n",
      "Still looking for  Port Philip Bay\n",
      "*** Found Wellington [Exact match]\n",
      "{'placename': 'Wellington', 'locations': {'best_match':     LocationName Category  Longitude    Latitude       PartOf\n",
      "531   Wellington  Capital      -41.3  174.783333  New Zealand}}\n",
      "Still looking for  Dromedary\n",
      "Still looking for  Mount Wellington\n",
      "Still looking for  Launceston\n",
      "Still looking for  Smyrna\n",
      "Still looking for  Pyramid Island\n",
      "Still looking for  Rocky Point\n",
      "Still looking for  Port Davey\n",
      "Still looking for  Mount Direction\n",
      "Still looking for  Macquarie Harbour\n",
      "Still looking for  Mount Heemskirk\n",
      "Still looking for  Mount Zeehan\n",
      "Still looking for  King's River\n",
      "Still looking for  Sarah Island\n",
      "Still looking for  Philip's Island\n",
      "Still looking for  Hobart Town\n",
      "Still looking for  earth\n",
      "Still looking for  south-east\n",
      "Still looking for  Ladybird\n",
      "Still looking for  Commandant\n",
      "Still looking for  Port Arthur\n",
      "*** Found Honduras [Exact match]\n",
      "{'placename': 'Honduras', 'locations': {'best_match':     LocationName Category  Longitude   Latitude           PartOf\n",
      "200     Honduras  Country       14.1 -87.216667  Central America}}\n",
      "Still looking for  Arthur\n",
      "Still looking for  Hells Gates\n",
      "Still looking for  England\n",
      "Still looking for  New Town\n",
      "Still looking for  verandah.-She\n",
      "Still looking for  Grummet Island\n",
      "Still looking for  Grummet\n",
      "Still looking for  Malabar\n",
      "Still looking for  Dawes\n",
      "Still looking for  Sydney\n"
     ]
    }
   ],
   "source": [
    "\n",
    "geolocdata = [] # all the data about placenames and locations, once linked\n",
    "\n",
    "for placename in placenames_df['Placename']:\n",
    "    \n",
    "    # create a new geoloc entry about this placename\n",
    "    new_geolocdata={} \n",
    "    ## start a record for a placename\n",
    "    new_geolocdata['placename'] = placename\n",
    "    new_geolocdata['locations'] = {} # start with no location details\n",
    "    new_geolocdata['locations']['best_match'] = [] # start with no match\n",
    "    \n",
    "    # normalise its case and remove any leading whitespace\n",
    "    # This will be needed later for the gazzetteer\n",
    "    #safe_placename = urllib.parse.quote(placename.strip().lower()) \n",
    "\n",
    "    # Exact match\n",
    "    if(placename in list(locref_df['LocationName'])):\n",
    "        \n",
    "        print(\"*** Found\", placename,\"[Exact match]\")\n",
    "        # Copy the details from the reference file entry\n",
    "        new_geolocdata['locations']['best_match'] = locref_df[locref_df['LocationName']==placename]\n",
    "        \n",
    "    else:\n",
    "        print(\"Still looking for \", placename)\n",
    "    \n",
    "    # If you have a match, show it\n",
    "    if (len(new_geolocdata['locations']['best_match']) > 0):\n",
    "        print(new_geolocdata)\n",
    "        \n",
    "    geolocdata.append(new_geolocdata) # add the new placename data to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you have recorded the matches (and mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'placename': 'the sleepy sea', 'locations': {'best_match': []}},\n",
       " {'placename': 'the Bay of Biscay', 'locations': {'best_match': []}},\n",
       " {'placename': 'Heath', 'locations': {'best_match': []}},\n",
       " {'placename': 'London',\n",
       "  'locations': {'best_match':     LocationName Category  Longitude  Latitude          PartOf\n",
       "   266       London  Capital       51.5 -0.083333  United Kingdom}},\n",
       " {'placename': \"Van Diemen's Land\", 'locations': {'best_match': []}},\n",
       " {'placename': 'Vickers', 'locations': {'best_match': []}},\n",
       " {'placename': 'Sylvia', 'locations': {'best_match': []}},\n",
       " {'placename': 'Bath', 'locations': {'best_match': []}},\n",
       " {'placename': \"Julia Vickers's\", 'locations': {'best_match': []}},\n",
       " {'placename': 'Frere', 'locations': {'best_match': []}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geolocdata[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What locations did you end up finding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['London Capital 51.5 -0.083333 United Kingdom',\n",
       " 'India Country 28.6 77.2 Asia',\n",
       " 'Italy Country 41.9 12.483333 Europe',\n",
       " 'Victoria Capital -4.616667 55.45 Seychelles',\n",
       " 'Wellington Capital -41.3 174.783333 New Zealand',\n",
       " 'Honduras Country 14.1 -87.216667 Central America']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchdata = [p['locations']['best_match'].to_string(index=False,header=False) for p in geolocdata \n",
    "             if len(p['locations']['best_match'])>0]\n",
    "matchdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now forget about the dataframe with the complete set of reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del locref_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching a Gazetteer for Locations <a class=\"anchor\" id=\"section-searchgazzeteer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search [Open Street Map (ODM)](https://nominatim.org/release-docs/develop/api/Search/) for locations that match the unknown placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install ratelimit\n",
    "import requests\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "from pprint import pprint\n",
    "from ratelimit import limits, RateLimitException, sleep_and_retry\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many (max) results do we want for each name?\n",
    "#[TO DO] Make this a user setting, defaulting to 5\n",
    "# The normal is (Default: 10, Maximum: 50), according to https://nominatim.org/release-docs/develop/api/Search/\n",
    "limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send rate-limited requests that stay within n requests per second\n",
    "# [TO DO] add link to webpage about this\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=1)\n",
    "def osm_call_api(url):\n",
    "    response = requests.get(url)\n",
    "    return response\n",
    "\n",
    "# Format the api response to make comparison easier\n",
    "def osm_format_response(input):\n",
    "\n",
    "    # extract the country name, if any\n",
    "    hyperlocation = None;\n",
    "    if input[\"display_name\"].find(','):\n",
    "        # break up the address\n",
    "        namesplit = input[\"display_name\"].split(',')\n",
    "        # extract the rightmost term from the split\n",
    "        hyperlocation = namesplit[len(namesplit)-1]\n",
    "        #hyperlocation=len(namesplit)\n",
    "        \n",
    "        \n",
    "    # for now, keep the names the similiar to what OSM calls them\n",
    "    response = {\"Name\": input[\"display_name\"], \n",
    "              \"Type\": input[\"type\"],\n",
    "              \"Latitude\": input[\"lat\"], \n",
    "              \"Longitude\": input[\"lon\"],\n",
    "              \"PartOf\": hyperlocation,\n",
    "              \"Gazetteer\": \"OSM\",\n",
    "              \"Importance\": input[\"importance\"]\n",
    "                }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO BE DONE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now move to the data that is needed for the geolocation project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for the sleepy sea\n",
      "looking for the Bay of Biscay\n",
      "looking for Heath\n",
      "looking for Van Diemen's Land\n",
      "looking for Vickers\n",
      "looking for Sylvia\n",
      "looking for Bath\n",
      "looking for Julia Vickers's\n",
      "looking for Frere\n",
      "looking for Chatham\n",
      "looking for CHAPTER II\n",
      "looking for Surgeon Pine\n",
      "looking for Coromandel\n",
      "looking for Pine\n",
      "looking for the Hydaspes for Calcutta\n",
      "looking for the poop guard\n",
      "looking for MONOTONY\n",
      "looking for Three'll\n",
      "looking for Van Diemen's\n",
      "looking for Tasman\n",
      "looking for Cape Pillar\n",
      "looking for Pirates' Bay\n",
      "looking for east\n",
      "looking for west\n",
      "looking for the Isle of Wight\n",
      "looking for the South-West Cape\n",
      "looking for Swan Port\n",
      "looking for Mediterranean\n",
      "looking for Maria Island\n",
      "looking for the Three Thumbs\n",
      "looking for Peninsula\n",
      "looking for Storm Bay\n",
      "looking for Storing Island\n",
      "looking for Sorrell\n",
      "looking for Bruny Island\n",
      "looking for Mount Royal\n",
      "looking for D'Entrecasteaux Channel\n",
      "looking for Actaeon\n",
      "looking for the South Cape\n",
      "looking for New Norfolk\n",
      "looking for Derwent\n",
      "looking for the Southern Ocean\n",
      "looking for Tamar\n",
      "looking for Port Philip Bay\n",
      "looking for Dromedary\n",
      "looking for Mount Wellington\n",
      "looking for Launceston\n",
      "looking for Smyrna\n",
      "looking for Pyramid Island\n",
      "looking for Rocky Point\n",
      "looking for Port Davey\n",
      "looking for Mount Direction\n",
      "looking for Macquarie Harbour\n",
      "looking for Mount Heemskirk\n",
      "looking for Mount Zeehan\n",
      "looking for King's River\n",
      "looking for Sarah Island\n",
      "looking for Philip's Island\n",
      "looking for Hobart Town\n",
      "looking for earth\n",
      "looking for south-east\n",
      "looking for Ladybird\n",
      "looking for Commandant\n",
      "looking for Port Arthur\n",
      "looking for Arthur\n",
      "looking for Hells Gates\n",
      "looking for England\n",
      "looking for New Town\n",
      "looking for verandah.-She\n",
      "looking for Grummet Island\n",
      "looking for Grummet\n",
      "looking for Malabar\n",
      "looking for Dawes\n",
      "looking for Sydney\n"
     ]
    }
   ],
   "source": [
    "# For every placename in our list\n",
    "for p in geolocdata:\n",
    "    # Already found a location, so skip to the next placename\n",
    "    if len(p['locations']['best_match']) > 0:\n",
    "        continue\n",
    "        \n",
    "    placename = p['placename']\n",
    "    print (\"looking for\",placename)\n",
    "\n",
    "    # query the OSM database\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?q={placename}&format=json&limit={limit}\"\n",
    "    response = osm_call_api(url)\n",
    "    response_dict = json.loads(response.text)\n",
    "    #print(response.text)\n",
    "\n",
    "    p['locations']['candidates']=None\n",
    "    \n",
    "    # Handle no results found\n",
    "    if len(response_dict) is 0:\n",
    "        # skip to the next placename\n",
    "        continue\n",
    "        \n",
    "    # Save the possible locations for later processing\n",
    "    data_frames = []\n",
    "\n",
    "    # Handle results found\n",
    "    for response_record in response_dict:\n",
    "        #  Use this to look at a reduced set of data from the results\n",
    "        #print(response_record)\n",
    "        cleaned_response = osm_format_response(response_record)\n",
    "        #print(\"   ...... \",cleaned_data)\n",
    "\n",
    "        # Add the data to a dataframe\n",
    "        df = pd.DataFrame(columns = [#'name' , \n",
    "                                     'Location',\n",
    "                                     \"Category\",\n",
    "                                     \"Latitude\",\n",
    "                                     \"Longitude\",\n",
    "                                     \"PartOf\",\n",
    "                                     \"Gazetteer\",\n",
    "                                     \"Certainity\"])\n",
    "        df = df.append({#\"name\": placename, \n",
    "                        \"Location\": cleaned_response[\"Name\"],\n",
    "                        \"Category\": cleaned_response[\"Type\"],\n",
    "                        'Latitude': cleaned_response[\"Latitude\"],\n",
    "                        'Longitude': cleaned_response[\"Longitude\"],\n",
    "                        'PartOf': cleaned_response[\"PartOf\"],\n",
    "                        'Gazetteer': cleaned_response[\"Gazetteer\"],\n",
    "                        'Certainity': cleaned_response[\"Importance\"]}, \n",
    "                       ignore_index=True)\n",
    "        data_frames.append(df)\n",
    "        \n",
    "        # print the output\n",
    "        matchdata = df.to_string(index=False,header=False)\n",
    "        #print(\"  *  \",matchdata)\n",
    "\n",
    "\n",
    "        #print(df)\n",
    "\n",
    "    # add the results to the geoloc dataframe\n",
    "    # review the outcomes later\n",
    "    p['locations']['candidates'] = data_frames\n",
    "\n",
    "    #print(p['locations']['candidates'])\n",
    "        \n",
    "    #exit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What placenames have you still not found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the sleepy sea',\n",
       " \"Julia Vickers's\",\n",
       " 'the Hydaspes for Calcutta',\n",
       " 'the poop guard',\n",
       " 'MONOTONY',\n",
       " 'the Three Thumbs',\n",
       " 'Storing Island',\n",
       " 'Commandant',\n",
       " 'verandah.-She',\n",
       " 'Grummet Island']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatcheddata = [p['placename'] for p in geolocdata \n",
    "             if len(p['locations']['best_match'])==0 and p['locations']['candidates']==None]\n",
    "unmatcheddata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where have you found locations for placenames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the Bay of Biscay',\n",
       " 'Heath',\n",
       " 'London',\n",
       " \"Van Diemen's Land\",\n",
       " 'Vickers',\n",
       " 'Sylvia',\n",
       " 'Bath',\n",
       " 'Frere',\n",
       " 'Chatham',\n",
       " 'CHAPTER II',\n",
       " 'Surgeon Pine',\n",
       " 'Coromandel',\n",
       " 'Pine',\n",
       " 'India',\n",
       " \"Three'll\",\n",
       " \"Van Diemen's\",\n",
       " 'Tasman',\n",
       " 'Cape Pillar',\n",
       " \"Pirates' Bay\",\n",
       " 'east',\n",
       " 'west',\n",
       " 'the Isle of Wight',\n",
       " 'the South-West Cape',\n",
       " 'Swan Port',\n",
       " 'Mediterranean',\n",
       " 'Maria Island',\n",
       " 'Peninsula',\n",
       " 'Storm Bay',\n",
       " 'Italy',\n",
       " 'Sorrell',\n",
       " 'Bruny Island',\n",
       " 'Mount Royal',\n",
       " \"D'Entrecasteaux Channel\",\n",
       " 'Actaeon',\n",
       " 'the South Cape',\n",
       " 'New Norfolk',\n",
       " 'Derwent',\n",
       " 'the Southern Ocean',\n",
       " 'Tamar',\n",
       " 'Victoria',\n",
       " 'Port Philip Bay',\n",
       " 'Wellington',\n",
       " 'Dromedary',\n",
       " 'Mount Wellington',\n",
       " 'Launceston',\n",
       " 'Smyrna',\n",
       " 'Pyramid Island',\n",
       " 'Rocky Point',\n",
       " 'Port Davey',\n",
       " 'Mount Direction',\n",
       " 'Macquarie Harbour',\n",
       " 'Mount Heemskirk',\n",
       " 'Mount Zeehan',\n",
       " \"King's River\",\n",
       " 'Sarah Island',\n",
       " \"Philip's Island\",\n",
       " 'Hobart Town',\n",
       " 'earth',\n",
       " 'south-east',\n",
       " 'Ladybird',\n",
       " 'Port Arthur',\n",
       " 'Honduras',\n",
       " 'Arthur',\n",
       " 'Hells Gates',\n",
       " 'England',\n",
       " 'New Town',\n",
       " 'Grummet',\n",
       " 'Malabar',\n",
       " 'Dawes',\n",
       " 'Sydney']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchdata = [p['placename'] for p in geolocdata \n",
    "             if len(p['locations']['best_match'])>0 or p['locations']['candidates']!=None]\n",
    "matchdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Open Street Map is a wonderful resource, it focusses on current names of geographic locations. If the original source of your placenames was not written in the recent decades, then the OSM may not know the appropriate names of locations for the time of the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to also look up a historical gazetteer, like the TLC. \n",
    "\n",
    "Like the OSM API, the TLCMap API has various options, like which type of search to use and whether to search any data in the database enetred by the public, rather than that which has been verified or entered by experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TO DO] Describe the TLCMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, you will look for exact matches between the placenames and the locations, and not consider any publicly entered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which order to do different searches for known locations\n",
    "search_type = 'exact' # alt values: 'exact', 'fuzzy', 'contains' \n",
    "\n",
    "# Flag whether to use data provided by the public\n",
    "search_public_data = False # alt values = True, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for the OSM, you will need a few functions to query the the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlc_build_url(placename: str, search_type: str, search_public_data: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Build a url to query the tlcmap/ghap API.\n",
    "    placename: the place we're trying to locate\n",
    "    search_type: what search type to use (accepts one of ['contains','fuzzy','exact'])\n",
    "    \n",
    "    ref: https://www.tlcmap.org/guides/ghap/#ws\n",
    "    \"\"\"\n",
    "    safe_placename = urllib.parse.quote(placename.strip().lower())\n",
    "\n",
    "    url = f\"https://tlcmap.org/ghap/search?\"\n",
    "\n",
    "    if search_type == 'fuzzy':\n",
    "        url += f\"fuzzyname={safe_placename}\"\n",
    "    elif search_type == 'exact':\n",
    "        url += f\"name={safe_placename}\"\n",
    "    elif search_type == 'contains':\n",
    "        url += f\"containsname={safe_placename}\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Search Australian National Placenames Survey provided data\n",
    "    url += \"&searchausgaz=on\"\n",
    "    \n",
    "    # Search public provided data, this data could be unreliable\n",
    "    if search_public_data == True:\n",
    "        url += \"&searchpublicdatasets=on\"\n",
    "    \n",
    "    # Retrieve data as JSON\n",
    "    url += \"&format=json\"\n",
    "    \n",
    "    # limit the number of results\n",
    "    url += \"&paging=1\"\n",
    "\n",
    "    return url\n",
    "\n",
    "# Send rate-limited requests that stay within n requests per second\n",
    "# [TO DO] add link to webpage about this\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=1)\n",
    "def tlc_call_api(url):\n",
    "    r = requests.get(url)\n",
    "    if r.url == 'https://tlcmap.org/ghap/maxpaging':\n",
    "        return None\n",
    "    #if (type(response)==bytes and str(response) == 'No search results to display.'):\n",
    "    #    return None\n",
    "    #print(\"Response:\",r)\n",
    "\n",
    "    # If the reply says the placename wasn't found, customise the JSON data for the reply\n",
    "    if r.content.decode() == \"No search results to display.\":\n",
    "        # This should have obviously just be an empty list of features, but TLCMap is badly behaved\n",
    "        response = json.loads('{\"type\": \"FeatureCollection\",\"metadata\": {},\"features\": []}')\n",
    "    # SUCCESS! Record the spatial data provided in the reply\n",
    "    elif r.ok:\n",
    "        response = r.json()    # get [lon, lat] for spatial matches\n",
    "\n",
    "    return response\n",
    "\n",
    "    #log(f\"Query returned {response.status_code}\")\n",
    "    if response.ok:\n",
    "    #    \"\"\"\n",
    "    #    NOTE: we could catch json.decoder.JSONDecodeError, but since json=<3.4 doesn't raise this,\n",
    "    #         a generic ValueError is more portable\n",
    "    #    See: https://stackoverflow.com/questions/44714046/python3-unable-to-import-jsondecodeerror-from-json-decoder\n",
    "    #    \"\"\"\n",
    "    #    try:\n",
    "    #       data = json.loads(response.content)\n",
    "    #    except ValueError: #Error handling for 0 matches \n",
    "    #        return None\n",
    "        return response\n",
    "    return None\n",
    "\n",
    "def tlc_query_name(placename: str, search_type: str):\n",
    "    \"\"\"\n",
    "    Use tlcmap/ghap API to check a placename, implemented fuzzy search but will not handle non returns.\n",
    "    \"\"\"\n",
    "    url = tlc_build_url(placename, search_type, search_public_data)\n",
    "    #print(url)\n",
    "    if url:\n",
    "        return tlc_call_api(url)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the api response to make comparison easier\n",
    "def tlc_format_response(input):\n",
    "\n",
    "    #if input = \"b'No search results to display.'\":\n",
    "    #    return None\n",
    "    \n",
    "    return input\n",
    "\n",
    "    # extract the country name, if any\n",
    "    hyperlocation = None;\n",
    "    if input[\"display_name\"].find(','):\n",
    "        # break up the address\n",
    "        namesplit = input[\"display_name\"].split(',')\n",
    "        # extract the rightmost term from the split\n",
    "        hyperlocation = namesplit[len(namesplit)-1]\n",
    "        #hyperlocation=len(namesplit)\n",
    "        \n",
    "        \n",
    "    # for now, keep the names the similiar to what OSM calls them\n",
    "    response = {\"Name\": input[\"display_name\"], \n",
    "              \"Type\": input[\"type\"],\n",
    "              \"Latitude\": input[\"lat\"], \n",
    "              \"Longitude\": input[\"lon\"],\n",
    "              \"PartOf\": hyperlocation,\n",
    "              \"Gazetteer\": \"OSM\",\n",
    "              \"Importance\": input[\"importance\"]\n",
    "                }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now search the TLCMap for locations matching the same placenames you previously searched for in the OSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for the sleepy sea\n",
      "looking for the Bay of Biscay\n",
      "looking for Heath\n",
      "{'name': 'Heath', 'placename': 'Heath', 'description': 'A trig station about 4.5km SW by W of Avondale and about 6km NE by N of the locality Laheys Creek.          ', 'id': 'a82f3', 'state': 'NSW', 'parish': 'BLACKHEATH', 'feature_term': 'trig station', 'original_data_source': 'State Records (TLCM)', 'latitude': '-32.13166666666667', 'longitude': '149.3011111111111', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a82f3', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Van Diemen's Land\n",
      "{'name': \"Van Diemen's Land\", 'placename': \"Van Diemen's Land\", 'description': 'former name of Tasmania', 'id': 'a1696d', 'state': 'TAS', 'feature_term': 'island1', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a1696d', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Vickers\n",
      "looking for Sylvia\n",
      "{'name': 'Sylvia', 'placename': 'Sylvia', 'description': 'Alternative Name: --; Location: 143°49\\'00\" E, 20°51\\'00\" S; QLD Comments: --; Approved; Current', 'id': 'a2b8db', 'state': 'QLD', 'feature_term': 'parish', 'original_data_source': 'State Records (ANPS)', 'latitude': '-20.85', 'longitude': '143.81666666666666', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a2b8db', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Bath\n",
      "{'name': 'Bath', 'placename': 'Bath', 'id': 'a6034a', 'state': 'SA', 'parish': 'Port Adelaide', 'feature_term': 'locality unbounded)', 'original_data_source': 'State Records (ANPS)', 'latitude': '-34.83706', 'longitude': '138.49131', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a6034a', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Julia Vickers's\n",
      "looking for Frere\n",
      "looking for Chatham\n",
      "{'name': 'Chatham', 'placename': 'Chatham', 'description': 'Status: Official; Location: 116.8343, -31.30709', 'id': 'a41714', 'state': 'WA', 'feature_term': 'homestead', 'original_data_source': 'State Records (ANPS)', 'latitude': '-31.30709', 'longitude': '116.8343', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a41714', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for CHAPTER II\n",
      "looking for Surgeon Pine\n",
      "looking for Coromandel\n",
      "{'name': 'Coromandel', 'placename': 'Coromandel', 'description': 'Status: Official; Location: 119.0265, -34.32488', 'id': 'a3a77d', 'state': 'WA', 'feature_term': 'homestead', 'original_data_source': 'State Records (ANPS)', 'latitude': '-34.32488', 'longitude': '119.0265', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a3a77d', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Pine\n",
      "{'name': 'Pine', 'placename': 'Pine', 'description': 'A parish in the Western Division Parish DP No. 752739          ', 'id': 'adc80', 'state': 'NSW', 'parish': 'PINE', 'feature_term': 'parish', 'original_data_source': 'State Records (TLCM)', 'latitude': '-29.66333333333333', 'longitude': '147.55222222222224', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=adc80', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for the Hydaspes for Calcutta\n",
      "looking for the poop guard\n",
      "looking for MONOTONY\n",
      "looking for Three'll\n",
      "looking for Van Diemen's\n",
      "looking for Tasman\n",
      "{'name': 'Tasman', 'placename': 'Tasman', 'description': '32° 28 / 144° 35', 'id': 'a1696c', 'state': 'NSW', 'feature_term': 'homestead', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a1696c', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Cape Pillar\n",
      "{'name': 'Cape Pillar', 'placename': 'Cape Pillar', 'description': 'Official', 'id': 'a17512', 'state': 'TAS', 'feature_term': 'Cape', 'original_data_source': 'Australian Gazetteer', 'latitude': '-43.22000122', 'longitude': '148.0099945', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a17512', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Pirates' Bay\n",
      "looking for east\n",
      "{'name': 'East', 'placename': 'East', 'description': 'Status: Official; Location: 122.5848, -30.59869', 'id': 'a44006', 'state': 'WA', 'original_data_source': 'State Records (ANPS)', 'latitude': '-30.59869', 'longitude': '122.5848', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a44006', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for west\n",
      "looking for the Isle of Wight\n",
      "looking for the South-West Cape\n",
      "looking for Swan Port\n",
      "looking for Mediterranean\n",
      "looking for Maria Island\n",
      "{'name': 'Maria Island', 'placename': 'Maria Island', 'description': \"Status: Official; Other Names: -; Location:135° 43' 59, -14° 52' 0\", 'id': 'a46fee', 'state': 'NT', 'original_data_source': 'State Records (ANPS)', 'latitude': '-14.866666666666667', 'longitude': '135.73305555555555', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a46fee', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for the Three Thumbs\n",
      "looking for Peninsula\n",
      "{'name': 'Peninsula', 'placename': 'Peninsula', 'description': 'Status: Official; Location: 116.0693, -33.92686', 'id': 'a3d7d5', 'state': 'WA', 'feature_term': 'homestead', 'original_data_source': 'State Records (ANPS)', 'latitude': '-33.92686', 'longitude': '116.0693', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a3d7d5', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Storm Bay\n",
      "{'name': 'Storm Bay', 'placename': 'Storm Bay', 'description': 'A bay a 0.5 km north of Church Point and a 0.5 km           ', 'id': 'a104ed', 'state': 'NSW', 'parish': 'KIAMA', 'feature_term': 'bay', 'original_data_source': 'State Records (TLCM)', 'latitude': '-34.665', 'longitude': '150.86777777777777', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a104ed', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Storing Island\n",
      "looking for Sorrell\n",
      "looking for Bruny Island\n",
      "{'name': 'Bruny Island', 'placename': 'Bruny Island', 'id': 'a13dfd', 'state': 'TAS', 'feature_term': 'island1', 'original_data_source': 'Australian Gazetteer', 'latitude': '-43.29000092', 'longitude': '147.2799988', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a13dfd', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Mount Royal\n",
      "{'name': 'Mount Royal', 'placename': 'Mount Royal', 'description': 'Eminence in Mount Royal Range about 6.5 km east south east of Cockcrow Mountain. It is surmounted by Royal Trig. Station.          ', 'id': 'a9878', 'state': 'NSW', 'parish': 'BOONABILLA', 'feature_term': 'mountain', 'original_data_source': 'State Records (TLCM)', 'latitude': '-32.181666666666665', 'longitude': '151.33444444444444', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a9878', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for D'Entrecasteaux Channel\n",
      "{'name': \"D'Entrecasteaux Channel\", 'placename': \"D'Entrecasteaux Channel\", 'description': 'Official', 'id': 'a17af4', 'state': 'TAS', 'feature_term': 'Channel1', 'original_data_source': 'Australian Gazetteer', 'latitude': '-43.25999832', 'longitude': '147.2599945', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a17af4', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Actaeon\n",
      "looking for the South Cape\n",
      "looking for New Norfolk\n",
      "{'name': 'New Norfolk', 'placename': 'New Norfolk', 'id': 'a13e01', 'state': 'TAS', 'feature_term': 'locality (bounded)', 'original_data_source': 'Australian Gazetteer', 'latitude': '-42.77999878', 'longitude': '147.0500031', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a13e01', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Derwent\n",
      "{'name': 'Derwent', 'placename': 'Derwent', 'description': 'Alternative Name: --; Location: 140°40\\'00\" E, 21°42\\'00\" S; QLD Comments: --; Approved; Current', 'id': 'a260b1', 'state': 'QLD', 'feature_term': 'parish', 'original_data_source': 'State Records (ANPS)', 'latitude': '-21.7', 'longitude': '140.66666666666666', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a260b1', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for the Southern Ocean\n",
      "looking for Tamar\n",
      "{'name': 'Tamar', 'placename': 'Tamar', 'description': 'A lands administrative division.           ', 'id': 'a109e6', 'state': 'NSW', 'parish': 'TAMAR', 'feature_term': 'parish', 'original_data_source': 'State Records (TLCM)', 'latitude': '-35.815', 'longitude': '144.56777777777776', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a109e6', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Port Philip Bay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for Dromedary\n",
      "{'name': 'Dromedary', 'placename': 'Dromedary', 'description': 'A trig. station on Mount Dromedary about 4.5 km W by N of Central Tilba. TS1877.          ', 'id': 'a5cfb', 'state': 'NSW', 'parish': 'NAROOMA', 'feature_term': 'trig station', 'original_data_source': 'State Records (TLCM)', 'latitude': '-36.29833333333333', 'longitude': '150.03444444444443', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a5cfb', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Mount Wellington\n",
      "{'name': 'Mount Wellington', 'placename': 'Mount Wellington', 'description': 'Official', 'id': 'a19560', 'state': 'TAS', 'feature_term': 'Mountain', 'original_data_source': 'Australian Gazetteer', 'latitude': '-42.88999939', 'longitude': '147.2299957', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a19560', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Launceston\n",
      "{'name': 'Launceston', 'placename': 'Launceston', 'id': 'a13dee', 'state': 'TAS', 'feature_term': 'city', 'original_data_source': 'Australian Gazetteer', 'latitude': '-41.43999863', 'longitude': '147.1399994', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a13dee', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Smyrna\n",
      "looking for Pyramid Island\n",
      "{'name': 'Pyramid Island', 'placename': 'Pyramid Island', 'description': 'Unofficial', 'id': 'a19c70', 'state': 'TAS', 'feature_term': 'Island', 'original_data_source': 'Australian Gazetteer', 'latitude': '-39.81999969', 'longitude': '147.2299957', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a19c70', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Rocky Point\n",
      "{'name': 'Rocky Point', 'placename': 'Rocky Point', 'description': 'A point about 3 km E by N of Minnie Water Trig. Station and about 1 km N of Pipers Hill.          ', 'id': 'aec48', 'state': 'NSW', 'parish': 'SCOPE', 'feature_term': 'point', 'original_data_source': 'State Records (TLCM)', 'latitude': '-29.765', 'longitude': '153.3011111111111', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=aec48', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Port Davey\n",
      "{'name': 'Port Davey', 'placename': 'Port Davey', 'description': 'Official', 'id': 'a19b83', 'state': 'TAS', 'feature_term': 'Bay', 'original_data_source': 'Australian Gazetteer', 'latitude': '-43.33000183', 'longitude': '145.9100037', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a19b83', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Mount Direction\n",
      "{'name': 'Mount Direction', 'placename': 'Mount Direction', 'description': 'official; 143.334583333333, -37.1873611111111', 'id': 'a2025f', 'state': 'VIC', 'feature_term': 'mountain', 'original_data_source': 'Australian Gazetteer', 'latitude': '-37.18736267', 'longitude': '143.3345795', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a2025f', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Macquarie Harbour\n",
      "{'name': 'Macquarie Harbour', 'placename': 'Macquarie Harbour', 'description': 'Official', 'id': 'a19005', 'state': 'TAS', 'feature_term': 'Harbour', 'original_data_source': 'Australian Gazetteer', 'latitude': '-42.29999924', 'longitude': '145.3600006', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a19005', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Mount Heemskirk\n",
      "{'name': 'Mount Heemskirk', 'placename': 'Mount Heemskirk', 'description': '41° 51 S / 145° 10 E', 'id': 'a16962', 'state': 'TAS', 'feature_term': 'mountain', 'original_data_source': 'Australian Gazetteer', 'latitude': '-41.84999847', 'longitude': '145.1699982', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a16962', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Mount Zeehan\n",
      "{'name': 'Mount Zeehan', 'placename': 'Mount Zeehan', 'description': '41° 55 S / 145° 19 E', 'id': 'a16963', 'state': 'TAS', 'feature_term': 'hill', 'original_data_source': 'Australian Gazetteer', 'latitude': '-41.91999817', 'longitude': '145.3200073', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a16963', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for King's River\n",
      "looking for Sarah Island\n",
      "{'name': 'Sarah Island', 'placename': 'Sarah Island', 'id': 'a13dec', 'state': 'TAS', 'feature_term': 'island1', 'original_data_source': 'Australian Gazetteer', 'latitude': '-42.38000107', 'longitude': '145.4400024', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a13dec', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Philip's Island\n",
      "looking for Hobart Town\n",
      "looking for earth\n",
      "looking for south-east\n",
      "looking for Ladybird\n",
      "looking for Commandant\n",
      "looking for Port Arthur\n",
      "{'name': 'Port Arthur', 'placename': 'Port Arthur', 'id': 'a687df', 'state': 'SA', 'parish': 'Clinton', 'feature_term': 'locality (bounded)', 'original_data_source': 'State Records (ANPS)', 'latitude': '-34.14898', 'longitude': '138.06368', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a687df', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Arthur\n",
      "{'name': 'Arthur', 'placename': 'Arthur', 'description': 'A lands administrative division.           ', 'id': 'a57f', 'state': 'NSW', 'parish': 'ARTHUR', 'feature_term': 'parish', 'original_data_source': 'State Records (TLCM)', 'latitude': '-32.681666666666665', 'longitude': '149.8011111111111', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a57f', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Hells Gates\n",
      "{'name': 'Hells Gates', 'placename': 'Hells Gates', 'description': 'A deep bay in the cliff north of Stevens Point.', 'id': 'a13db3', 'state': 'NSW', 'parish': 'LORD HOWE ISLAND', 'feature_term': 'bay1', 'anps_sources': [{'anps_id': 81331, 'source_id': 935, 'source_type': 'Website', 'title': 'Geographical Names Register', 'author': '', 'isbn': '', 'publisher': 'Geographical Names Board of New South Wales', 'source_place': 'www.gnb.nsw.gov.au', 'source_date': '2000+', 'source_location': '', 'anps_library': 'N', 'source_status': 'Primary', 'source_notes': ''}], 'original_data_source': 'ANPS Research', 'latitude': '-31.518472222222222', 'longitude': '159.06355555555558', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a13db3', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for England\n",
      "{'name': 'England', 'placename': 'England', 'description': 'Alternative Name: --; Location: 152°28\\'00\" E, 27°26\\'00\" S; QLD Comments: --; Approved; Current', 'id': 'a268fa', 'state': 'QLD', 'feature_term': 'parish', 'original_data_source': 'State Records (ANPS)', 'latitude': '-27.433333333333334', 'longitude': '152.46666666666667', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a268fa', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for New Town\n",
      "{'name': 'New Town', 'placename': 'New Town', 'id': 'a6a97d', 'state': 'SA', 'parish': 'Wallaroo', 'feature_term': 'locality (bounded)', 'original_data_source': 'State Records (ANPS)', 'latitude': '-33.95489', 'longitude': '137.69592', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a6a97d', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for verandah.-She\n",
      "looking for Grummet Island\n",
      "{'name': 'Grummet Island', 'placename': 'Grummet Island', 'description': 'Official', 'id': 'a18425', 'state': 'TAS', 'feature_term': 'Island', 'original_data_source': 'Australian Gazetteer', 'latitude': '-42.38000107', 'longitude': '145.4499969', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a18425', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Grummet\n",
      "looking for Malabar\n",
      "{'name': 'Malabar', 'placename': 'Malabar', 'id': 'a6808c', 'state': 'SA', 'parish': 'Clinton', 'feature_term': 'homestead', 'original_data_source': 'State Records (ANPS)', 'latitude': '-34.19637', 'longitude': '137.87361', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a6808c', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Dawes\n",
      "{'name': 'Dawes', 'placename': 'Dawes', 'description': 'A parish in the Western Division Parish DP No. 757477          ', 'id': 'a52df', 'state': 'NSW', 'parish': 'DAWES', 'feature_term': 'parish', 'original_data_source': 'State Records (TLCM)', 'latitude': '-31.17527777777778', 'longitude': '142.79805555555555', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a52df', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n",
      "looking for Sydney\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Sydney', 'placename': 'Sydney', 'description': 'Alternative Name: --; Location: 143°10\\'00\" E, 26°28\\'00\" S; QLD Comments: --; Approved; Current', 'id': 'a2b8ce', 'state': 'QLD', 'feature_term': 'parish', 'original_data_source': 'State Records (ANPS)', 'latitude': '-26.466666666666665', 'longitude': '143.16666666666666', 'TLCMapLinkBack': 'https://tlcmap.org/ghap/search?id=a2b8ce', 'TLCMapDataset': 'https://tlcmap.org/ghap/'}\n"
     ]
    }
   ],
   "source": [
    "# For every placename in our list\n",
    "for p in geolocdata:\n",
    "    # Already found a location, so skip to the next placename\n",
    "    if len(p['locations']['best_match']) > 0:\n",
    "        continue\n",
    "        \n",
    "    placename = p['placename']\n",
    "    print (\"looking for\",placename)\n",
    "\n",
    "    # query the OSM database\n",
    "    response = tlc_query_name(placename,search_type)\n",
    "    #response_dict = response.to_dict()\n",
    "    #print(response.text)\n",
    "    #print(response)\n",
    "    \n",
    "    #p['locations']['candidates']=None\n",
    "    \n",
    "    # Handle no results found\n",
    "    if response is None:\n",
    "        # skip to the next placename\n",
    "        continue\n",
    "        \n",
    "    # Save the possible locations for later processing\n",
    "    data_frames = []\n",
    "\n",
    "    # Handle results found\n",
    "    for response_record in response[\"features\"]:\n",
    "        #  Use this to look at a reduced set of data from the results\n",
    "        print(response_record['properties'])\n",
    "        cleaned_response = tlc_format_response(response_record)\n",
    "        #if cleaned_response != None:\n",
    "        #    print(\"   ...... \",cleaned_response)\n",
    "            \n",
    "        locdata={}\n",
    "            \n",
    "        if cleaned_response:\n",
    "            continue\n",
    "            \n",
    "            if len(cleaned_response[\"features\"]):\n",
    "                for locationfeatures in cleaned_response['features']:\n",
    "                    # Gather the locdata for one of the placename's location \n",
    "                    if 'placename' in locationfeatures['properties']:\n",
    "                        locdata['name'] = locationfeatures['properties']['placename']\n",
    "                    else:\n",
    "                        locdata['name'] = \"NoLocationName\"\n",
    "                    locdata['coordinates']=locationfeatures[\"geometry\"][\"coordinates\"]\n",
    "                    # for future heuristics, keep a record of which political state it is located in\n",
    "                    if 'state' in locationfeatures['properties']:\n",
    "                        state_count.append(locationfeatures['properties']['state'])\n",
    "                        locdata['state']=locationfeatures['properties']['state']\n",
    "                    else:\n",
    "                        locdata['state']=None\n",
    "\n",
    "        print(locdata)\n",
    "                        \n",
    "        # Add the data to a dataframe\n",
    "        df = pd.DataFrame(columns = [#'name' , \n",
    "                                     'Location',\n",
    "                                     \"Category\",\n",
    "                                     \"Latitude\",\n",
    "                                     \"Longitude\",\n",
    "                                     \"PartOf\",\n",
    "                                     \"Gazetteer\",\n",
    "                                     \"Certainity\"])\n",
    "        #df = df.append({#\"name\": placename, \n",
    "        #                \"Location\": cleaned_response[\"Name\"],\n",
    "        #                \"Category\": cleaned_response[\"Type\"],\n",
    "        #                'Latitude': cleaned_response[\"Latitude\"],\n",
    "        #                'Longitude': cleaned_response[\"Longitude\"],\n",
    "        #                'PartOf': cleaned_response[\"PartOf\"],\n",
    "        #                'Gazetteer': cleaned_response[\"Gazetteer\"],\n",
    "        #                'Certainity': cleaned_response[\"Importance\"]}, \n",
    "        #               ignore_index=True)\n",
    "        #data_frames.append(df)\n",
    "        \n",
    "        ## print the output\n",
    "        #matchdata = df.to_string(index=False,header=False)\n",
    "        #print(\"  *  \",matchdata)\n",
    "\n",
    "\n",
    "        #print(df)\n",
    "\n",
    "    # add the results to the geoloc dataframe\n",
    "    # review the outcomes later\n",
    "    # Match sure you don't write any candidates previously added from another gazetteer.\n",
    "    #p['locations']['candidates'] = data_frames\n",
    "\n",
    "    #print(p['locations']['candidates'])\n",
    "        \n",
    "    #exit()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
